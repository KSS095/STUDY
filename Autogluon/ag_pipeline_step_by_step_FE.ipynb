{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86d2d495",
   "metadata": {},
   "source": [
    "\n",
    "# AutoGluon 분류 파이프라인 (Stratified K-Fold, 셀별 실행 · 함수형태 X)\n",
    "\n",
    "**목표**: `train.csv`(타깃: `target`)로 학습 → `test.csv` 분류 결과 생성  \n",
    "**요구사항 반영**\n",
    "- Stratified K-Fold (OOF & 폴드 성능)\n",
    "- **각 기능별 셀**로 구현(함수 정의 없이 바로 실행)\n",
    "- 데이터 전처리:\n",
    "  - **G1 그룹**: `[\"X_05\",\"X_09\",\"X_20\",\"X_22\",\"X_25\",\"X_51\"]` → 표준화(평균0, 분산1) → **PCA(설명분산 0.95)** → `PC_G1_1, PC_G1_2, ...`만 남기고 G1 원본 6개 변수 **삭제**\n",
    "  - **PAIR_GROUPS(쌍 그룹)**: 각 쌍에서 대표 1개만 남김. **대표 선택 기준**: 결측률↓, mutual information(↑), ANOVA-F(↑)\n",
    "- AutoGluon으로 학습/검증/추론\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e2c456a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Config loaded.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 1. Imports & Config ===\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_classif, f_classif\n",
    "\n",
    "try:\n",
    "    from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "    AUTOGluon_OK = True\n",
    "except Exception as e:\n",
    "    AUTOGluon_OK = False\n",
    "    print('[WARN] AutoGluon import 실패:', e)\n",
    "\n",
    "# 경로/설정\n",
    "DATA_DIR = Path('.')\n",
    "TRAIN_PATH = DATA_DIR / 'train.csv'\n",
    "TEST_PATH  = DATA_DIR / 'test.csv'\n",
    "TARGET_COL = 'target'\n",
    "ID_COL = None  # 예: 'id' (없으면 None)\n",
    "\n",
    "N_FOLDS = 5\n",
    "RANDOM_STATE = 42\n",
    "PRESETS = 'medium_quality'\n",
    "TIME_LIMIT = 1800  # 초(폴드당)\n",
    "\n",
    "SAVE_ROOT = Path('./ag_cv_models')\n",
    "OOF_PATH  = Path('./oof_predictions.csv')\n",
    "FULL_DIR  = Path('./ag_full_model')\n",
    "SUB_PATH  = Path('./submission.csv')\n",
    "\n",
    "# 그룹 정의\n",
    "# 상관 계수 0.95 이상인 쌍\n",
    "# 파생 변수 만들 예정\n",
    "# 파생 변수 만들고 컬럼을 삭제 안하는 게 더 성능이 좋게 나옴.(왜지?)\n",
    "G1 = [\"X_05\",\"X_09\",\"X_20\",\"X_22\",\"X_25\",\"X_51\"]\n",
    "VAR_RATIO = 0.95  # PCA 설명분산 임계\n",
    "\n",
    "# 일단 여기선 별로 사용이 안 됨.\n",
    "# 쌍 변수 그룹 (필요시 더 추가)\n",
    "# 그 다음으로 상관 계수 높은 쌍\n",
    "# 얘네들은 한 쌍에서 한 feature는 죽이기\n",
    "PAIR_GROUPS = [\n",
    "    [\"X_04\",\"X_39\"],\n",
    "    [\"X_06\",\"X_45\"],\n",
    "    ####################\n",
    "    [\"X_10\",\"X_17\"],\n",
    "    [\"X_07\",\"X_33\"],\n",
    "    [\"X_12\",\"X_21\"],\n",
    "    [\"X_26\",\"X_30\"],\n",
    "    [\"X_38\",\"X_47\"],\n",
    "    \n",
    "    # [\"X_05\",\"X_25\"], ...  # 예시로 더 넣을 수 있음\n",
    "]\n",
    "\n",
    "# 재현성\n",
    "random.seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print('[INFO] Config loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6baf9e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] train.shape = (21693, 54)\n",
      "[INFO] test.shape  = (15004, 53)\n",
      "[INFO] 클래스 수: 21\n",
      "[INFO] 예시: ['0', '1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '3', '4', '5', '6', '7', '8']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 2. Load train/test, 타깃 점검 ===\n",
    "assert TRAIN_PATH.exists(), f'train.csv가 {TRAIN_PATH} 에 없습니다.'\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "print('[INFO] train.shape =', train.shape)\n",
    "assert TARGET_COL in train.columns, f'{TARGET_COL} 컬럼이 train에 없습니다.'\n",
    "\n",
    "if TEST_PATH.exists():\n",
    "    test = pd.read_csv(TEST_PATH)\n",
    "    print('[INFO] test.shape  =', test.shape)\n",
    "else:\n",
    "    test = None\n",
    "    print('[WARN] test.csv 가 없어 추론 셀은 스킵될 수 있습니다.')\n",
    "\n",
    "# 타깃 클린업(문자열 통일 + 결측 제거)\n",
    "y = train[TARGET_COL].astype(str)\n",
    "na_mask = y.isna() | (y.str.lower()=='nan') | (y=='None') | (y=='')\n",
    "if na_mask.any():\n",
    "    print(f'[WARN] 타깃 결측/유사결측 {na_mask.sum()}건 발견 → 해당 행 제거')\n",
    "    train = train.loc[~na_mask].reset_index(drop=True)\n",
    "    y = train[TARGET_COL].astype(str)\n",
    "\n",
    "ALL_CLASSES = sorted(y.unique())\n",
    "print('[INFO] 클래스 수:', len(ALL_CLASSES))\n",
    "print('[INFO] 예시:', ALL_CLASSES[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "06254dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 대표 선택 결과: {('X_04', 'X_39'): 'X_04', ('X_06', 'X_45'): 'X_06', ('X_10', 'X_17'): 'X_17', ('X_07', 'X_33'): 'X_07', ('X_12', 'X_21'): 'X_12', ('X_26', 'X_30'): 'X_26', ('X_38', 'X_47'): 'X_38'}\n",
      "[INFO] 삭제 대상(대표 아닌 변수): ['X_39', 'X_45', 'X_10', 'X_33', 'X_21', 'X_30', 'X_47']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 3. PAIR_GROUPS 전역 대표 변수 선택 (train 기준) ===\n",
    "# - 결측률 낮을수록 좋음\n",
    "# - mutual information 높을수록 좋음\n",
    "# - ANOVA-F 높을수록 좋음\n",
    "# 점수 = (-결측률) 정규화 + (MI) 정규화 + (F) 정규화  (각 1/3 가중)\n",
    "\n",
    "# 고상관 쌍(PAIR_GROUPS)의 각 쌍에서 대표 변수 1개만 남기기 위해, 두 변수의 “품질+유용성”을 점수로 계산해 비교하는 로직\n",
    "# -> 39, 45 삭제\n",
    "\n",
    "rep_map = {}         # {('X_04','X_39'): 'X_04', ...}\n",
    "to_drop_pairs = []   # 대표가 아닌 변수 목록\n",
    "\n",
    "# 스코어 계산을 위한 임시 데이터(결측 채움: 수치 median) 구성\n",
    "num_cols = train.drop(columns=[TARGET_COL]).select_dtypes(include=[np.number]).columns.tolist()\n",
    "tmp = train.copy()\n",
    "\n",
    "if len(num_cols) > 0:\n",
    "    imputer_num = SimpleImputer(strategy='median')\n",
    "    tmp[num_cols] = imputer_num.fit_transform(tmp[num_cols])\n",
    "\n",
    "# 각 쌍에 대해 점수 산출\n",
    "for pair in PAIR_GROUPS:\n",
    "    pair = [c for c in pair if c in train.columns and c != TARGET_COL]\n",
    "    if len(pair) != 2:\n",
    "        print(f'[WARN] 쌍 {pair} 중 유효 컬럼이 2개가 아닙니다. 스킵.')\n",
    "        continue\n",
    "\n",
    "    cols_ok = []\n",
    "    scores = []\n",
    "    for col in pair:\n",
    "        # 결측률 (원본 기준)\n",
    "        miss_rate = train[col].isna().mean()\n",
    "\n",
    "        # MI/F는 수치형에서만 바로 계산. 수치가 아닐 경우 임시 변환(범주→순번 인코딩)\n",
    "        s = tmp[col]\n",
    "        if not np.issubdtype(s.dtype, np.number):\n",
    "            # 간단 라벨 인코딩\n",
    "            s = s.astype('category').cat.codes\n",
    "\n",
    "        X_ = s.values.reshape(-1, 1)\n",
    "        y_ = train[TARGET_COL].astype(str).values\n",
    "\n",
    "        # MI (discrete target)\n",
    "        try:\n",
    "            mi = mutual_info_classif(X_, y_, discrete_features=True, random_state=RANDOM_STATE)[0]\n",
    "        except Exception:\n",
    "            mi = 0.0\n",
    "\n",
    "        # ANOVA-F\n",
    "        try:\n",
    "            f, _ = f_classif(X_, pd.factorize(y_)[0])\n",
    "            f = float(f[0])\n",
    "        except Exception:\n",
    "            f = 0.0\n",
    "\n",
    "        cols_ok.append(col)\n",
    "        scores.append({'col': col, 'miss': miss_rate, 'mi': mi, 'f': f})\n",
    "\n",
    "    if len(scores) != 2:\n",
    "        print(f'[WARN] {pair} 점수 계산 실패(스킵)')\n",
    "        continue\n",
    "\n",
    "    # 정규화\n",
    "    miss_vals = np.array([s['miss'] for s in scores])\n",
    "    mi_vals   = np.array([s['mi']   for s in scores])\n",
    "    f_vals    = np.array([s['f']    for s in scores])\n",
    "\n",
    "    def norm(v):\n",
    "        v = v.astype(float)\n",
    "        if np.allclose(v.max(), v.min()):\n",
    "            return np.zeros_like(v)\n",
    "        return (v - v.min()) / (v.max() - v.min())\n",
    "\n",
    "    miss_n = norm(1 - miss_vals)  # 결측률 낮을수록↑ → (1 - miss)\n",
    "    mi_n   = norm(mi_vals)\n",
    "    f_n    = norm(f_vals)\n",
    "\n",
    "    final  = (miss_n + mi_n + f_n) / 3.0\n",
    "    best_idx = int(np.argmax(final))\n",
    "    rep = scores[best_idx]['col']\n",
    "    rep_map[tuple(pair)] = rep\n",
    "\n",
    "    drop_cols = [c for c in pair if c != rep]\n",
    "    to_drop_pairs.extend(drop_cols)\n",
    "\n",
    "print('[INFO] 대표 선택 결과:', rep_map)\n",
    "print('[INFO] 삭제 대상(대표 아닌 변수):', to_drop_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ed51c973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] train_pair.shape = (21693, 47)\n",
      "[INFO] test_pair.shape  = (15004, 46)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 4. 대표 아닌 변수 삭제(전역) ===\n",
    "train_pair = train.drop(columns=[c for c in to_drop_pairs if c in train.columns], errors='ignore').copy()\n",
    "if test is not None:\n",
    "    test_pair = test.drop(columns=[c for c in to_drop_pairs if c in test.columns], errors='ignore').copy()\n",
    "else:\n",
    "    test_pair = None\n",
    "\n",
    "print('[INFO] train_pair.shape =', train_pair.shape)\n",
    "if test_pair is not None:\n",
    "    print('[INFO] test_pair.shape  =', test_pair.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4502f01b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'X_01', 'X_02', 'X_03', 'X_04', 'X_05', 'X_06', 'X_07', 'X_08',\n",
       "       'X_09', 'X_11', 'X_12', 'X_13', 'X_14', 'X_15', 'X_16', 'X_17', 'X_18',\n",
       "       'X_19', 'X_20', 'X_22', 'X_23', 'X_24', 'X_25', 'X_26', 'X_27', 'X_28',\n",
       "       'X_29', 'X_31', 'X_32', 'X_34', 'X_35', 'X_36', 'X_37', 'X_38', 'X_40',\n",
       "       'X_41', 'X_42', 'X_43', 'X_44', 'X_46', 'X_48', 'X_49', 'X_50', 'X_51',\n",
       "       'X_52', 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pair.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "37efda8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 파생 피처 추가 완료. train_pair: (21693, 62) | test_pair: (15004, 61)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 파생 피처 추가 (함수 없이 바로 적용) ===\n",
    "_base_cols = [c for c in train_pair.columns if c != TARGET_COL]\n",
    "_num_cols  = train_pair[_base_cols].select_dtypes(include=[float, int]).columns\n",
    "\n",
    "# 1) 행 단위 요약\n",
    "train_pair['row_na_cnt'] = train_pair[_base_cols].isna().sum(axis=1)\n",
    "if test_pair is not None:\n",
    "    test_pair['row_na_cnt'] = test_pair[_base_cols].isna().sum(axis=1)\n",
    "\n",
    "train_pair['row_num_mean'] = train_pair[_num_cols].mean(axis=1)\n",
    "train_pair['row_num_std']  = train_pair[_num_cols].std(axis=1)\n",
    "if test_pair is not None:\n",
    "    test_pair['row_num_mean'] = test_pair[_num_cols].mean(axis=1)\n",
    "    test_pair['row_num_std']  = test_pair[_num_cols].std(axis=1)\n",
    "\n",
    "if len(_num_cols) > 0:\n",
    "    _Q1  = train_pair[_num_cols].quantile(0.25)\n",
    "    _Q3  = train_pair[_num_cols].quantile(0.75)\n",
    "    _IQR = (_Q3 - _Q1).replace(0, 1e-12)\n",
    "\n",
    "    _below_tr = (train_pair[_num_cols] < (_Q1 - 1.5 * _IQR))\n",
    "    _above_tr = (train_pair[_num_cols] > (_Q3 + 1.5 * _IQR))\n",
    "    train_pair['row_outlier_cnt'] = (_below_tr | _above_tr).sum(axis=1)\n",
    "\n",
    "    if test_pair is not None:\n",
    "        _below_te = (test_pair[_num_cols] < (_Q1 - 1.5 * _IQR))\n",
    "        _above_te = (test_pair[_num_cols] > (_Q3 + 1.5 * _IQR))\n",
    "        test_pair['row_outlier_cnt'] = (_below_te | _above_te).sum(axis=1)\n",
    "\n",
    "# 2) G1 그룹 통계\n",
    "_g1_cols = [c for c in G1 if c in train_pair.columns]\n",
    "if len(_g1_cols) > 0:\n",
    "    train_pair['G1_mean']  = train_pair[_g1_cols].mean(axis=1)\n",
    "    train_pair['G1_std']   = train_pair[_g1_cols].std(axis=1)\n",
    "    train_pair['G1_min']   = train_pair[_g1_cols].min(axis=1)\n",
    "    train_pair['G1_max']   = train_pair[_g1_cols].max(axis=1)\n",
    "    train_pair['G1_range'] = train_pair['G1_max'] - train_pair['G1_min']\n",
    "\n",
    "    if test_pair is not None:\n",
    "        test_pair['G1_mean']  = test_pair[_g1_cols].mean(axis=1)\n",
    "        test_pair['G1_std']   = test_pair[_g1_cols].std(axis=1)\n",
    "        test_pair['G1_min']   = test_pair[_g1_cols].min(axis=1)\n",
    "        test_pair['G1_max']   = test_pair[_g1_cols].max(axis=1)\n",
    "        test_pair['G1_range'] = test_pair['G1_max'] - test_pair['G1_min']\n",
    "\n",
    "\n",
    "# 3) 비선형 파생 (예: X_05, X_09)\n",
    "if 'X_05' in train_pair.columns and 'X_09' in train_pair.columns:\n",
    "    train_pair['X05_div_X09'] = train_pair['X_05'] / (train_pair['X_09'].replace(0, 1e-12) + 1e-12)\n",
    "    train_pair['X05_mul_X09'] = train_pair['X_05'] * train_pair['X_09']\n",
    "    if test_pair is not None:\n",
    "        test_pair['X05_div_X09'] = test_pair['X_05'] / (test_pair['X_09'].replace(0, 1e-12) + 1e-12)\n",
    "        test_pair['X05_mul_X09'] = test_pair['X_05'] * test_pair['X_09']\n",
    "\n",
    "# if 'X_05' in train_pair.columns:\n",
    "#     train_pair['X05_sq'] = train_pair['X_05'] ** 2\n",
    "#     if test_pair is not None:\n",
    "#         test_pair['X05_sq'] = test_pair['X_05'] ** 2\n",
    "\n",
    "# if 'X_09' in train_pair.columns:\n",
    "#     train_pair['X09_sq'] = train_pair['X_09'] ** 2\n",
    "#     if test_pair is not None:\n",
    "#         test_pair['X09_sq'] = test_pair['X_09'] ** 2\n",
    "\n",
    "\n",
    "if 'X_20' in train_pair.columns and 'X_22' in train_pair.columns:\n",
    "    train_pair['X20_div_X22'] = train_pair['X_20'] / (train_pair['X_22'].replace(0, 1e-12) + 1e-12)\n",
    "    train_pair['X20_mul_X22'] = train_pair['X_20'] * train_pair['X_22']\n",
    "    if test_pair is not None:\n",
    "        test_pair['X20_div_X22'] = test_pair['X_20'] / (test_pair['X_22'].replace(0, 1e-12) + 1e-12)\n",
    "        test_pair['X20_mul_X22'] = test_pair['X_20'] * test_pair['X_22']\n",
    "\n",
    "# if 'X_20' in train_pair.columns:\n",
    "#     train_pair['X20_sq'] = train_pair['X_20'] ** 2\n",
    "#     if test_pair is not None:\n",
    "#         test_pair['X20_sq'] = test_pair['X_20'] ** 2\n",
    "\n",
    "# if 'X_22' in train_pair.columns:\n",
    "#     train_pair['X22_sq'] = train_pair['X_22'] ** 2\n",
    "#     if test_pair is not None:\n",
    "#         test_pair['X22_sq'] = test_pair['X_22'] ** 2\n",
    "\n",
    "\n",
    "\n",
    "if 'X_25' in train_pair.columns and 'X_51' in train_pair.columns:\n",
    "    train_pair['X25_div_X51'] = train_pair['X_25'] / (train_pair['X_51'].replace(0, 1e-12) + 1e-12)\n",
    "    train_pair['X25_mul_X51'] = train_pair['X_25'] * train_pair['X_51']\n",
    "    if test_pair is not None:\n",
    "        test_pair['X25_div_X51'] = test_pair['X_25'] / (test_pair['X_51'].replace(0, 1e-12) + 1e-12)\n",
    "        test_pair['X25_mul_X51'] = test_pair['X_25'] * test_pair['X_51']\n",
    "\n",
    "# if 'X_25' in train_pair.columns:\n",
    "#     train_pair['X25_sq'] = train_pair['X_25'] ** 2\n",
    "#     if test_pair is not None:\n",
    "#         test_pair['X25_sq'] = test_pair['X_25'] ** 2\n",
    "\n",
    "# if 'X_51' in train_pair.columns:\n",
    "#     train_pair['X51_sq'] = train_pair['X_51'] ** 2\n",
    "#     if test_pair is not None:\n",
    "#         test_pair['X51_sq'] = test_pair['X_51'] ** 2\n",
    "\n",
    "\n",
    "print('[INFO] 파생 피처 추가 완료. train_pair:', train_pair.shape, '| test_pair:', (None if test_pair is None else test_pair.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f88b4e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_cv_models\\fold_1\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.9\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          28\n",
      "Memory Avail:       16.36 GB / 31.69 GB (51.6%)\n",
      "Disk Space Avail:   819.80 GB / 953.01 GB (86.0%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== [Fold 1/5] =====\n",
      "[INFO] ag_cv_models\\fold_1 재사용/덮어쓰기\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ... Time limit = 1800s\n",
      "AutoGluon will save models to \"c:\\Users\\SSAFY\\Downloads\\Autogluon\\ag_cv_models\\fold_1\"\n",
      "Train Data Rows:    17354\n",
      "Train Data Columns: 56\n",
      "Tuning Data Rows:    4339\n",
      "Tuning Data Columns: 56\n",
      "Label Column:       target\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
      "\tFirst 10 (of 21) unique label values:  [np.int64(0), np.int64(20), np.int64(1), np.int64(19), np.int64(15), np.int64(8), np.int64(12), np.int64(4), np.int64(5), np.int64(11)]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 21\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16741.66 MB\n",
      "\tTrain Data (Original)  Memory Usage: 10.51 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['row_na_cnt']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['ID']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('object', []) : 1 | ['ID']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_06', ...]\n",
      "\t\t('int', [])   :  1 | ['row_outlier_cnt']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_06', ...]\n",
      "\t\t('int', [])   :  1 | ['row_outlier_cnt']\n",
      "\t0.1s = Fit runtime\n",
      "\t54 features in original data used to generate 54 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 8.94 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.16s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_macro'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 1799.84s of the 1799.84s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.1/16.3 GB\n",
      "\t0.8643\t = Validation score   (f1_macro)\n",
      "\t17.68s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 1782.10s of the 1782.10s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.1/16.4 GB\n",
      "\t0.7948\t = Validation score   (f1_macro)\n",
      "\t13.26s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1767.65s of the 1767.65s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.1/16.4 GB\n",
      "\t0.8175\t = Validation score   (f1_macro)\n",
      "\t11.69s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 1755.12s of the 1755.12s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/16.4 GB\n",
      "\t0.7735\t = Validation score   (f1_macro)\n",
      "\t4.15s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 1750.65s of the 1750.65s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/16.1 GB\n",
      "\t0.7692\t = Validation score   (f1_macro)\n",
      "\t9.27s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 1741.08s of the 1741.08s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.8013\t = Validation score   (f1_macro)\n",
      "\t211.01s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 1529.98s of the 1529.98s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/16.6 GB\n",
      "\t0.7521\t = Validation score   (f1_macro)\n",
      "\t1.44s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 1527.87s of the 1527.87s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/16.0 GB\n",
      "\t0.7348\t = Validation score   (f1_macro)\n",
      "\t1.56s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 1525.81s of the 1525.81s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.8119\t = Validation score   (f1_macro)\n",
      "\t21.98s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 1503.34s of the 1503.34s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.0/16.4 GB\n",
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t0.8545\t = Validation score   (f1_macro)\n",
      "\t252.94s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 1250.34s of the 1250.34s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.4/16.3 GB\n",
      "\t0.8067\t = Validation score   (f1_macro)\n",
      "\t28.54s\t = Training   runtime\n",
      "\t0.36s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1220.12s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.6, 'NeuralNetTorch': 0.4}\n",
      "\t0.8716\t = Validation score   (f1_macro)\n",
      "\t0.49s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 580.38s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 45645.1 rows/s (4339 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\SSAFY\\Downloads\\Autogluon\\ag_cv_models\\fold_1\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_cv_models\\fold_2\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.9\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          28\n",
      "Memory Avail:       16.83 GB / 31.69 GB (53.1%)\n",
      "Disk Space Avail:   819.84 GB / 953.01 GB (86.0%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1] macro-F1: 0.8716\n",
      "\n",
      "===== [Fold 2/5] =====\n",
      "[INFO] ag_cv_models\\fold_2 재사용/덮어쓰기\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ... Time limit = 1800s\n",
      "AutoGluon will save models to \"c:\\Users\\SSAFY\\Downloads\\Autogluon\\ag_cv_models\\fold_2\"\n",
      "Train Data Rows:    17354\n",
      "Train Data Columns: 56\n",
      "Tuning Data Rows:    4339\n",
      "Tuning Data Columns: 56\n",
      "Label Column:       target\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
      "\tFirst 10 (of 21) unique label values:  [np.int64(20), np.int64(1), np.int64(15), np.int64(0), np.int64(8), np.int64(16), np.int64(14), np.int64(18), np.int64(3), np.int64(5)]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 21\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17218.25 MB\n",
      "\tTrain Data (Original)  Memory Usage: 10.51 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['row_na_cnt']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['ID']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('object', []) : 1 | ['ID']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_06', ...]\n",
      "\t\t('int', [])   :  1 | ['row_outlier_cnt']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_06', ...]\n",
      "\t\t('int', [])   :  1 | ['row_outlier_cnt']\n",
      "\t0.2s = Fit runtime\n",
      "\t54 features in original data used to generate 54 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 8.94 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.2s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_macro'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 1799.80s of the 1799.80s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.1/16.8 GB\n",
      "\t0.8651\t = Validation score   (f1_macro)\n",
      "\t17.57s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 1782.16s of the 1782.16s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.1/16.8 GB\n",
      "\t0.7918\t = Validation score   (f1_macro)\n",
      "\t11.67s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1769.64s of the 1769.64s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.1/16.9 GB\n",
      "\t0.8142\t = Validation score   (f1_macro)\n",
      "\t12.39s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 1756.33s of the 1756.33s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/16.9 GB\n",
      "\t0.7831\t = Validation score   (f1_macro)\n",
      "\t4.41s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 1751.51s of the 1751.51s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/16.3 GB\n",
      "\t0.7769\t = Validation score   (f1_macro)\n",
      "\t9.69s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 1741.46s of the 1741.45s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.7954\t = Validation score   (f1_macro)\n",
      "\t241.65s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 1499.68s of the 1499.68s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/17.1 GB\n",
      "\t0.7569\t = Validation score   (f1_macro)\n",
      "\t1.39s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 1497.79s of the 1497.79s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/16.4 GB\n",
      "\t0.7459\t = Validation score   (f1_macro)\n",
      "\t1.48s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 1495.83s of the 1495.83s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.8055\t = Validation score   (f1_macro)\n",
      "\t20.0s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 1475.35s of the 1475.35s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.0/16.6 GB\n",
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t0.8546\t = Validation score   (f1_macro)\n",
      "\t248.54s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 1226.75s of the 1226.75s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.4/16.6 GB\n",
      "\t0.8088\t = Validation score   (f1_macro)\n",
      "\t28.3s\t = Training   runtime\n",
      "\t0.34s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1196.95s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.5, 'NeuralNetTorch': 0.25, 'CatBoost': 0.125, 'LightGBMLarge': 0.125}\n",
      "\t0.8694\t = Validation score   (f1_macro)\n",
      "\t0.47s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 603.56s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 9353.8 rows/s (4339 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\SSAFY\\Downloads\\Autogluon\\ag_cv_models\\fold_2\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_cv_models\\fold_3\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.9\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          28\n",
      "Memory Avail:       16.69 GB / 31.69 GB (52.7%)\n",
      "Disk Space Avail:   819.87 GB / 953.01 GB (86.0%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 2] macro-F1: 0.8694\n",
      "\n",
      "===== [Fold 3/5] =====\n",
      "[INFO] ag_cv_models\\fold_3 재사용/덮어쓰기\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ... Time limit = 1800s\n",
      "AutoGluon will save models to \"c:\\Users\\SSAFY\\Downloads\\Autogluon\\ag_cv_models\\fold_3\"\n",
      "Train Data Rows:    17354\n",
      "Train Data Columns: 56\n",
      "Tuning Data Rows:    4339\n",
      "Tuning Data Columns: 56\n",
      "Label Column:       target\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
      "\tFirst 10 (of 21) unique label values:  [np.int64(0), np.int64(20), np.int64(1), np.int64(19), np.int64(8), np.int64(16), np.int64(12), np.int64(14), np.int64(18), np.int64(3)]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 21\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17067.53 MB\n",
      "\tTrain Data (Original)  Memory Usage: 10.51 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['row_na_cnt']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['ID']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('object', []) : 1 | ['ID']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_06', ...]\n",
      "\t\t('int', [])   :  1 | ['row_outlier_cnt']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_06', ...]\n",
      "\t\t('int', [])   :  1 | ['row_outlier_cnt']\n",
      "\t0.2s = Fit runtime\n",
      "\t54 features in original data used to generate 54 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 8.94 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.19s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_macro'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 1799.81s of the 1799.81s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.1/16.7 GB\n",
      "\t0.8644\t = Validation score   (f1_macro)\n",
      "\t21.26s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 1778.49s of the 1778.49s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.1/16.7 GB\n",
      "\t0.7956\t = Validation score   (f1_macro)\n",
      "\t15.1s\t = Training   runtime\n",
      "\t0.27s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1762.06s of the 1762.06s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.1/16.7 GB\n",
      "\t0.8164\t = Validation score   (f1_macro)\n",
      "\t9.02s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 1752.53s of the 1752.53s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/16.7 GB\n",
      "\t0.7727\t = Validation score   (f1_macro)\n",
      "\t4.33s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 1747.87s of the 1747.87s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/16.2 GB\n",
      "\t0.7751\t = Validation score   (f1_macro)\n",
      "\t9.33s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 1738.21s of the 1738.21s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.8007\t = Validation score   (f1_macro)\n",
      "\t253.91s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 1484.12s of the 1484.12s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/16.9 GB\n",
      "\t0.758\t = Validation score   (f1_macro)\n",
      "\t1.69s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 1481.64s of the 1481.64s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/16.3 GB\n",
      "\t0.746\t = Validation score   (f1_macro)\n",
      "\t1.65s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 1479.19s of the 1479.19s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.8091\t = Validation score   (f1_macro)\n",
      "\t20.44s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 1458.34s of the 1458.34s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.0/16.9 GB\n",
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t0.8581\t = Validation score   (f1_macro)\n",
      "\t327.97s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 1130.30s of the 1130.30s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.4/16.8 GB\n",
      "\t0.8058\t = Validation score   (f1_macro)\n",
      "\t23.07s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1106.22s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.625, 'NeuralNetTorch': 0.25, 'LightGBM': 0.125}\n",
      "\t0.8776\t = Validation score   (f1_macro)\n",
      "\t0.45s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 694.26s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 20646.0 rows/s (4339 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\SSAFY\\Downloads\\Autogluon\\ag_cv_models\\fold_3\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_cv_models\\fold_4\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.9\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          28\n",
      "Memory Avail:       16.98 GB / 31.69 GB (53.6%)\n",
      "Disk Space Avail:   819.97 GB / 953.01 GB (86.0%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 1800s\n",
      "AutoGluon will save models to \"c:\\Users\\SSAFY\\Downloads\\Autogluon\\ag_cv_models\\fold_4\"\n",
      "Train Data Rows:    17355\n",
      "Train Data Columns: 56\n",
      "Tuning Data Rows:    4338\n",
      "Tuning Data Columns: 56\n",
      "Label Column:       target\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
      "\tFirst 10 (of 21) unique label values:  [np.int64(0), np.int64(20), np.int64(1), np.int64(19), np.int64(15), np.int64(8), np.int64(16), np.int64(12), np.int64(14), np.int64(18)]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 3] macro-F1: 0.8776\n",
      "\n",
      "===== [Fold 4/5] =====\n",
      "[INFO] ag_cv_models\\fold_4 재사용/덮어쓰기\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Data Class Count: 21\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17378.43 MB\n",
      "\tTrain Data (Original)  Memory Usage: 10.51 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['row_na_cnt']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['ID']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('object', []) : 1 | ['ID']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_06', ...]\n",
      "\t\t('int', [])   :  1 | ['row_outlier_cnt']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_06', ...]\n",
      "\t\t('int', [])   :  1 | ['row_outlier_cnt']\n",
      "\t0.1s = Fit runtime\n",
      "\t54 features in original data used to generate 54 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 8.94 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.17s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_macro'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 1799.83s of the 1799.83s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.1/16.9 GB\n",
      "\t0.8612\t = Validation score   (f1_macro)\n",
      "\t20.33s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 1779.43s of the 1779.43s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.1/17.0 GB\n",
      "\t0.7946\t = Validation score   (f1_macro)\n",
      "\t15.57s\t = Training   runtime\n",
      "\t0.27s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1762.51s of the 1762.51s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.1/17.0 GB\n",
      "\t0.8131\t = Validation score   (f1_macro)\n",
      "\t10.14s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 1751.69s of the 1751.69s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/17.0 GB\n",
      "\t0.7841\t = Validation score   (f1_macro)\n",
      "\t3.84s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 1747.53s of the 1747.53s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/16.5 GB\n",
      "\t0.781\t = Validation score   (f1_macro)\n",
      "\t8.93s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 1738.31s of the 1738.31s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.7971\t = Validation score   (f1_macro)\n",
      "\t215.65s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 1522.56s of the 1522.56s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/16.9 GB\n",
      "\t0.7637\t = Validation score   (f1_macro)\n",
      "\t1.29s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 1520.75s of the 1520.75s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/16.2 GB\n",
      "\t0.7531\t = Validation score   (f1_macro)\n",
      "\t1.36s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 1518.84s of the 1518.84s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.8126\t = Validation score   (f1_macro)\n",
      "\t16.48s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 1501.97s of the 1501.97s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.0/16.7 GB\n",
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t0.8541\t = Validation score   (f1_macro)\n",
      "\t314.07s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 1187.83s of the 1187.83s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.4/16.9 GB\n",
      "\t0.8124\t = Validation score   (f1_macro)\n",
      "\t26.11s\t = Training   runtime\n",
      "\t0.28s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1160.42s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.556, 'NeuralNetTorch': 0.222, 'RandomForestEntr': 0.111, 'ExtraTreesEntr': 0.111}\n",
      "\t0.8697\t = Validation score   (f1_macro)\n",
      "\t0.48s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 640.09s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 13983.5 rows/s (4338 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\SSAFY\\Downloads\\Autogluon\\ag_cv_models\\fold_4\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_cv_models\\fold_5\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.9\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          28\n",
      "Memory Avail:       16.57 GB / 31.69 GB (52.3%)\n",
      "Disk Space Avail:   819.73 GB / 953.01 GB (86.0%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 1800s\n",
      "AutoGluon will save models to \"c:\\Users\\SSAFY\\Downloads\\Autogluon\\ag_cv_models\\fold_5\"\n",
      "Train Data Rows:    17355\n",
      "Train Data Columns: 56\n",
      "Tuning Data Rows:    4338\n",
      "Tuning Data Columns: 56\n",
      "Label Column:       target\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
      "\tFirst 10 (of 21) unique label values:  [np.int64(0), np.int64(19), np.int64(15), np.int64(1), np.int64(16), np.int64(12), np.int64(14), np.int64(18), np.int64(3), np.int64(20)]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 4] macro-F1: 0.8697\n",
      "\n",
      "===== [Fold 5/5] =====\n",
      "[INFO] ag_cv_models\\fold_5 재사용/덮어쓰기\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Data Class Count: 21\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16952.99 MB\n",
      "\tTrain Data (Original)  Memory Usage: 10.51 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['row_na_cnt']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['ID']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('object', []) : 1 | ['ID']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_06', ...]\n",
      "\t\t('int', [])   :  1 | ['row_outlier_cnt']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_06', ...]\n",
      "\t\t('int', [])   :  1 | ['row_outlier_cnt']\n",
      "\t0.1s = Fit runtime\n",
      "\t54 features in original data used to generate 54 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 8.94 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.18s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_macro'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 1799.82s of the 1799.82s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.1/16.5 GB\n",
      "\t0.8655\t = Validation score   (f1_macro)\n",
      "\t16.77s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 1783.00s of the 1783.00s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.1/16.7 GB\n",
      "\t0.7967\t = Validation score   (f1_macro)\n",
      "\t10.27s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1771.94s of the 1771.94s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.1/16.7 GB\n",
      "\t0.8148\t = Validation score   (f1_macro)\n",
      "\t10.65s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 1760.61s of the 1760.61s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/16.7 GB\n",
      "\t0.7841\t = Validation score   (f1_macro)\n",
      "\t3.77s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 1756.51s of the 1756.51s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/16.4 GB\n",
      "\t0.7803\t = Validation score   (f1_macro)\n",
      "\t9.17s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 1747.03s of the 1747.03s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.8037\t = Validation score   (f1_macro)\n",
      "\t209.74s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 1537.19s of the 1537.19s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/16.1 GB\n",
      "\t0.7601\t = Validation score   (f1_macro)\n",
      "\t1.4s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 1535.19s of the 1535.19s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/15.6 GB\n",
      "\t0.744\t = Validation score   (f1_macro)\n",
      "\t1.38s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 1533.29s of the 1533.29s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.8144\t = Validation score   (f1_macro)\n",
      "\t17.41s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 1515.49s of the 1515.49s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.0/16.0 GB\n",
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t0.8526\t = Validation score   (f1_macro)\n",
      "\t305.8s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 1209.64s of the 1209.64s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.4/16.0 GB\n",
      "\t0.8111\t = Validation score   (f1_macro)\n",
      "\t25.64s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1182.81s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.75, 'NeuralNetTorch': 0.25}\n",
      "\t0.8709\t = Validation score   (f1_macro)\n",
      "\t0.47s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 617.69s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 45489.7 rows/s (4338 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\SSAFY\\Downloads\\Autogluon\\ag_cv_models\\fold_5\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 5] macro-F1: 0.8709\n",
      "\n",
      "===== CV 결과 =====\n",
      "Fold scores: [0.8716, 0.8694, 0.8776, 0.8697, 0.8709]\n",
      "OOF macro-F1: 0.8720\n",
      "[INFO] OOF 저장: C:\\Users\\SSAFY\\Downloads\\Autogluon\\oof_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 5. Stratified K-Fold CV (fold별로 G1 표준화+PCA 적용 후 AutoGluon 학습) ===\n",
    "if not AUTOGluon_OK:\n",
    "    raise ImportError('AutoGluon 불러오기 실패. 설치 필요: pip install autogluon.tabular')\n",
    "\n",
    "SAVE_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "fold_indices = list(skf.split(train_pair.drop(columns=[TARGET_COL]), train_pair[TARGET_COL].astype(str)))\n",
    "\n",
    "oof_pred = pd.Series(index=np.arange(len(train_pair)), dtype=object)\n",
    "fold_scores = []\n",
    "\n",
    "for fold, (trn_idx, val_idx) in enumerate(fold_indices, start=1):\n",
    "    print(f'\\n===== [Fold {fold}/{N_FOLDS}] =====')\n",
    "    trn_df = train_pair.iloc[trn_idx].reset_index(drop=True).copy()\n",
    "    val_df = train_pair.iloc[val_idx].reset_index(drop=True).copy()\n",
    "\n",
    "    # --- G1 처리: 표준화 → PCA(0.95) → PC_G1_* 생성, G1 원본 삭제 ---\n",
    "    g1_cols = [c for c in G1 if c in trn_df.columns and c != TARGET_COL]\n",
    "    if len(g1_cols) > 0:\n",
    "        # 결측 채우기(수치 median) → 스케일러/ PCA 학습은 trn 기준\n",
    "        imputer_g1 = SimpleImputer(strategy='median')\n",
    "        scaler_g1  = StandardScaler()\n",
    "        pca_g1     = PCA(n_components=VAR_RATIO, svd_solver='full')\n",
    "\n",
    "        trn_g1 = imputer_g1.fit_transform(trn_df[g1_cols])\n",
    "        trn_g1 = scaler_g1.fit_transform(trn_g1)\n",
    "        trn_g1_pc = pca_g1.fit_transform(trn_g1)\n",
    "\n",
    "        # 검증 변환\n",
    "        val_g1 = imputer_g1.transform(val_df[g1_cols])\n",
    "        val_g1 = scaler_g1.transform(val_g1)\n",
    "        val_g1_pc = pca_g1.transform(val_g1)\n",
    "\n",
    "        # PC 열명 만들기\n",
    "        n_pc = trn_g1_pc.shape[1]\n",
    "        pc_cols = [f'PC_G1_{i+1}' for i in range(n_pc)]\n",
    "\n",
    "        # 데이터프레임에 붙이고 원본 G1 삭제\n",
    "        trn_pc_df = pd.DataFrame(trn_g1_pc, columns=pc_cols)\n",
    "        val_pc_df = pd.DataFrame(val_g1_pc, columns=pc_cols)\n",
    "\n",
    "        trn_df = pd.concat([trn_df.drop(columns=g1_cols), trn_pc_df], axis=1)\n",
    "        val_df = pd.concat([val_df.drop(columns=g1_cols), val_pc_df], axis=1)\n",
    "    else:\n",
    "        print('[INFO] 이번 fold에서 G1 컬럼이 존재하지 않아 PCA 스킵')\n",
    "\n",
    "\n",
    "    # AutoGluon 학습\n",
    "    fold_dir = SAVE_ROOT / f'fold_{fold}'\n",
    "    if fold_dir.exists():\n",
    "        print(f'[INFO] {fold_dir} 재사용/덮어쓰기')\n",
    "    fold_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    ag_trn = TabularDataset(trn_df)\n",
    "    ag_val = TabularDataset(val_df)\n",
    "\n",
    "    predictor = TabularPredictor(label=TARGET_COL, path=str(fold_dir), eval_metric='f1_macro')\n",
    "    predictor.fit(train_data=ag_trn, tuning_data=ag_val, presets=PRESETS, time_limit=TIME_LIMIT, verbosity=2)\n",
    "    # predictor.fit(train_data=ag_trn, presets=PRESETS, time_limit=TIME_LIMIT, verbosity=2)\n",
    "\n",
    "    # 검증 예측\n",
    "    y_true = val_df[TARGET_COL].astype(str).reset_index(drop=True)\n",
    "    y_pred = predictor.predict(ag_val).astype(str).reset_index(drop=True)\n",
    "\n",
    "    score = f1_score(y_true, y_pred, average='macro', labels=ALL_CLASSES, zero_division=0)\n",
    "    print(f'[Fold {fold}] macro-F1: {score:.4f}')\n",
    "    fold_scores.append(score)\n",
    "\n",
    "    oof_pred.iloc[val_idx] = y_pred.values\n",
    "\n",
    "# OOF 성능\n",
    "y_all = train_pair[TARGET_COL].astype(str).reset_index(drop=True)\n",
    "assert oof_pred.isna().sum() == 0, 'OOF 예측에 결측이 있습니다.'\n",
    "\n",
    "oof_score = f1_score(y_all, oof_pred.astype(str), average='macro', labels=ALL_CLASSES, zero_division=0)\n",
    "print('\\n===== CV 결과 =====')\n",
    "print('Fold scores:', np.round(fold_scores, 4).tolist())\n",
    "print(f'OOF macro-F1: {oof_score:.4f}')\n",
    "\n",
    "oof_df = pd.DataFrame({'y_true': y_all, 'y_pred': oof_pred.astype(str)})\n",
    "oof_df.to_csv(OOF_PATH, index=False, encoding='utf-8')\n",
    "print('[INFO] OOF 저장:', OOF_PATH.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b614d758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_full_model\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.9\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          28\n",
      "Memory Avail:       16.12 GB / 31.69 GB (50.9%)\n",
      "Disk Space Avail:   819.78 GB / 953.01 GB (86.0%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 1800s\n",
      "AutoGluon will save models to \"c:\\Users\\SSAFY\\Downloads\\Autogluon\\ag_full_model\"\n",
      "Train Data Rows:    21693\n",
      "Train Data Columns: 56\n",
      "Label Column:       target\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
      "\tFirst 10 (of 21) unique label values:  [np.int64(0), np.int64(20), np.int64(1), np.int64(19), np.int64(15), np.int64(8), np.int64(16), np.int64(12), np.int64(14), np.int64(18)]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 21\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16512.63 MB\n",
      "\tTrain Data (Original)  Memory Usage: 10.51 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['row_na_cnt']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['ID']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('object', []) : 1 | ['ID']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_06', ...]\n",
      "\t\t('int', [])   :  1 | ['row_outlier_cnt']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_06', ...]\n",
      "\t\t('int', [])   :  1 | ['row_outlier_cnt']\n",
      "\t0.1s = Fit runtime\n",
      "\t54 features in original data used to generate 54 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 8.94 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.16s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_macro'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 19523, Val Rows: 2170\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 1799.84s of the 1799.84s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.1/16.1 GB\n",
      "\t0.8519\t = Validation score   (f1_macro)\n",
      "\t17.89s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 1781.91s of the 1781.90s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.2/16.1 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_logloss: 0.652526\tvalid_set's f1_macro: 0.785492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.789\t = Validation score   (f1_macro)\n",
      "\t17.71s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1762.65s of the 1762.65s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.2/15.9 GB\n",
      "\t0.8012\t = Validation score   (f1_macro)\n",
      "\t9.59s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 1752.43s of the 1752.43s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.3/16.0 GB\n",
      "\t0.762\t = Validation score   (f1_macro)\n",
      "\t4.8s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 1747.29s of the 1747.29s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.3/15.4 GB\n",
      "\t0.7664\t = Validation score   (f1_macro)\n",
      "\t10.77s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 1736.21s of the 1736.21s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.7978\t = Validation score   (f1_macro)\n",
      "\t214.13s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 1521.98s of the 1521.98s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.3/17.0 GB\n",
      "\t0.743\t = Validation score   (f1_macro)\n",
      "\t1.37s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 1519.96s of the 1519.96s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.3/16.3 GB\n",
      "\t0.7292\t = Validation score   (f1_macro)\n",
      "\t1.44s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 1518.03s of the 1518.02s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.7992\t = Validation score   (f1_macro)\n",
      "\t18.92s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 1498.72s of the 1498.72s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.0/16.8 GB\n",
      "c:\\Users\\SSAFY\\Downloads\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t0.8517\t = Validation score   (f1_macro)\n",
      "\t349.37s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 1149.30s of the 1149.30s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.4/16.7 GB\n",
      "\t0.7951\t = Validation score   (f1_macro)\n",
      "\t22.29s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1126.43s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.333, 'CatBoost': 0.333, 'NeuralNetTorch': 0.333}\n",
      "\t0.8639\t = Validation score   (f1_macro)\n",
      "\t0.34s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 673.95s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 26612.6 rows/s (2170 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\SSAFY\\Downloads\\Autogluon\\ag_full_model\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] submission 저장: C:\\Users\\SSAFY\\Downloads\\Autogluon\\submission.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 6. Full-train + Test 예측 (전역 대표/드롭 적용 기준, G1은 full-train 기준으로 PCA) ===\n",
    "if test is None:\n",
    "    print('[WARN] test.csv 없음 → 스킵')\n",
    "else:\n",
    "    # 전역 pair-drop 반영된 사본 사용\n",
    "    trn_full = train_pair.copy()\n",
    "    tst_full = test_pair.copy()\n",
    "\n",
    "    # G1 처리: 결측→표준화→PCA(0.95) → PC_G1_* 남기고 원본 삭제 (full-train 기준으로 적합)\n",
    "    g1_cols_full = [c for c in G1 if c in trn_full.columns and c != TARGET_COL]\n",
    "    if len(g1_cols_full) > 0:\n",
    "        imputer_g1_full = SimpleImputer(strategy='median')\n",
    "        scaler_g1_full  = StandardScaler()\n",
    "        pca_g1_full     = PCA(n_components=VAR_RATIO, svd_solver='full')\n",
    "\n",
    "        trn_g1f = imputer_g1_full.fit_transform(trn_full[g1_cols_full])\n",
    "        trn_g1f = scaler_g1_full.fit_transform(trn_g1f)\n",
    "        trn_g1f_pc = pca_g1_full.fit_transform(trn_g1f)\n",
    "\n",
    "        tst_g1f = imputer_g1_full.transform(tst_full[g1_cols_full])\n",
    "        tst_g1f = scaler_g1_full.transform(tst_g1f)\n",
    "        tst_g1f_pc = pca_g1_full.transform(tst_g1f)\n",
    "\n",
    "        n_pc_full = trn_g1f_pc.shape[1]\n",
    "        pc_cols_full = [f'PC_G1_{i+1}' for i in range(n_pc_full)]\n",
    "\n",
    "        trn_pc_full = pd.DataFrame(trn_g1f_pc, columns=pc_cols_full)\n",
    "        tst_pc_full = pd.DataFrame(tst_g1f_pc, columns=pc_cols_full)\n",
    "\n",
    "        trn_full = pd.concat([trn_full.drop(columns=g1_cols_full), trn_pc_full], axis=1)\n",
    "        tst_full = pd.concat([tst_full.drop(columns=g1_cols_full), tst_pc_full], axis=1)\n",
    "    else:\n",
    "        print('[INFO] full-train 기준 G1 컬럼 없음 → PCA 스킵')\n",
    "\n",
    "\n",
    "    # AutoGluon 전체 학습\n",
    "    FULL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    ag_full = TabularDataset(trn_full)\n",
    "    full_predictor = TabularPredictor(label=TARGET_COL, path=str(FULL_DIR), eval_metric='f1_macro')\n",
    "    full_predictor.fit(train_data=ag_full, presets=PRESETS, time_limit=TIME_LIMIT, verbosity=2)\n",
    "\n",
    "    # Test 예측 & 제출 저장\n",
    "    ag_test = TabularDataset(tst_full)\n",
    "    test_pred = full_predictor.predict(ag_test).astype(str)\n",
    "\n",
    "    if ID_COL is not None and ID_COL in test.columns:\n",
    "        sub = pd.DataFrame({ID_COL: test[ID_COL], TARGET_COL: test_pred})\n",
    "    else:\n",
    "        sub = pd.DataFrame({'row_id': np.arange(len(test_pred)), TARGET_COL: test_pred})\n",
    "\n",
    "    sub.to_csv(SUB_PATH, index=False, encoding='utf-8')\n",
    "    print('[INFO] submission 저장:', SUB_PATH.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9351c1",
   "metadata": {},
   "source": [
    "\n",
    "## 전처리 메모 (요약)\n",
    "- **PAIR_GROUPS 대표 선택**: `(-결측률) 정규화 + MI 정규화 + F 정규화` 평균으로 대표 1개 선택(전역 고정). 대표 외 변수는 **삭제**.\n",
    "- **G1 그룹**: 수치 결측 median 대체 → 표준화(`StandardScaler`) → `PCA(n_components=0.95)` → `PC_G1_i`만 유지, G1 원본 **삭제**.\n",
    "- **K-Fold 내 누수 방지**: G1의 `imputer/scaler/PCA`는 **각 폴드의 train 부분으로만 적합** 후 val에 적용. (전역 pair-drop은 기준 통일을 위해 train 전체에서 1회 산정)\n",
    "- AutoGluon은 나머지 결측/범주형 처리를 자체적으로 수행.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
