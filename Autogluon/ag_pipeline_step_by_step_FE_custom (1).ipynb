{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86d2d495",
   "metadata": {},
   "source": [
    "\n",
    "# AutoGluon 분류 파이프라인 (Stratified K-Fold, 셀별 실행 · 함수형태 X)\n",
    "\n",
    "**목표**: `train.csv`(타깃: `target`)로 학습 → `test.csv` 분류 결과 생성  \n",
    "**요구사항 반영**\n",
    "- Stratified K-Fold (OOF & 폴드 성능)\n",
    "- **각 기능별 셀**로 구현(함수 정의 없이 바로 실행)\n",
    "- 데이터 전처리:\n",
    "  - **G1 그룹**: `[\"X_05\",\"X_09\",\"X_20\",\"X_22\",\"X_25\",\"X_51\"]` → 표준화(평균0, 분산1) → **PCA(설명분산 0.95)** → `PC_G1_1, PC_G1_2, ...`만 남기고 G1 원본 6개 변수 **삭제**\n",
    "  - **PAIR_GROUPS(쌍 그룹)**: 각 쌍에서 대표 1개만 남김. **대표 선택 기준**: 결측률↓, mutual information(↑), ANOVA-F(↑)\n",
    "- AutoGluon으로 학습/검증/추론\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔧 Custom Feature Engineering Block (자동 추가)\n",
    "\n",
    "요구사항에 따라 아래 로직을 추가했습니다.\n",
    "\n",
    "- **중요 피처**: `X_08, X_19, X_29, X_46, X_40, X_41, X_49`\n",
    "  - 이 중 **상관관계가 높은 쌍(기본 임계값 |corr| ≥ 0.95)**이 1개 이상 존재하면 → 해당 쌍들에 대해 **곱(product) 피처** 생성 + 각 피처 **제곱(sq) 피처** 생성\n",
    "  - 상관관계가 높은 쌍이 **하나도 없으면** → **제곱(sq) 피처만** 생성\n",
    "- **제거할 피처**: `X_17`, `X_09`, `X_40`, `X_21_sq`\n",
    "  - `X_40`은 중요 피처 목록에도 있었으나, **최종적으로 삭제 요청**에 따라 파생에 사용하지 않고 제거합니다.\n",
    "- **나머지 전처리 로직은 그대로 유지**합니다.\n",
    "- **Idempotent(중복 안전)**: 동일 셀을 여러 번 실행해도 중복 생성되지 않도록 구현했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2c456a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Config loaded.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 1. Imports & Config ===\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_classif, f_classif\n",
    "\n",
    "try:\n",
    "    from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "    AUTOGluon_OK = True\n",
    "except Exception as e:\n",
    "    AUTOGluon_OK = False\n",
    "    print('[WARN] AutoGluon import 실패:', e)\n",
    "\n",
    "# 경로/설정\n",
    "DATA_DIR = Path('.')\n",
    "TRAIN_PATH = DATA_DIR / 'train.csv'\n",
    "TEST_PATH  = DATA_DIR / 'test.csv'\n",
    "TARGET_COL = 'target'\n",
    "ID_COL = None  # 예: 'id' (없으면 None)\n",
    "\n",
    "N_FOLDS = 5\n",
    "RANDOM_STATE = 42\n",
    "PRESETS = 'medium_quality'\n",
    "TIME_LIMIT = 1800  # 초(폴드당)\n",
    "\n",
    "SAVE_ROOT = Path('./ag_cv_models')\n",
    "OOF_PATH  = Path('./oof_predictions.csv')\n",
    "FULL_DIR  = Path('./ag_full_model')\n",
    "SUB_PATH  = Path('./submission.csv')\n",
    "\n",
    "# 그룹 정의\n",
    "G1 = [\"X_05\",\"X_09\",\"X_20\",\"X_22\",\"X_25\",\"X_51\"]\n",
    "VAR_RATIO = 0.95  # PCA 설명분산 임계\n",
    "\n",
    "# 일단 여기선 별로 사용이 안 됨.\n",
    "# 쌍 변수 그룹 (필요시 더 추가)\n",
    "PAIR_GROUPS = [\n",
    "    [\"X_04\",\"X_39\"],\n",
    "    [\"X_06\",\"X_45\"],\n",
    "    ####################\n",
    "    [\"X_10\",\"X_17\"],\n",
    "    [\"X_07\",\"X_33\"],\n",
    "    [\"X_12\",\"X_21\"],\n",
    "    [\"X_26\",\"X_30\"],\n",
    "    [\"X_38\",\"X_47\"],\n",
    "    \n",
    "    # [\"X_05\",\"X_25\"], ...  # 예시로 더 넣을 수 있음\n",
    "]\n",
    "\n",
    "# 재현성\n",
    "random.seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print('[INFO] Config loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📊 Feature Importance (after training)\n",
    "- AutoGluon `TabularPredictor`가 있으면 `predictor.feature_importance(...)` 사용\n",
    "- 그렇지 않으면, `feature_importances_` 속성이 있는 트리계열 모델에서 중요도 추출\n",
    "- 상위 30개를 가로바(barh)로 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FI] 중요도를 계산할 수 있는 학습 객체를 찾지 못했습니다.\n"
     ]
    }
   ],
   "source": [
    "# === Feature Importance (Generic) ===\n",
    "import inspect\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 후보 데이터/라벨명\n",
    "label_candidates = [\"target\", \"TARGET\", \"label\", \"Label\", \"y\"]\n",
    "df_candidates = [\"train_pair\", \"train\", \"train_df\", \"df_train\"]\n",
    "X_candidates  = [\"X_train\", \"X_tr\", \"X\"]\n",
    "model_candidates = [\"predictor\", \"model\", \"clf\", \"estimator\"]\n",
    "\n",
    "def _detect_label_name(df):\n",
    "    for col in df.columns:\n",
    "        for lab in label_candidates:\n",
    "            if col.lower() == lab.lower():\n",
    "                return col\n",
    "    return None\n",
    "\n",
    "def _get_first_existing(names):\n",
    "    g = globals()\n",
    "    for nm in names:\n",
    "        if nm in g:\n",
    "            return nm, g[nm]\n",
    "    return None, None\n",
    "\n",
    "imp_df = None\n",
    "used_route = None\n",
    "\n",
    "# 1) Try AutoGluon TabularPredictor\n",
    "name_pred, predictor = _get_first_existing([\"predictor\"])\n",
    "if predictor is not None:\n",
    "    try:\n",
    "        # Check class name to avoid false positives\n",
    "        clsname = predictor.__class__.__name__.lower()\n",
    "        if \"tabularpredictor\" in clsname and hasattr(predictor, \"feature_importance\"):\n",
    "            # find a labeled df\n",
    "            df_name, df_obj = _get_first_existing(df_candidates)\n",
    "            if isinstance(df_obj, pd.DataFrame):\n",
    "                label_col = _detect_label_name(df_obj)\n",
    "                if label_col is not None:\n",
    "                    fi = predictor.feature_importance(df_obj, silent=True)\n",
    "                    # AutoGluon returns a DataFrame with 'importance' column (usually 'importance' or 'importance_abs')\n",
    "                    # Normalize to a standard format\n",
    "                    if isinstance(fi, pd.DataFrame):\n",
    "                        # try common column names\n",
    "                        value_col = None\n",
    "                        for c in [\"importance\", \"importance_value\", \"importance_mean\", \"importance_abs\"]:\n",
    "                            if c in fi.columns:\n",
    "                                value_col = c\n",
    "                                break\n",
    "                        if value_col is None and fi.shape[1] >= 1:\n",
    "                            value_col = fi.columns[0]\n",
    "                        imp_df = fi[[value_col]].copy()\n",
    "                        imp_df.columns = [\"importance\"]\n",
    "                    else:\n",
    "                        # Unexpected; coerce\n",
    "                        imp_df = pd.DataFrame({\"importance\": fi})\n",
    "                    used_route = f\"AutoGluon({name_pred}) on {df_name}\"\n",
    "    except Exception as e:\n",
    "        print(\"[FI] AutoGluon route skipped:\", repr(e))\n",
    "\n",
    "# 2) Try any model with feature_importances_\n",
    "if imp_df is None:\n",
    "    # Pick a model object exposing feature_importances_\n",
    "    model_obj = None\n",
    "    for k, v in list(globals().items()):\n",
    "        try:\n",
    "            if hasattr(v, \"feature_importances_\"):\n",
    "                model_obj = v\n",
    "                model_name = k\n",
    "                break\n",
    "        except Exception:\n",
    "            continue\n",
    "    if model_obj is not None:\n",
    "        # Determine feature names\n",
    "        feat_names = None\n",
    "        # Prefer X_train with columns\n",
    "        Xn, Xobj = _get_first_existing([\"X_train\", \"X_tr\"])\n",
    "        if isinstance(Xobj, pd.DataFrame):\n",
    "            feat_names = list(Xobj.columns)\n",
    "        if feat_names is None:\n",
    "            # fallback: from train_pair excluding label\n",
    "            df_name, df_obj = _get_first_existing(df_candidates)\n",
    "            if isinstance(df_obj, pd.DataFrame):\n",
    "                label_col = _detect_label_name(df_obj)\n",
    "                if label_col is not None:\n",
    "                    feat_names = [c for c in df_obj.columns if c != label_col]\n",
    "        # Build df\n",
    "        try:\n",
    "            vals = np.array(model_obj.feature_importances_).ravel()\n",
    "            if feat_names is None:\n",
    "                feat_names = [f\"f{i}\" for i in range(len(vals))]\n",
    "            imp_df = pd.DataFrame({\"feature\": feat_names, \"importance\": vals}).set_index(\"feature\")\n",
    "            used_route = f\"{model_name}.feature_importances_\"\n",
    "        except Exception as e:\n",
    "            print(\"[FI] feature_importances_ route skipped:\", repr(e))\n",
    "\n",
    "# 3) If nothing worked, exit gracefully\n",
    "if imp_df is None or imp_df.empty:\n",
    "    print(\"[FI] 중요도를 계산할 수 있는 학습 객체를 찾지 못했습니다.\")\n",
    "else:\n",
    "    # Sort and take top 30\n",
    "    imp_df = imp_df.sort_values(\"importance\", ascending=False).head(30)\n",
    "    display(imp_df)\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, max(4, 0.3*len(imp_df))))\n",
    "    imp_df.sort_values(\"importance\").plot(kind=\"barh\", legend=False)\n",
    "    plt.title(f\"Feature Importance (top {len(imp_df)})\\n{used_route if used_route else ''}\")\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6baf9e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] train.shape = (21693, 54)\n",
      "[INFO] test.shape  = (15004, 53)\n",
      "[INFO] 클래스 수: 21\n",
      "[INFO] 예시: ['0', '1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '3', '4', '5', '6', '7', '8']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 2. Load train/test, 타깃 점검 ===\n",
    "assert TRAIN_PATH.exists(), f'train.csv가 {TRAIN_PATH} 에 없습니다.'\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "print('[INFO] train.shape =', train.shape)\n",
    "assert TARGET_COL in train.columns, f'{TARGET_COL} 컬럼이 train에 없습니다.'\n",
    "\n",
    "if TEST_PATH.exists():\n",
    "    test = pd.read_csv(TEST_PATH)\n",
    "    print('[INFO] test.shape  =', test.shape)\n",
    "else:\n",
    "    test = None\n",
    "    print('[WARN] test.csv 가 없어 추론 셀은 스킵될 수 있습니다.')\n",
    "\n",
    "# 타깃 클린업(문자열 통일 + 결측 제거)\n",
    "y = train[TARGET_COL].astype(str)\n",
    "na_mask = y.isna() | (y.str.lower()=='nan') | (y=='None') | (y=='')\n",
    "if na_mask.any():\n",
    "    print(f'[WARN] 타깃 결측/유사결측 {na_mask.sum()}건 발견 → 해당 행 제거')\n",
    "    train = train.loc[~na_mask].reset_index(drop=True)\n",
    "    y = train[TARGET_COL].astype(str)\n",
    "\n",
    "ALL_CLASSES = sorted(y.unique())\n",
    "print('[INFO] 클래스 수:', len(ALL_CLASSES))\n",
    "print('[INFO] 예시:', ALL_CLASSES[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06254dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 대표 선택 결과: {('X_04', 'X_39'): 'X_04', ('X_06', 'X_45'): 'X_06', ('X_10', 'X_17'): 'X_17', ('X_07', 'X_33'): 'X_07', ('X_12', 'X_21'): 'X_12', ('X_26', 'X_30'): 'X_26', ('X_38', 'X_47'): 'X_38'}\n",
      "[INFO] 삭제 대상(대표 아닌 변수): ['X_39', 'X_45', 'X_10', 'X_33', 'X_21', 'X_30', 'X_47']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 3. PAIR_GROUPS 전역 대표 변수 선택 (train 기준) ===\n",
    "# - 결측률 낮을수록 좋음\n",
    "# - mutual information 높을수록 좋음\n",
    "# - ANOVA-F 높을수록 좋음\n",
    "# 점수 = (-결측률) 정규화 + (MI) 정규화 + (F) 정규화  (각 1/3 가중)\n",
    "\n",
    "# 고상관 쌍(PAIR_GROUPS)의 각 쌍에서 대표 변수 1개만 남기기 위해, 두 변수의 “품질+유용성”을 점수로 계산해 비교하는 로직\n",
    "# -> 39, 45 삭제\n",
    "\n",
    "rep_map = {}         # {('X_04','X_39'): 'X_04', ...}\n",
    "to_drop_pairs = []   # 대표가 아닌 변수 목록\n",
    "\n",
    "# 스코어 계산을 위한 임시 데이터(결측 채움: 수치 median) 구성\n",
    "num_cols = train.drop(columns=[TARGET_COL]).select_dtypes(include=[np.number]).columns.tolist()\n",
    "tmp = train.copy()\n",
    "\n",
    "if len(num_cols) > 0:\n",
    "    imputer_num = SimpleImputer(strategy='median')\n",
    "    tmp[num_cols] = imputer_num.fit_transform(tmp[num_cols])\n",
    "\n",
    "# 각 쌍에 대해 점수 산출\n",
    "for pair in PAIR_GROUPS:\n",
    "    pair = [c for c in pair if c in train.columns and c != TARGET_COL]\n",
    "    if len(pair) != 2:\n",
    "        print(f'[WARN] 쌍 {pair} 중 유효 컬럼이 2개가 아닙니다. 스킵.')\n",
    "        continue\n",
    "\n",
    "    cols_ok = []\n",
    "    scores = []\n",
    "    for col in pair:\n",
    "        # 결측률 (원본 기준)\n",
    "        miss_rate = train[col].isna().mean()\n",
    "\n",
    "        # MI/F는 수치형에서만 바로 계산. 수치가 아닐 경우 임시 변환(범주→순번 인코딩)\n",
    "        s = tmp[col]\n",
    "        if not np.issubdtype(s.dtype, np.number):\n",
    "            # 간단 라벨 인코딩\n",
    "            s = s.astype('category').cat.codes\n",
    "\n",
    "        X_ = s.values.reshape(-1, 1)\n",
    "        y_ = train[TARGET_COL].astype(str).values\n",
    "\n",
    "        # MI (discrete target)\n",
    "        try:\n",
    "            mi = mutual_info_classif(X_, y_, discrete_features=True, random_state=RANDOM_STATE)[0]\n",
    "        except Exception:\n",
    "            mi = 0.0\n",
    "\n",
    "        # ANOVA-F\n",
    "        try:\n",
    "            f, _ = f_classif(X_, pd.factorize(y_)[0])\n",
    "            f = float(f[0])\n",
    "        except Exception:\n",
    "            f = 0.0\n",
    "\n",
    "        cols_ok.append(col)\n",
    "        scores.append({'col': col, 'miss': miss_rate, 'mi': mi, 'f': f})\n",
    "\n",
    "    if len(scores) != 2:\n",
    "        print(f'[WARN] {pair} 점수 계산 실패(스킵)')\n",
    "        continue\n",
    "\n",
    "    # 정규화\n",
    "    miss_vals = np.array([s['miss'] for s in scores])\n",
    "    mi_vals   = np.array([s['mi']   for s in scores])\n",
    "    f_vals    = np.array([s['f']    for s in scores])\n",
    "\n",
    "    def norm(v):\n",
    "        v = v.astype(float)\n",
    "        if np.allclose(v.max(), v.min()):\n",
    "            return np.zeros_like(v)\n",
    "        return (v - v.min()) / (v.max() - v.min())\n",
    "\n",
    "    miss_n = norm(1 - miss_vals)  # 결측률 낮을수록↑ → (1 - miss)\n",
    "    mi_n   = norm(mi_vals)\n",
    "    f_n    = norm(f_vals)\n",
    "\n",
    "    final  = (miss_n + mi_n + f_n) / 3.0\n",
    "    best_idx = int(np.argmax(final))\n",
    "    rep = scores[best_idx]['col']\n",
    "    rep_map[tuple(pair)] = rep\n",
    "\n",
    "    drop_cols = [c for c in pair if c != rep]\n",
    "    to_drop_pairs.extend(drop_cols)\n",
    "\n",
    "print('[INFO] 대표 선택 결과:', rep_map)\n",
    "print('[INFO] 삭제 대상(대표 아닌 변수):', to_drop_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed51c973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] train_pair.shape = (21693, 47)\n",
      "[INFO] test_pair.shape  = (15004, 46)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 4. 대표 아닌 변수 삭제(전역) ===\n",
    "train_pair = train.drop(columns=[c for c in to_drop_pairs if c in train.columns], errors='ignore').copy()\n",
    "if test is not None:\n",
    "    test_pair = test.drop(columns=[c for c in to_drop_pairs if c in test.columns], errors='ignore').copy()\n",
    "else:\n",
    "    test_pair = None\n",
    "\n",
    "print('[INFO] train_pair.shape =', train_pair.shape)\n",
    "if test_pair is not None:\n",
    "    print('[INFO] test_pair.shape  =', test_pair.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4502f01b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'X_01', 'X_02', 'X_03', 'X_04', 'X_05', 'X_06', 'X_07', 'X_08',\n",
       "       'X_09', 'X_11', 'X_12', 'X_13', 'X_14', 'X_15', 'X_16', 'X_17', 'X_18',\n",
       "       'X_19', 'X_20', 'X_22', 'X_23', 'X_24', 'X_25', 'X_26', 'X_27', 'X_28',\n",
       "       'X_29', 'X_31', 'X_32', 'X_34', 'X_35', 'X_36', 'X_37', 'X_38', 'X_40',\n",
       "       'X_41', 'X_42', 'X_43', 'X_44', 'X_46', 'X_48', 'X_49', 'X_50', 'X_51',\n",
       "       'X_52', 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pair.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37efda8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 파생 피처 추가 완료. train_pair: (21693, 68) | test_pair: (15004, 67)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 파생 피처 추가 (함수 없이 바로 적용) ===\n",
    "_base_cols = [c for c in train_pair.columns if c != TARGET_COL]\n",
    "_num_cols  = train_pair[_base_cols].select_dtypes(include=[float, int]).columns\n",
    "\n",
    "# 1) 행 단위 요약\n",
    "train_pair['row_na_cnt'] = train_pair[_base_cols].isna().sum(axis=1)\n",
    "if test_pair is not None:\n",
    "    test_pair['row_na_cnt'] = test_pair[_base_cols].isna().sum(axis=1)\n",
    "\n",
    "train_pair['row_num_mean'] = train_pair[_num_cols].mean(axis=1)\n",
    "train_pair['row_num_std']  = train_pair[_num_cols].std(axis=1)\n",
    "if test_pair is not None:\n",
    "    test_pair['row_num_mean'] = test_pair[_num_cols].mean(axis=1)\n",
    "    test_pair['row_num_std']  = test_pair[_num_cols].std(axis=1)\n",
    "\n",
    "if len(_num_cols) > 0:\n",
    "    _Q1  = train_pair[_num_cols].quantile(0.25)\n",
    "    _Q3  = train_pair[_num_cols].quantile(0.75)\n",
    "    _IQR = (_Q3 - _Q1).replace(0, 1e-12)\n",
    "\n",
    "    _below_tr = (train_pair[_num_cols] < (_Q1 - 1.5 * _IQR))\n",
    "    _above_tr = (train_pair[_num_cols] > (_Q3 + 1.5 * _IQR))\n",
    "    train_pair['row_outlier_cnt'] = (_below_tr | _above_tr).sum(axis=1)\n",
    "\n",
    "    if test_pair is not None:\n",
    "        _below_te = (test_pair[_num_cols] < (_Q1 - 1.5 * _IQR))\n",
    "        _above_te = (test_pair[_num_cols] > (_Q3 + 1.5 * _IQR))\n",
    "        test_pair['row_outlier_cnt'] = (_below_te | _above_te).sum(axis=1)\n",
    "\n",
    "# 2) G1 그룹 통계\n",
    "_g1_cols = [c for c in G1 if c in train_pair.columns]\n",
    "if len(_g1_cols) > 0:\n",
    "    train_pair['G1_mean']  = train_pair[_g1_cols].mean(axis=1)\n",
    "    train_pair['G1_std']   = train_pair[_g1_cols].std(axis=1)\n",
    "    train_pair['G1_min']   = train_pair[_g1_cols].min(axis=1)\n",
    "    train_pair['G1_max']   = train_pair[_g1_cols].max(axis=1)\n",
    "    train_pair['G1_range'] = train_pair['G1_max'] - train_pair['G1_min']\n",
    "\n",
    "    if test_pair is not None:\n",
    "        test_pair['G1_mean']  = test_pair[_g1_cols].mean(axis=1)\n",
    "        test_pair['G1_std']   = test_pair[_g1_cols].std(axis=1)\n",
    "        test_pair['G1_min']   = test_pair[_g1_cols].min(axis=1)\n",
    "        test_pair['G1_max']   = test_pair[_g1_cols].max(axis=1)\n",
    "        test_pair['G1_range'] = test_pair['G1_max'] - test_pair['G1_min']\n",
    "\n",
    "\n",
    "# 3) 비선형 파생 (예: X_05, X_09)\n",
    "if 'X_05' in train_pair.columns and 'X_09' in train_pair.columns:\n",
    "    train_pair['X05_div_X09'] = train_pair['X_05'] / (train_pair['X_09'].replace(0, 1e-12) + 1e-12)\n",
    "    train_pair['X05_mul_X09'] = train_pair['X_05'] * train_pair['X_09']\n",
    "    if test_pair is not None:\n",
    "        test_pair['X05_div_X09'] = test_pair['X_05'] / (test_pair['X_09'].replace(0, 1e-12) + 1e-12)\n",
    "        test_pair['X05_mul_X09'] = test_pair['X_05'] * test_pair['X_09']\n",
    "\n",
    "if 'X_05' in train_pair.columns:\n",
    "    train_pair['X05_sq'] = train_pair['X_05'] ** 2\n",
    "    if test_pair is not None:\n",
    "        test_pair['X05_sq'] = test_pair['X_05'] ** 2\n",
    "\n",
    "if 'X_09' in train_pair.columns:\n",
    "    train_pair['X09_sq'] = train_pair['X_09'] ** 2\n",
    "    if test_pair is not None:\n",
    "        test_pair['X09_sq'] = test_pair['X_09'] ** 2\n",
    "\n",
    "\n",
    "if 'X_20' in train_pair.columns and 'X_22' in train_pair.columns:\n",
    "    train_pair['X20_div_X22'] = train_pair['X_20'] / (train_pair['X_22'].replace(0, 1e-12) + 1e-12)\n",
    "    train_pair['X20_mul_X22'] = train_pair['X_20'] * train_pair['X_22']\n",
    "    if test_pair is not None:\n",
    "        test_pair['X20_div_X22'] = test_pair['X_20'] / (test_pair['X_22'].replace(0, 1e-12) + 1e-12)\n",
    "        test_pair['X20_mul_X22'] = test_pair['X_20'] * test_pair['X_22']\n",
    "\n",
    "if 'X_20' in train_pair.columns:\n",
    "    train_pair['X20_sq'] = train_pair['X_20'] ** 2\n",
    "    if test_pair is not None:\n",
    "        test_pair['X20_sq'] = test_pair['X_20'] ** 2\n",
    "\n",
    "if 'X_22' in train_pair.columns:\n",
    "    train_pair['X22_sq'] = train_pair['X_22'] ** 2\n",
    "    if test_pair is not None:\n",
    "        test_pair['X22_sq'] = test_pair['X_22'] ** 2\n",
    "\n",
    "\n",
    "\n",
    "if 'X_25' in train_pair.columns and 'X_51' in train_pair.columns:\n",
    "    train_pair['X25_div_X51'] = train_pair['X_25'] / (train_pair['X_51'].replace(0, 1e-12) + 1e-12)\n",
    "    train_pair['X25_mul_X51'] = train_pair['X_25'] * train_pair['X_51']\n",
    "    if test_pair is not None:\n",
    "        test_pair['X25_div_X51'] = test_pair['X_25'] / (test_pair['X_51'].replace(0, 1e-12) + 1e-12)\n",
    "        test_pair['X25_mul_X51'] = test_pair['X_25'] * test_pair['X_51']\n",
    "\n",
    "if 'X_25' in train_pair.columns:\n",
    "    train_pair['X25_sq'] = train_pair['X_25'] ** 2\n",
    "    if test_pair is not None:\n",
    "        test_pair['X25_sq'] = test_pair['X_25'] ** 2\n",
    "\n",
    "if 'X_51' in train_pair.columns:\n",
    "    train_pair['X51_sq'] = train_pair['X_51'] ** 2\n",
    "    if test_pair is not None:\n",
    "        test_pair['X51_sq'] = test_pair['X_51'] ** 2\n",
    "\n",
    "\n",
    "\n",
    "# import itertools\n",
    "\n",
    "# # 1) pairwise 조합 (곱/나눗셈)\n",
    "# for f1, f2 in itertools.combinations(G1, 2):\n",
    "#     if f1 in train_pair.columns and f2 in train_pair.columns:\n",
    "#         # 나눗셈 (f1 / f2)\n",
    "#         train_pair[f\"{f1}_div_{f2}\"] = train_pair[f1] / (train_pair[f2].replace(0, 1e-12) + 1e-12)\n",
    "#         if test_pair is not None:\n",
    "#             test_pair[f\"{f1}_div_{f2}\"] = test_pair[f1] / (test_pair[f2].replace(0, 1e-12) + 1e-12)\n",
    "\n",
    "#         # 곱 (f1 * f2)\n",
    "#         train_pair[f\"{f1}_mul_{f2}\"] = train_pair[f1] * train_pair[f2]\n",
    "#         if test_pair is not None:\n",
    "#             test_pair[f\"{f1}_mul_{f2}\"] = test_pair[f1] * test_pair[f2]\n",
    "\n",
    "# # 2) 각 feature에 대해 제곱\n",
    "# for f in G1:\n",
    "#     if f in train_pair.columns:\n",
    "#         train_pair[f\"{f}_sq\"] = train_pair[f] ** 2\n",
    "#         if test_pair is not None:\n",
    "#             test_pair[f\"{f}_sq\"] = test_pair[f] ** 2\n",
    "\n",
    "print('[INFO] 파생 피처 추가 완료. train_pair:', train_pair.shape, '| test_pair:', (None if test_pair is None else test_pair.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a024cdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === Custom Feature Engineering (요구사항 반영) ===\n",
    "# from itertools import combinations\n",
    "\n",
    "# # 중요 피처\n",
    "# important_cols = [\"X_08\", \"X_19\", \"X_29\", \"X_46\", \"X_40\", \"X_41\", \"X_49\"]\n",
    "# # 제거할 피처\n",
    "# # drop_cols = [\"X_17\", \"X_09\", \"X_40\", \"X_21_sq\"]\n",
    "\n",
    "# # drop_cols 제외 후 실제 존재하는 중요 피처만 필터링\n",
    "# exist_importants = [c for c in important_cols if c in train_pair.columns and c not in drop_cols]\n",
    "\n",
    "# # 상관관계 확인 (임계값: 0.95)\n",
    "# high_corr_pairs = []\n",
    "# if len(exist_importants) >= 2:\n",
    "#     corr = train_pair[exist_importants].corr().abs()\n",
    "#     for a, b in combinations(exist_importants, 2):\n",
    "#         if corr.loc[a, b] >= 0.7:\n",
    "#             high_corr_pairs.append((a, b))\n",
    "\n",
    "# # 제곱 피처 생성\n",
    "# for c in exist_importants:\n",
    "#     new_col = f\"{c}_sq\"\n",
    "#     if new_col not in train_pair.columns:\n",
    "#         train_pair[new_col] = train_pair[c] ** 2\n",
    "#         test_pair[new_col] = test_pair[c] ** 2\n",
    "\n",
    "# # 곱 피처 생성 (상관관계 높은 경우만)\n",
    "# if high_corr_pairs:\n",
    "#     for a, b in high_corr_pairs:\n",
    "#         new_col = f\"{a}x{b}\"\n",
    "#         if new_col not in train_pair.columns:\n",
    "#             train_pair[new_col] = train_pair[a] * train_pair[b]\n",
    "#             test_pair[new_col] = test_pair[a] * test_pair[b]\n",
    "\n",
    "# if high_corr_pairs:\n",
    "#     for a, b in high_corr_pairs:\n",
    "#         new_col = f\"{a}x{b}\"\n",
    "#         if new_col not in train_pair.columns:\n",
    "#             train_pair[new_col] = train_pair[a] * train_pair[b]\n",
    "#             test_pair[new_col] = test_pair[a] * test_pair[b]\n",
    "            \n",
    "# # # 불필요 피처 제거\n",
    "# # for c in drop_cols:\n",
    "# #     if c in train_pair.columns:\n",
    "# #         train_pair.drop(columns=[c], inplace=True)\n",
    "# #     if c in test_pair.columns:\n",
    "# #         test_pair.drop(columns=[c], inplace=True)\n",
    "\n",
    "# print(\"[Custom FE] 제곱 추가:\", [f\"{c}_sq\" for c in exist_importants])\n",
    "# print(\"[Custom FE] 곱 추가:\", [f\"{a}x{b}\" for a, b in high_corr_pairs])\n",
    "# print(\"[Custom FE] 제거:\", drop_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62c97dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Custom FE] row_neg_count 추가 완료\n",
      "[Custom FE] scaling 추가: ['X_11', 'X_19', 'X_37', 'X_40']\n",
      "[Custom FE] squares: ['X_08_sq', 'X_19_sq', 'X_29_sq', 'X_46_sq', 'X_41_sq', 'X_49_sq']\n",
      "[Custom FE] products: ['X_08xX_19', 'X_08xX_29']\n",
      "[Custom FE] ratios: ['X_08_div_X_19', 'X_08_div_X_29', 'X_19_div_X_08', 'X_29_div_X_08']\n",
      "[Custom FE] 제거: ['X_17', 'X_09', 'X_40', 'X_21_sq']\n"
     ]
    }
   ],
   "source": [
    "# === Custom FE (inline, k-fold 직전) ===\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ---- 설정 ----\n",
    "important_cols = [\"X_08\", \"X_19\", \"X_29\", \"X_46\", \"X_40\", \"X_41\", \"X_49\"]\n",
    "drop_cols = [\"X_17\", \"X_09\", \"X_40\", \"X_21_sq\"]\n",
    "corr_threshold = 0.7\n",
    "scale_cols = [\"X_11\", \"X_19\", \"X_37\", \"X_40\"]\n",
    "label_candidates = [\"target\", \"TARGET\", \"label\", \"Label\", \"y\"]\n",
    "\n",
    "# ---- 유틸 ----\n",
    "def _safe_ratio(a: pd.Series, b: pd.Series, eps: float = 1e-9) -> pd.Series:\n",
    "    # 0/0 또는 분모 0 대응\n",
    "    return a / (b.replace(0, np.nan))\n",
    "    # 필요시: return a / (b + eps)\n",
    "\n",
    "def _numeric_feature_cols(df: pd.DataFrame):\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    to_exclude = []\n",
    "    for c in num_cols:\n",
    "        for lab in label_candidates:\n",
    "            if c.lower() == lab.lower():\n",
    "                to_exclude.append(c)\n",
    "                break\n",
    "    # id 류 컬럼 제외(있다면)\n",
    "    for c in num_cols:\n",
    "        if c.lower() in (\"id\", \"idx\"):\n",
    "            to_exclude.append(c)\n",
    "    to_exclude = list(dict.fromkeys(to_exclude))\n",
    "    return [c for c in num_cols if c not in to_exclude]\n",
    "\n",
    "# ---- 음수 개수 피처 ----\n",
    "_numeric_cols_train = _numeric_feature_cols(train_pair)\n",
    "train_pair[\"row_neg_count\"] = (train_pair[_numeric_cols_train] < 0).sum(axis=1)\n",
    "\n",
    "_numeric_cols_test = [c for c in _numeric_cols_train if c in test_pair.columns]\n",
    "test_pair[\"row_neg_count\"] = (test_pair[_numeric_cols_test] < 0).sum(axis=1)\n",
    "\n",
    "# ---- 스케일링 ----\n",
    "for c in scale_cols:\n",
    "    if c in train_pair.columns:\n",
    "        scaler = StandardScaler()\n",
    "        train_pair[f\"{c}_std\"] = scaler.fit_transform(train_pair[[c]]).ravel()\n",
    "        if c in test_pair.columns:\n",
    "            test_pair[f\"{c}_std\"] = scaler.transform(test_pair[[c]]).ravel()\n",
    "\n",
    "# ---- 중요 피처 파생 ----\n",
    "exist_importants = [c for c in important_cols if c in train_pair.columns and c not in drop_cols]\n",
    "\n",
    "# 상관관계 계산\n",
    "high_corr_pairs = []\n",
    "if len(exist_importants) >= 2:\n",
    "    corr = train_pair[exist_importants].corr().abs()\n",
    "    for a, b in combinations(exist_importants, 2):\n",
    "        if corr.loc[a, b] >= corr_threshold:\n",
    "            high_corr_pairs.append((a, b))\n",
    "\n",
    "# 제곱\n",
    "for c in exist_importants:\n",
    "    sq = f\"{c}_sq\"\n",
    "    if sq not in train_pair.columns and np.issubdtype(train_pair[c].dtype, np.number):\n",
    "        train_pair[sq] = train_pair[c] ** 2\n",
    "        if c in test_pair.columns:\n",
    "            test_pair[sq] = test_pair[c] ** 2\n",
    "\n",
    "# 곱/나눗셈 (고상관일 때)\n",
    "if high_corr_pairs:\n",
    "    for a, b in high_corr_pairs:\n",
    "        prod = f\"{a}x{b}\"\n",
    "        if prod not in train_pair.columns:\n",
    "            train_pair[prod] = train_pair[a] * train_pair[b]\n",
    "            if (a in test_pair.columns) and (b in test_pair.columns):\n",
    "                test_pair[prod] = test_pair[a] * test_pair[b]\n",
    "        # ratio a/b, b/a\n",
    "        div1 = f\"{a}_div_{b}\"\n",
    "        div2 = f\"{b}_div_{a}\"\n",
    "        if div1 not in train_pair.columns:\n",
    "            train_pair[div1] = _safe_ratio(train_pair[a], train_pair[b])\n",
    "            if (a in test_pair.columns) and (b in test_pair.columns):\n",
    "                test_pair[div1] = _safe_ratio(test_pair[a], test_pair[b])\n",
    "        if div2 not in train_pair.columns:\n",
    "            train_pair[div2] = _safe_ratio(train_pair[b], train_pair[a])\n",
    "            if (a in test_pair.columns) and (b in test_pair.columns):\n",
    "                test_pair[div2] = _safe_ratio(test_pair[b], test_pair[a])\n",
    "\n",
    "# ---- 컬럼 제거 ----\n",
    "for c in drop_cols:\n",
    "    if c in train_pair.columns:\n",
    "        train_pair.drop(columns=[c], inplace=True)\n",
    "    if c in test_pair.columns:\n",
    "        test_pair.drop(columns=[c], inplace=True)\n",
    "\n",
    "print(\"[Custom FE] row_neg_count 추가 완료\")\n",
    "print(\"[Custom FE] scaling 추가:\", [c for c in scale_cols if f\"{c}_std\" in train_pair.columns])\n",
    "print(\"[Custom FE] squares:\", [f\"{c}_sq\" for c in exist_importants])\n",
    "print(\"[Custom FE] products:\", [f\"{a}x{b}\" for a,b in high_corr_pairs])\n",
    "print(\"[Custom FE] ratios:\", [f\"{a}_div_{b}\" for a,b in high_corr_pairs] + [f\"{b}_div_{a}\" for a,b in high_corr_pairs])\n",
    "print(\"[Custom FE] 제거:\", [c for c in drop_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d81a6bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'X_01', 'X_02', 'X_03', 'X_04', 'X_05', 'X_06', 'X_07', 'X_08',\n",
       "       'X_11', 'X_12', 'X_13', 'X_14', 'X_15', 'X_16', 'X_18', 'X_19', 'X_20',\n",
       "       'X_22', 'X_23', 'X_24', 'X_25', 'X_26', 'X_27', 'X_28', 'X_29', 'X_31',\n",
       "       'X_32', 'X_34', 'X_35', 'X_36', 'X_37', 'X_38', 'X_41', 'X_42', 'X_43',\n",
       "       'X_44', 'X_46', 'X_48', 'X_49', 'X_50', 'X_51', 'X_52', 'target',\n",
       "       'row_na_cnt', 'row_num_mean', 'row_num_std', 'row_outlier_cnt',\n",
       "       'G1_mean', 'G1_std', 'G1_min', 'G1_max', 'G1_range', 'X05_div_X09',\n",
       "       'X05_mul_X09', 'X05_sq', 'X09_sq', 'X20_div_X22', 'X20_mul_X22',\n",
       "       'X20_sq', 'X22_sq', 'X25_div_X51', 'X25_mul_X51', 'X25_sq', 'X51_sq',\n",
       "       'row_neg_count', 'X_11_std', 'X_19_std', 'X_37_std', 'X_40_std',\n",
       "       'X_08_sq', 'X_19_sq', 'X_29_sq', 'X_46_sq', 'X_41_sq', 'X_49_sq',\n",
       "       'X_08xX_19', 'X_08_div_X_19', 'X_19_div_X_08', 'X_08xX_29',\n",
       "       'X_08_div_X_29', 'X_29_div_X_08'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pair.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f88b4e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== [Fold 1/5] =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_cv_models\\fold_1\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.9\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          28\n",
      "Memory Avail:       21.84 GB / 31.69 GB (68.9%)\n",
      "Disk Space Avail:   837.35 GB / 953.01 GB (87.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 1800s\n",
      "AutoGluon will save models to \"c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\ag_cv_models\\fold_1\"\n",
      "Train Data Rows:    17354\n",
      "Train Data Columns: 77\n",
      "Tuning Data Rows:    4339\n",
      "Tuning Data Columns: 77\n",
      "Label Column:       target\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
      "\tFirst 10 (of 21) unique label values:  [np.int64(0), np.int64(20), np.int64(1), np.int64(19), np.int64(15), np.int64(8), np.int64(12), np.int64(4), np.int64(5), np.int64(11)]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 21\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    22361.17 MB\n",
      "\tTrain Data (Original)  Memory Usage: 13.99 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['row_na_cnt']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['ID']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('object', []) : 1 | ['ID']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 73 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_06', ...]\n",
      "\t\t('int', [])   :  2 | ['row_outlier_cnt', 'row_neg_count']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 73 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_06', ...]\n",
      "\t\t('int', [])   :  2 | ['row_outlier_cnt', 'row_neg_count']\n",
      "\t0.2s = Fit runtime\n",
      "\t75 features in original data used to generate 75 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 12.41 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.21s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_macro'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 1799.79s of the 1799.79s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.1/21.6 GB\n",
      "\t0.8649\t = Validation score   (f1_macro)\n",
      "\t20.69s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 1779.03s of the 1779.03s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.2/21.6 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_logloss: 0.546444\tvalid_set's f1_macro: 0.823349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.8263\t = Validation score   (f1_macro)\n",
      "\t24.22s\t = Training   runtime\n",
      "\t0.44s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1752.43s of the 1752.43s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.2/21.7 GB\n",
      "\t0.8286\t = Validation score   (f1_macro)\n",
      "\t13.32s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 1738.14s of the 1738.14s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/21.7 GB\n",
      "\t0.8014\t = Validation score   (f1_macro)\n",
      "\t4.19s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 1733.70s of the 1733.70s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/21.4 GB\n",
      "\t0.7985\t = Validation score   (f1_macro)\n",
      "\t9.42s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 1724.03s of the 1724.03s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.8156\t = Validation score   (f1_macro)\n",
      "\t269.12s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 1454.84s of the 1454.84s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/21.2 GB\n",
      "\t0.7804\t = Validation score   (f1_macro)\n",
      "\t1.12s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 1453.26s of the 1453.26s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/20.8 GB\n",
      "\t0.7713\t = Validation score   (f1_macro)\n",
      "\t1.16s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 1451.73s of the 1451.73s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.8262\t = Validation score   (f1_macro)\n",
      "\t18.52s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 1432.85s of the 1432.85s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.0/20.9 GB\n",
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t0.8641\t = Validation score   (f1_macro)\n",
      "\t123.61s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 1309.17s of the 1309.17s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.4/21.0 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_logloss: 0.706388\tvalid_set's f1_macro: 0.824661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.8251\t = Validation score   (f1_macro)\n",
      "\t34.82s\t = Training   runtime\n",
      "\t0.41s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1272.30s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.5, 'NeuralNetTorch': 0.25, 'ExtraTreesGini': 0.083, 'ExtraTreesEntr': 0.083, 'RandomForestEntr': 0.042, 'XGBoost': 0.042}\n",
      "\t0.8746\t = Validation score   (f1_macro)\n",
      "\t0.45s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 528.18s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 10932.8 rows/s (4339 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\ag_cv_models\\fold_1\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_cv_models\\fold_2\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.9\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          28\n",
      "Memory Avail:       20.82 GB / 31.69 GB (65.7%)\n",
      "Disk Space Avail:   835.57 GB / 953.01 GB (87.7%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1] macro-F1: 0.8746\n",
      "\n",
      "===== [Fold 2/5] =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ... Time limit = 1800s\n",
      "AutoGluon will save models to \"c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\ag_cv_models\\fold_2\"\n",
      "Train Data Rows:    17354\n",
      "Train Data Columns: 77\n",
      "Tuning Data Rows:    4339\n",
      "Tuning Data Columns: 77\n",
      "Label Column:       target\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
      "\tFirst 10 (of 21) unique label values:  [np.int64(20), np.int64(1), np.int64(15), np.int64(0), np.int64(8), np.int64(16), np.int64(14), np.int64(18), np.int64(3), np.int64(5)]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 21\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    21302.49 MB\n",
      "\tTrain Data (Original)  Memory Usage: 13.99 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['row_na_cnt']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['ID']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('object', []) : 1 | ['ID']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 73 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_06', ...]\n",
      "\t\t('int', [])   :  2 | ['row_outlier_cnt', 'row_neg_count']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 73 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_06', ...]\n",
      "\t\t('int', [])   :  2 | ['row_outlier_cnt', 'row_neg_count']\n",
      "\t0.2s = Fit runtime\n",
      "\t75 features in original data used to generate 75 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 12.41 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.22s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_macro'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 1799.78s of the 1799.78s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.1/20.8 GB\n",
      "\t0.8661\t = Validation score   (f1_macro)\n",
      "\t16.0s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 1783.71s of the 1783.71s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.2/20.9 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_logloss: 0.553579\tvalid_set's f1_macro: 0.831979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.8329\t = Validation score   (f1_macro)\n",
      "\t24.96s\t = Training   runtime\n",
      "\t0.43s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1756.53s of the 1756.53s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.2/20.8 GB\n",
      "\t0.8294\t = Validation score   (f1_macro)\n",
      "\t10.83s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 1745.04s of the 1745.04s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/20.8 GB\n",
      "\t0.8013\t = Validation score   (f1_macro)\n",
      "\t4.53s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 1740.24s of the 1740.24s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/20.5 GB\n",
      "\t0.8052\t = Validation score   (f1_macro)\n",
      "\t9.99s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 1729.97s of the 1729.97s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.8201\t = Validation score   (f1_macro)\n",
      "\t270.56s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 1459.33s of the 1459.33s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/20.6 GB\n",
      "\t0.788\t = Validation score   (f1_macro)\n",
      "\t1.18s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 1457.78s of the 1457.78s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/20.0 GB\n",
      "\t0.7726\t = Validation score   (f1_macro)\n",
      "\t1.21s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 1456.22s of the 1456.22s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.8314\t = Validation score   (f1_macro)\n",
      "\t18.96s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 1436.87s of the 1436.87s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.0/20.3 GB\n",
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t0.8618\t = Validation score   (f1_macro)\n",
      "\t237.32s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 1199.48s of the 1199.48s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.4/20.6 GB\n",
      "\t0.8255\t = Validation score   (f1_macro)\n",
      "\t27.82s\t = Training   runtime\n",
      "\t0.25s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1170.40s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.75, 'NeuralNetTorch': 0.25}\n",
      "\t0.8722\t = Validation score   (f1_macro)\n",
      "\t0.45s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 630.08s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 40063.1 rows/s (4339 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\ag_cv_models\\fold_2\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_cv_models\\fold_3\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.9\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          28\n",
      "Memory Avail:       20.49 GB / 31.69 GB (64.7%)\n",
      "Disk Space Avail:   833.83 GB / 953.01 GB (87.5%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 1800s\n",
      "AutoGluon will save models to \"c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\ag_cv_models\\fold_3\"\n",
      "Train Data Rows:    17354\n",
      "Train Data Columns: 77\n",
      "Tuning Data Rows:    4339\n",
      "Tuning Data Columns: 77\n",
      "Label Column:       target\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
      "\tFirst 10 (of 21) unique label values:  [np.int64(0), np.int64(20), np.int64(1), np.int64(19), np.int64(8), np.int64(16), np.int64(12), np.int64(14), np.int64(18), np.int64(3)]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 2] macro-F1: 0.8722\n",
      "\n",
      "===== [Fold 3/5] =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Data Class Count: 21\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    21003.62 MB\n",
      "\tTrain Data (Original)  Memory Usage: 13.99 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['row_na_cnt']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['ID']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('object', []) : 1 | ['ID']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 73 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_06', ...]\n",
      "\t\t('int', [])   :  2 | ['row_outlier_cnt', 'row_neg_count']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 73 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_06', ...]\n",
      "\t\t('int', [])   :  2 | ['row_outlier_cnt', 'row_neg_count']\n",
      "\t0.2s = Fit runtime\n",
      "\t75 features in original data used to generate 75 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 12.41 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.22s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_macro'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 1799.78s of the 1799.78s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.1/20.5 GB\n",
      "\t0.8676\t = Validation score   (f1_macro)\n",
      "\t17.73s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 1781.99s of the 1781.99s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.2/20.5 GB\n",
      "\t0.8247\t = Validation score   (f1_macro)\n",
      "\t8.93s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1772.46s of the 1772.46s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.2/20.5 GB\n",
      "\t0.8358\t = Validation score   (f1_macro)\n",
      "\t8.66s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 1763.37s of the 1763.37s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/20.5 GB\n",
      "\t0.8022\t = Validation score   (f1_macro)\n",
      "\t3.92s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 1759.19s of the 1759.19s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/20.2 GB\n",
      "\t0.8019\t = Validation score   (f1_macro)\n",
      "\t9.44s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 1749.52s of the 1749.52s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.8219\t = Validation score   (f1_macro)\n",
      "\t238.71s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 1510.73s of the 1510.73s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/20.5 GB\n",
      "\t0.7889\t = Validation score   (f1_macro)\n",
      "\t1.19s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 1509.17s of the 1509.16s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/20.0 GB\n",
      "\t0.7825\t = Validation score   (f1_macro)\n",
      "\t1.26s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 1507.48s of the 1507.48s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.8299\t = Validation score   (f1_macro)\n",
      "\t19.21s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 1487.92s of the 1487.92s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.0/20.4 GB\n",
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t0.8637\t = Validation score   (f1_macro)\n",
      "\t295.88s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 1191.96s of the 1191.96s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.4/20.5 GB\n",
      "\t0.8298\t = Validation score   (f1_macro)\n",
      "\t27.9s\t = Training   runtime\n",
      "\t0.27s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1162.71s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.5, 'NeuralNetTorch': 0.188, 'ExtraTreesGini': 0.125, 'LightGBM': 0.062, 'ExtraTreesEntr': 0.062, 'XGBoost': 0.062}\n",
      "\t0.8766\t = Validation score   (f1_macro)\n",
      "\t0.45s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 637.77s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 10586.3 rows/s (4339 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\ag_cv_models\\fold_3\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_cv_models\\fold_4\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.9\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          28\n",
      "Memory Avail:       20.34 GB / 31.69 GB (64.2%)\n",
      "Disk Space Avail:   832.13 GB / 953.01 GB (87.3%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 1800s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 3] macro-F1: 0.8766\n",
      "\n",
      "===== [Fold 4/5] =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AutoGluon will save models to \"c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\ag_cv_models\\fold_4\"\n",
      "Train Data Rows:    17355\n",
      "Train Data Columns: 77\n",
      "Tuning Data Rows:    4338\n",
      "Tuning Data Columns: 77\n",
      "Label Column:       target\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
      "\tFirst 10 (of 21) unique label values:  [np.int64(0), np.int64(20), np.int64(1), np.int64(19), np.int64(15), np.int64(8), np.int64(16), np.int64(12), np.int64(14), np.int64(18)]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 21\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    20809.96 MB\n",
      "\tTrain Data (Original)  Memory Usage: 13.99 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['row_na_cnt']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['ID']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('object', []) : 1 | ['ID']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 73 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_06', ...]\n",
      "\t\t('int', [])   :  2 | ['row_outlier_cnt', 'row_neg_count']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 73 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_06', ...]\n",
      "\t\t('int', [])   :  2 | ['row_outlier_cnt', 'row_neg_count']\n",
      "\t0.2s = Fit runtime\n",
      "\t75 features in original data used to generate 75 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 12.41 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.21s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_macro'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 1799.79s of the 1799.79s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.1/20.3 GB\n",
      "\t0.8649\t = Validation score   (f1_macro)\n",
      "\t17.51s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 1782.22s of the 1782.21s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.2/20.3 GB\n",
      "\t0.8285\t = Validation score   (f1_macro)\n",
      "\t13.86s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1767.14s of the 1767.14s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.2/20.3 GB\n",
      "\t0.8372\t = Validation score   (f1_macro)\n",
      "\t9.92s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 1756.67s of the 1756.67s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/20.3 GB\n",
      "\t0.8109\t = Validation score   (f1_macro)\n",
      "\t4.03s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 1752.41s of the 1752.41s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/20.0 GB\n",
      "\t0.8154\t = Validation score   (f1_macro)\n",
      "\t9.81s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 1742.37s of the 1742.37s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.8225\t = Validation score   (f1_macro)\n",
      "\t285.75s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 1456.52s of the 1456.52s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/20.4 GB\n",
      "\t0.7962\t = Validation score   (f1_macro)\n",
      "\t1.24s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 1454.90s of the 1454.90s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/19.8 GB\n",
      "\t0.787\t = Validation score   (f1_macro)\n",
      "\t1.39s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 1453.12s of the 1453.12s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.8315\t = Validation score   (f1_macro)\n",
      "\t16.84s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 1435.99s of the 1435.99s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.0/20.4 GB\n",
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t0.8628\t = Validation score   (f1_macro)\n",
      "\t181.64s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 1254.27s of the 1254.27s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.4/20.4 GB\n",
      "\t0.8306\t = Validation score   (f1_macro)\n",
      "\t29.07s\t = Training   runtime\n",
      "\t0.34s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1223.74s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.333, 'ExtraTreesEntr': 0.2, 'NeuralNetTorch': 0.2, 'ExtraTreesGini': 0.133, 'RandomForestGini': 0.067, 'RandomForestEntr': 0.067}\n",
      "\t0.8772\t = Validation score   (f1_macro)\n",
      "\t0.5s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 576.8s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 8951.8 rows/s (4338 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\ag_cv_models\\fold_4\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_cv_models\\fold_5\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.9\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          28\n",
      "Memory Avail:       18.83 GB / 31.69 GB (59.4%)\n",
      "Disk Space Avail:   830.36 GB / 953.01 GB (87.1%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 4] macro-F1: 0.8772\n",
      "\n",
      "===== [Fold 5/5] =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ... Time limit = 1800s\n",
      "AutoGluon will save models to \"c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\ag_cv_models\\fold_5\"\n",
      "Train Data Rows:    17355\n",
      "Train Data Columns: 77\n",
      "Tuning Data Rows:    4338\n",
      "Tuning Data Columns: 77\n",
      "Label Column:       target\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
      "\tFirst 10 (of 21) unique label values:  [np.int64(0), np.int64(19), np.int64(15), np.int64(1), np.int64(16), np.int64(12), np.int64(14), np.int64(18), np.int64(3), np.int64(20)]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 21\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    19247.79 MB\n",
      "\tTrain Data (Original)  Memory Usage: 13.99 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['row_na_cnt']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['ID']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('object', []) : 1 | ['ID']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 73 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_06', ...]\n",
      "\t\t('int', [])   :  2 | ['row_outlier_cnt', 'row_neg_count']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 73 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_06', ...]\n",
      "\t\t('int', [])   :  2 | ['row_outlier_cnt', 'row_neg_count']\n",
      "\t0.2s = Fit runtime\n",
      "\t75 features in original data used to generate 75 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 12.41 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.26s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_macro'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 1799.74s of the 1799.74s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.1/18.8 GB\n",
      "\t0.8669\t = Validation score   (f1_macro)\n",
      "\t18.43s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 1781.25s of the 1781.25s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.2/17.4 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_logloss: 0.564171\tvalid_set's f1_macro: 0.824979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.827\t = Validation score   (f1_macro)\n",
      "\t21.81s\t = Training   runtime\n",
      "\t0.35s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1757.45s of the 1757.45s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.2/20.2 GB\n",
      "\t0.8296\t = Validation score   (f1_macro)\n",
      "\t10.1s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 1746.82s of the 1746.82s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/20.3 GB\n",
      "\t0.804\t = Validation score   (f1_macro)\n",
      "\t4.56s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 1742.00s of the 1742.00s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/20.0 GB\n",
      "\t0.8052\t = Validation score   (f1_macro)\n",
      "\t9.95s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 1731.81s of the 1731.81s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.8174\t = Validation score   (f1_macro)\n",
      "\t251.2s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 1480.53s of the 1480.53s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/20.6 GB\n",
      "\t0.7895\t = Validation score   (f1_macro)\n",
      "\t1.22s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 1478.89s of the 1478.89s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/20.0 GB\n",
      "\t0.7723\t = Validation score   (f1_macro)\n",
      "\t1.23s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 1477.21s of the 1477.21s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.8297\t = Validation score   (f1_macro)\n",
      "\t23.13s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 1453.67s of the 1453.67s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.0/19.6 GB\n",
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t0.8633\t = Validation score   (f1_macro)\n",
      "\t212.21s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 1241.39s of the 1241.39s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.4/19.0 GB\n",
      "\t0.8228\t = Validation score   (f1_macro)\n",
      "\t31.57s\t = Training   runtime\n",
      "\t0.29s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1208.28s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.75, 'NeuralNetTorch': 0.25}\n",
      "\t0.8764\t = Validation score   (f1_macro)\n",
      "\t0.46s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 592.21s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 36142.9 rows/s (4338 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\ag_cv_models\\fold_5\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 5] macro-F1: 0.8764\n",
      "\n",
      "===== CV 결과 =====\n",
      "Fold scores: [0.8746, 0.8722, 0.8766, 0.8772, 0.8764]\n",
      "OOF macro-F1: 0.8755\n",
      "[INFO] OOF 저장: C:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\oof_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 5. Stratified K-Fold CV (fold별로 G1 표준화+PCA 적용 후 AutoGluon 학습) ===\n",
    "if not AUTOGluon_OK:\n",
    "    raise ImportError('AutoGluon 불러오기 실패. 설치 필요: pip install autogluon.tabular')\n",
    "\n",
    "SAVE_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "fold_indices = list(skf.split(train_pair.drop(columns=[TARGET_COL]), train_pair[TARGET_COL].astype(str)))\n",
    "\n",
    "oof_pred = pd.Series(index=np.arange(len(train_pair)), dtype=object)\n",
    "fold_scores = []\n",
    "\n",
    "for fold, (trn_idx, val_idx) in enumerate(fold_indices, start=1):\n",
    "    print(f'\\n===== [Fold {fold}/{N_FOLDS}] =====')\n",
    "    trn_df = train_pair.iloc[trn_idx].reset_index(drop=True).copy()\n",
    "    val_df = train_pair.iloc[val_idx].reset_index(drop=True).copy()\n",
    "\n",
    "    # # --- G1 처리: 표준화 → PCA(0.95) → PC_G1_* 생성, G1 원본 삭제 ---\n",
    "    # g1_cols = [c for c in G1 if c in trn_df.columns and c != TARGET_COL]\n",
    "    # if len(g1_cols) > 0:\n",
    "    #     # 결측 채우기(수치 median) → 스케일러/ PCA 학습은 trn 기준\n",
    "    #     imputer_g1 = SimpleImputer(strategy='median')\n",
    "    #     scaler_g1  = StandardScaler()\n",
    "    #     pca_g1     = PCA(n_components=VAR_RATIO, svd_solver='full')\n",
    "\n",
    "    #     trn_g1 = imputer_g1.fit_transform(trn_df[g1_cols])\n",
    "    #     trn_g1 = scaler_g1.fit_transform(trn_g1)\n",
    "    #     trn_g1_pc = pca_g1.fit_transform(trn_g1)\n",
    "\n",
    "    #     # 검증 변환\n",
    "    #     val_g1 = imputer_g1.transform(val_df[g1_cols])\n",
    "    #     val_g1 = scaler_g1.transform(val_g1)\n",
    "    #     val_g1_pc = pca_g1.transform(val_g1)\n",
    "\n",
    "    #     # PC 열명 만들기\n",
    "    #     n_pc = trn_g1_pc.shape[1]\n",
    "    #     pc_cols = [f'PC_G1_{i+1}' for i in range(n_pc)]\n",
    "\n",
    "    #     # 데이터프레임에 붙이고 원본 G1 삭제\n",
    "    #     trn_pc_df = pd.DataFrame(trn_g1_pc, columns=pc_cols)\n",
    "    #     val_pc_df = pd.DataFrame(val_g1_pc, columns=pc_cols)\n",
    "\n",
    "    #     trn_df = pd.concat([trn_df.drop(columns=g1_cols), trn_pc_df], axis=1)\n",
    "    #     val_df = pd.concat([val_df.drop(columns=g1_cols), val_pc_df], axis=1)\n",
    "    # else:\n",
    "    #     print('[INFO] 이번 fold에서 G1 컬럼이 존재하지 않아 PCA 스킵')\n",
    "\n",
    "\n",
    "\n",
    "    # --- G1 처리: 표준화 → PCA(0.95) → PC_G1_* 생성, G1 원본 유지 ---\n",
    "    g1_cols = [c for c in G1 if c in trn_df.columns and c != TARGET_COL]\n",
    "    if len(g1_cols) > 0:\n",
    "        # 결측 채우기(수치 median) → 스케일러/ PCA 학습은 trn 기준\n",
    "        imputer_g1 = SimpleImputer(strategy='median')\n",
    "        scaler_g1  = StandardScaler()\n",
    "        pca_g1     = PCA(n_components=VAR_RATIO, svd_solver='full')\n",
    "\n",
    "        trn_g1 = imputer_g1.fit_transform(trn_df[g1_cols])\n",
    "        trn_g1 = scaler_g1.fit_transform(trn_g1)\n",
    "        trn_g1_pc = pca_g1.fit_transform(trn_g1)\n",
    "\n",
    "        # 검증 변환\n",
    "        val_g1 = imputer_g1.transform(val_df[g1_cols])\n",
    "        val_g1 = scaler_g1.transform(val_g1)\n",
    "        val_g1_pc = pca_g1.transform(val_g1)\n",
    "\n",
    "        # PC 열명 만들기\n",
    "        n_pc = trn_g1_pc.shape[1]\n",
    "        pc_cols = [f'PC_G1_{i+1}' for i in range(n_pc)]\n",
    "\n",
    "        # 데이터프레임에 붙이고 원본 G1 유지\n",
    "        trn_pc_df = pd.DataFrame(trn_g1_pc, columns=pc_cols, index=trn_df.index)\n",
    "        val_pc_df = pd.DataFrame(val_g1_pc, columns=pc_cols, index=val_df.index)\n",
    "\n",
    "        trn_df = pd.concat([trn_df, trn_pc_df], axis=1)\n",
    "        val_df = pd.concat([val_df, val_pc_df], axis=1)\n",
    "    else:\n",
    "        print('[INFO] 이번 fold에서 G1 컬럼이 존재하지 않아 PCA 스킵')\n",
    "\n",
    "\n",
    "\n",
    "    # AutoGluon 학습\n",
    "    fold_dir = SAVE_ROOT / f'fold_{fold}'\n",
    "    if fold_dir.exists():\n",
    "        print(f'[INFO] {fold_dir} 재사용/덮어쓰기')\n",
    "    fold_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    ag_trn = TabularDataset(trn_df)\n",
    "    ag_val = TabularDataset(val_df)\n",
    "\n",
    "    predictor = TabularPredictor(label=TARGET_COL, path=str(fold_dir), eval_metric='f1_macro')\n",
    "    predictor.fit(train_data=ag_trn, tuning_data=ag_val, presets=PRESETS, time_limit=TIME_LIMIT, verbosity=2)\n",
    "\n",
    "    # 검증 예측\n",
    "    y_true = val_df[TARGET_COL].astype(str).reset_index(drop=True)\n",
    "    y_pred = predictor.predict(ag_val).astype(str).reset_index(drop=True)\n",
    "\n",
    "    score = f1_score(y_true, y_pred, average='macro', labels=ALL_CLASSES, zero_division=0)\n",
    "    print(f'[Fold {fold}] macro-F1: {score:.4f}')\n",
    "    fold_scores.append(score)\n",
    "\n",
    "    oof_pred.iloc[val_idx] = y_pred.values\n",
    "\n",
    "# OOF 성능\n",
    "y_all = train_pair[TARGET_COL].astype(str).reset_index(drop=True)\n",
    "assert oof_pred.isna().sum() == 0, 'OOF 예측에 결측이 있습니다.'\n",
    "\n",
    "oof_score = f1_score(y_all, oof_pred.astype(str), average='macro', labels=ALL_CLASSES, zero_division=0)\n",
    "print('\\n===== CV 결과 =====')\n",
    "print('Fold scores:', np.round(fold_scores, 4).tolist())\n",
    "print(f'OOF macro-F1: {oof_score:.4f}')\n",
    "\n",
    "oof_df = pd.DataFrame({'y_true': y_all, 'y_pred': oof_pred.astype(str)})\n",
    "oof_df.to_csv(OOF_PATH, index=False, encoding='utf-8')\n",
    "print('[INFO] OOF 저장:', OOF_PATH.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b614d758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_full_model\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.9\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          28\n",
      "Memory Avail:       19.18 GB / 31.69 GB (60.5%)\n",
      "Disk Space Avail:   828.62 GB / 953.01 GB (86.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 1800s\n",
      "AutoGluon will save models to \"c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\ag_full_model\"\n",
      "Train Data Rows:    21693\n",
      "Train Data Columns: 82\n",
      "Label Column:       target\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
      "\tFirst 10 (of 21) unique label values:  [np.int64(0), np.int64(20), np.int64(1), np.int64(19), np.int64(15), np.int64(8), np.int64(16), np.int64(12), np.int64(14), np.int64(18)]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 21\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    19662.89 MB\n",
      "\tTrain Data (Original)  Memory Usage: 14.81 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['row_na_cnt']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['ID']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('object', []) : 1 | ['ID']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 78 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_05', ...]\n",
      "\t\t('int', [])   :  2 | ['row_outlier_cnt', 'row_neg_count']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 78 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_05', ...]\n",
      "\t\t('int', [])   :  2 | ['row_outlier_cnt', 'row_neg_count']\n",
      "\t0.2s = Fit runtime\n",
      "\t80 features in original data used to generate 80 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 13.24 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.21s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_macro'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 19523, Val Rows: 2170\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 1799.79s of the 1799.79s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.1/19.2 GB\n",
      "\t0.8528\t = Validation score   (f1_macro)\n",
      "\t20.05s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 1779.69s of the 1779.69s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.2/19.2 GB\n",
      "\t0.8226\t = Validation score   (f1_macro)\n",
      "\t13.58s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1765.12s of the 1765.11s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.2/19.2 GB\n",
      "\t0.8208\t = Validation score   (f1_macro)\n",
      "\t10.02s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 1754.61s of the 1754.61s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.3/19.2 GB\n",
      "\t0.8007\t = Validation score   (f1_macro)\n",
      "\t4.47s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 1749.87s of the 1749.87s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.3/18.7 GB\n",
      "\t0.7926\t = Validation score   (f1_macro)\n",
      "\t11.33s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 1738.29s of the 1738.29s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.8126\t = Validation score   (f1_macro)\n",
      "\t283.17s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 1455.04s of the 1455.04s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.3/19.2 GB\n",
      "\t0.7787\t = Validation score   (f1_macro)\n",
      "\t1.29s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 1453.36s of the 1453.36s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.3/18.5 GB\n",
      "\t0.7658\t = Validation score   (f1_macro)\n",
      "\t1.34s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 1451.62s of the 1451.62s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.8303\t = Validation score   (f1_macro)\n",
      "\t28.7s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 1422.48s of the 1422.48s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.1/18.8 GB\n",
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t0.8595\t = Validation score   (f1_macro)\n",
      "\t297.36s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 1125.06s of the 1125.06s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.4/19.1 GB\n",
      "\t0.818\t = Validation score   (f1_macro)\n",
      "\t27.29s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1096.94s of remaining time.\n",
      "\tEnsemble Weights: {'XGBoost': 0.333, 'NeuralNetTorch': 0.333, 'NeuralNetFastAI': 0.222, 'ExtraTreesEntr': 0.111}\n",
      "\t0.8641\t = Validation score   (f1_macro)\n",
      "\t0.34s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 703.45s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 10137.0 rows/s (2170 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\ag_full_model\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] submission 저장: C:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\submission.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 6. Full-train + Test 예측 (전역 대표/드롭 적용 기준, G1은 full-train 기준으로 PCA) ===\n",
    "if test is None:\n",
    "    print('[WARN] test.csv 없음 → 스킵')\n",
    "else:\n",
    "    # 전역 pair-drop 반영된 사본 사용\n",
    "    trn_full = train_pair.copy()\n",
    "    tst_full = test_pair.copy()\n",
    "\n",
    "    # # G1 처리: 결측→표준화→PCA(0.95) → PC_G1_* 남기고 원본 삭제 (full-train 기준으로 적합)\n",
    "    # g1_cols_full = [c for c in G1 if c in trn_full.columns and c != TARGET_COL]\n",
    "    # if len(g1_cols_full) > 0:\n",
    "    #     imputer_g1_full = SimpleImputer(strategy='median')\n",
    "    #     scaler_g1_full  = StandardScaler()\n",
    "    #     pca_g1_full     = PCA(n_components=VAR_RATIO, svd_solver='full')\n",
    "\n",
    "    #     trn_g1f = imputer_g1_full.fit_transform(trn_full[g1_cols_full])\n",
    "    #     trn_g1f = scaler_g1_full.fit_transform(trn_g1f)\n",
    "    #     trn_g1f_pc = pca_g1_full.fit_transform(trn_g1f)\n",
    "\n",
    "    #     tst_g1f = imputer_g1_full.transform(tst_full[g1_cols_full])\n",
    "    #     tst_g1f = scaler_g1_full.transform(tst_g1f)\n",
    "    #     tst_g1f_pc = pca_g1_full.transform(tst_g1f)\n",
    "\n",
    "    #     n_pc_full = trn_g1f_pc.shape[1]\n",
    "    #     pc_cols_full = [f'PC_G1_{i+1}' for i in range(n_pc_full)]\n",
    "\n",
    "    #     trn_pc_full = pd.DataFrame(trn_g1f_pc, columns=pc_cols_full)\n",
    "    #     tst_pc_full = pd.DataFrame(tst_g1f_pc, columns=pc_cols_full)\n",
    "\n",
    "    #     trn_full = pd.concat([trn_full.drop(columns=g1_cols_full), trn_pc_full], axis=1)\n",
    "    #     tst_full = pd.concat([tst_full.drop(columns=g1_cols_full), tst_pc_full], axis=1)\n",
    "    # else:\n",
    "    #     print('[INFO] full-train 기준 G1 컬럼 없음 → PCA 스킵')\n",
    "\n",
    "\n",
    "    # G1 처리: 결측→표준화→PCA(0.95) → PC_G1_* 추가 (원본은 유지)\n",
    "    g1_cols_full = [c for c in G1 if c in trn_full.columns and c != TARGET_COL]\n",
    "    if len(g1_cols_full) > 0:\n",
    "        imputer_g1_full = SimpleImputer(strategy='median')\n",
    "        scaler_g1_full  = StandardScaler()\n",
    "        pca_g1_full     = PCA(n_components=VAR_RATIO, svd_solver='full')\n",
    "\n",
    "        # train\n",
    "        trn_g1f = imputer_g1_full.fit_transform(trn_full[g1_cols_full])\n",
    "        trn_g1f = scaler_g1_full.fit_transform(trn_g1f)\n",
    "        trn_g1f_pc = pca_g1_full.fit_transform(trn_g1f)\n",
    "\n",
    "        # test\n",
    "        tst_g1f = imputer_g1_full.transform(tst_full[g1_cols_full])\n",
    "        tst_g1f = scaler_g1_full.transform(tst_g1f)\n",
    "        tst_g1f_pc = pca_g1_full.transform(tst_g1f)\n",
    "\n",
    "        # PC 컬럼 생성\n",
    "        n_pc_full = trn_g1f_pc.shape[1]\n",
    "        pc_cols_full = [f'PC_G1_{i+1}' for i in range(n_pc_full)]\n",
    "\n",
    "        trn_pc_full = pd.DataFrame(trn_g1f_pc, columns=pc_cols_full, index=trn_full.index)\n",
    "        tst_pc_full = pd.DataFrame(tst_g1f_pc, columns=pc_cols_full, index=tst_full.index)\n",
    "\n",
    "        # 원본은 유지하고 PC만 추가\n",
    "        trn_full = pd.concat([trn_full, trn_pc_full], axis=1)\n",
    "        tst_full = pd.concat([tst_full, tst_pc_full], axis=1)\n",
    "    else:\n",
    "        print('[INFO] full-train 기준 G1 컬럼 없음 → PCA 스킵')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # AutoGluon 전체 학습\n",
    "    FULL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    ag_full = TabularDataset(trn_full)\n",
    "    full_predictor = TabularPredictor(label=TARGET_COL, path=str(FULL_DIR), eval_metric='f1_macro')\n",
    "    full_predictor.fit(train_data=ag_full, presets=PRESETS, time_limit=TIME_LIMIT, verbosity=2)\n",
    "\n",
    "    # Test 예측 & 제출 저장\n",
    "    ag_test = TabularDataset(tst_full)\n",
    "    test_pred = full_predictor.predict(ag_test).astype(str)\n",
    "\n",
    "    if ID_COL is not None and ID_COL in test.columns:\n",
    "        sub = pd.DataFrame({ID_COL: test[ID_COL], TARGET_COL: test_pred})\n",
    "    else:\n",
    "        sub = pd.DataFrame({'row_id': np.arange(len(test_pred)), TARGET_COL: test_pred})\n",
    "\n",
    "    sub.to_csv(SUB_PATH, index=False, encoding='utf-8')\n",
    "    print('[INFO] submission 저장:', SUB_PATH.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8df4dd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "These features in provided data are not utilized by the predictor and will be ignored: ['ID', 'X_05', 'X_20', 'X_22', 'X_25', 'X_51', 'row_na_cnt']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"1 required columns are missing from the provided dataset to transform using AutoMLPipelineFeatureGenerator. 1 missing columns: ['PC_G1_1'] | 74 available columns: ['X_01', 'X_02', 'X_03', 'X_04', 'X_06', 'X_07', 'X_08', 'X_11', 'X_12', 'X_13', 'X_14', 'X_15', 'X_16', 'X_18', 'X_19', 'X_23', 'X_24', 'X_26', 'X_27', 'X_28', 'X_29', 'X_31', 'X_32', 'X_34', 'X_35', 'X_36', 'X_37', 'X_38', 'X_41', 'X_42', 'X_43', 'X_44', 'X_46', 'X_48', 'X_49', 'X_50', 'X_52', 'row_num_mean', 'row_num_std', 'row_outlier_cnt', 'G1_mean', 'G1_std', 'G1_min', 'G1_max', 'G1_range', 'X05_div_X09', 'X05_mul_X09', 'X05_sq', 'X09_sq', 'X20_div_X22', 'X20_mul_X22', 'X20_sq', 'X22_sq', 'X25_div_X51', 'X25_mul_X51', 'X25_sq', 'X51_sq', 'row_neg_count', 'X_11_std', 'X_19_std', 'X_37_std', 'X_40_std', 'X_08_sq', 'X_19_sq', 'X_29_sq', 'X_46_sq', 'X_41_sq', 'X_49_sq', 'X_08xX_19', 'X_08_div_X_19', 'X_19_div_X_08', 'X_08xX_29', 'X_08_div_X_29', 'X_29_div_X_08']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\autogluon\\features\\generators\\abstract.py:356\u001b[39m, in \u001b[36mAbstractFeatureGenerator.transform\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    353\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(X.columns) != \u001b[38;5;28mself\u001b[39m.features_in:\n\u001b[32m    354\u001b[39m         \u001b[38;5;66;03m# It comes at a cost when making a copy of the DataFrame,\u001b[39;00m\n\u001b[32m    355\u001b[39m         \u001b[38;5;66;03m# therefore, try avoid copying by checking the expected features first.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m         X = \u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures_in\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4112\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4115\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6264\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6263\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6264\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['PC_G1_1'] not in index\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# 중요도 계산\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m fi = \u001b[43mpredictor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeature_importance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# 전체 출력\u001b[39;00m\n\u001b[32m     16\u001b[39m display(fi)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:3490\u001b[39m, in \u001b[36mTabularPredictor.feature_importance\u001b[39m\u001b[34m(self, data, model, features, feature_stage, subsample_size, time_limit, num_shuffle_sets, include_confidence_band, confidence_level, silent)\u001b[39m\n\u001b[32m   3487\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_shuffle_sets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3488\u001b[39m     num_shuffle_sets = \u001b[32m10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m time_limit \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m5\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m3490\u001b[39m fi_df = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_learner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_feature_importance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3492\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3493\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_stage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3495\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubsample_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubsample_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_shuffle_sets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_shuffle_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3498\u001b[39m \u001b[43m    \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m=\u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3499\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3501\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_confidence_band:\n\u001b[32m   3502\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m confidence_level <= \u001b[32m0.5\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m confidence_level >= \u001b[32m1.0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\autogluon\\tabular\\learner\\abstract_learner.py:1010\u001b[39m, in \u001b[36mAbstractTabularLearner.get_feature_importance\u001b[39m\u001b[34m(self, model, X, y, features, feature_stage, subsample_size, silent, **kwargs)\u001b[39m\n\u001b[32m   1007\u001b[39m         X = X.drop(columns=unused_features)\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m feature_stage == \u001b[33m\"\u001b[39m\u001b[33moriginal\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1010\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_feature_importance_raw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubsample_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubsample_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform_func\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m=\u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m     X = \u001b[38;5;28mself\u001b[39m.transform_features(X)\n\u001b[32m   1014\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py:3522\u001b[39m, in \u001b[36mAbstractTabularTrainer._get_feature_importance_raw\u001b[39m\u001b[34m(self, X, y, model, eval_metric, **kwargs)\u001b[39m\n\u001b[32m   3520\u001b[39m model: AbstractModel = \u001b[38;5;28mself\u001b[39m.load_model(model)\n\u001b[32m   3521\u001b[39m predict_func_kwargs = \u001b[38;5;28mdict\u001b[39m(model=model)\n\u001b[32m-> \u001b[39m\u001b[32m3522\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompute_permutation_feature_importance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3523\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3524\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3525\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpredict_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpredict_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3526\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpredict_func_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpredict_func_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3527\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3528\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantile_levels\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquantile_levels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3529\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3530\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\autogluon\\core\\utils\\utils.py:983\u001b[39m, in \u001b[36mcompute_permutation_feature_importance\u001b[39m\u001b[34m(X, y, predict_func, eval_metric, features, subsample_size, num_shuffle_sets, predict_func_kwargs, transform_func, transform_func_kwargs, time_limit, silent, log_prefix, importance_as_list, random_state, **kwargs)\u001b[39m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m subsample \u001b[38;5;129;01mor\u001b[39;00m shuffle_repeat == \u001b[32m0\u001b[39m:\n\u001b[32m    982\u001b[39m     time_start_score = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m983\u001b[39m     X_transformed = X \u001b[38;5;28;01mif\u001b[39;00m transform_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mtransform_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtransform_func_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    984\u001b[39m     y_pred = predict_func(X_transformed, **predict_func_kwargs)\n\u001b[32m    985\u001b[39m     score_baseline = eval_metric(y, y_pred, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\autogluon\\tabular\\learner\\abstract_learner.py:488\u001b[39m, in \u001b[36mAbstractTabularLearner.transform_features\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    486\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtransform_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m    487\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m feature_generator \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.feature_generators:\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m         X = \u001b[43mfeature_generator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    489\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\autogluon\\features\\generators\\abstract.py:362\u001b[39m, in \u001b[36mAbstractFeatureGenerator.transform\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    360\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m X.columns:\n\u001b[32m    361\u001b[39m             missing_cols.append(col)\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[32m    363\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(missing_cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m required columns are missing from the provided dataset to transform using \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    364\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(missing_cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m missing columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_cols\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    365\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(X.columns))\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m available columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(X.columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    366\u001b[39m     )\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pre_astype_generator:\n\u001b[32m    368\u001b[39m     X = \u001b[38;5;28mself\u001b[39m._pre_astype_generator.transform(X)\n",
      "\u001b[31mKeyError\u001b[39m: \"1 required columns are missing from the provided dataset to transform using AutoMLPipelineFeatureGenerator. 1 missing columns: ['PC_G1_1'] | 74 available columns: ['X_01', 'X_02', 'X_03', 'X_04', 'X_06', 'X_07', 'X_08', 'X_11', 'X_12', 'X_13', 'X_14', 'X_15', 'X_16', 'X_18', 'X_19', 'X_23', 'X_24', 'X_26', 'X_27', 'X_28', 'X_29', 'X_31', 'X_32', 'X_34', 'X_35', 'X_36', 'X_37', 'X_38', 'X_41', 'X_42', 'X_43', 'X_44', 'X_46', 'X_48', 'X_49', 'X_50', 'X_52', 'row_num_mean', 'row_num_std', 'row_outlier_cnt', 'G1_mean', 'G1_std', 'G1_min', 'G1_max', 'G1_range', 'X05_div_X09', 'X05_mul_X09', 'X05_sq', 'X09_sq', 'X20_div_X22', 'X20_mul_X22', 'X20_sq', 'X22_sq', 'X25_div_X51', 'X25_mul_X51', 'X25_sq', 'X51_sq', 'row_neg_count', 'X_11_std', 'X_19_std', 'X_37_std', 'X_40_std', 'X_08_sq', 'X_19_sq', 'X_29_sq', 'X_46_sq', 'X_41_sq', 'X_49_sq', 'X_08xX_19', 'X_08_div_X_19', 'X_19_div_X_08', 'X_08xX_29', 'X_08_div_X_29', 'X_29_div_X_08']\""
     ]
    }
   ],
   "source": [
    "# === Feature Importance (AutoGluon: 전체) ===\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 라벨 자동 탐지\n",
    "label_candidates = [\"target\", \"TARGET\", \"label\", \"Label\", \"y\"]\n",
    "label_col = None\n",
    "for col in train_pair.columns:\n",
    "    if col in label_candidates:\n",
    "        label_col = col\n",
    "        break\n",
    "\n",
    "# 중요도 계산\n",
    "fi = predictor.feature_importance(train_pair, silent=True)\n",
    "\n",
    "# 전체 출력\n",
    "display(fi)\n",
    "\n",
    "# 시각화 (전체 피처)\n",
    "plt.figure(figsize=(8, max(6, 0.3*len(fi))))\n",
    "fi.sort_values(\"importance\").plot(kind=\"barh\", y=\"importance\", legend=False)\n",
    "plt.title(\"Feature Importance (All Features)\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9351c1",
   "metadata": {},
   "source": [
    "\n",
    "## 전처리 메모 (요약)\n",
    "- **PAIR_GROUPS 대표 선택**: `(-결측률) 정규화 + MI 정규화 + F 정규화` 평균으로 대표 1개 선택(전역 고정). 대표 외 변수는 **삭제**.\n",
    "- **G1 그룹**: 수치 결측 median 대체 → 표준화(`StandardScaler`) → `PCA(n_components=0.95)` → `PC_G1_i`만 유지, G1 원본 **삭제**.\n",
    "- **K-Fold 내 누수 방지**: G1의 `imputer/scaler/PCA`는 **각 폴드의 train 부분으로만 적합** 후 val에 적용. (전역 pair-drop은 기준 통일을 위해 train 전체에서 1회 산정)\n",
    "- AutoGluon은 나머지 결측/범주형 처리를 자체적으로 수행.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
