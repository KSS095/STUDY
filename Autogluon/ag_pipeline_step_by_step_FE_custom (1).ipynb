{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86d2d495",
   "metadata": {},
   "source": [
    "\n",
    "# AutoGluon Î∂ÑÎ•ò ÌååÏù¥ÌîÑÎùºÏù∏ (Stratified K-Fold, ÏÖÄÎ≥Ñ Ïã§Ìñâ ¬∑ Ìï®ÏàòÌòïÌÉú X)\n",
    "\n",
    "**Î™©Ìëú**: `train.csv`(ÌÉÄÍπÉ: `target`)Î°ú ÌïôÏäµ ‚Üí `test.csv` Î∂ÑÎ•ò Í≤∞Í≥º ÏÉùÏÑ±  \n",
    "**ÏöîÍµ¨ÏÇ¨Ìï≠ Î∞òÏòÅ**\n",
    "- Stratified K-Fold (OOF & Ìè¥Îìú ÏÑ±Îä•)\n",
    "- **Í∞Å Í∏∞Îä•Î≥Ñ ÏÖÄ**Î°ú Íµ¨ÌòÑ(Ìï®Ïàò Ï†ïÏùò ÏóÜÏù¥ Î∞îÎ°ú Ïã§Ìñâ)\n",
    "- Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨:\n",
    "  - **G1 Í∑∏Î£π**: `[\"X_05\",\"X_09\",\"X_20\",\"X_22\",\"X_25\",\"X_51\"]` ‚Üí ÌëúÏ§ÄÌôî(ÌèâÍ∑†0, Î∂ÑÏÇ∞1) ‚Üí **PCA(ÏÑ§Î™ÖÎ∂ÑÏÇ∞ 0.95)** ‚Üí `PC_G1_1, PC_G1_2, ...`Îßå ÎÇ®Í∏∞Í≥† G1 ÏõêÎ≥∏ 6Í∞ú Î≥ÄÏàò **ÏÇ≠Ï†ú**\n",
    "  - **PAIR_GROUPS(Ïåç Í∑∏Î£π)**: Í∞Å ÏåçÏóêÏÑú ÎåÄÌëú 1Í∞úÎßå ÎÇ®ÍπÄ. **ÎåÄÌëú ÏÑ†ÌÉù Í∏∞Ï§Ä**: Í≤∞Ï∏°Î•†‚Üì, mutual information(‚Üë), ANOVA-F(‚Üë)\n",
    "- AutoGluonÏúºÎ°ú ÌïôÏäµ/Í≤ÄÏ¶ù/Ï∂îÎ°†\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîß Custom Feature Engineering Block (ÏûêÎèô Ï∂îÍ∞Ä)\n",
    "\n",
    "ÏöîÍµ¨ÏÇ¨Ìï≠Ïóê Îî∞Îùº ÏïÑÎûò Î°úÏßÅÏùÑ Ï∂îÍ∞ÄÌñàÏäµÎãàÎã§.\n",
    "\n",
    "- **Ï§ëÏöî ÌîºÏ≤ò**: `X_08, X_19, X_29, X_46, X_40, X_41, X_49`\n",
    "  - Ïù¥ Ï§ë **ÏÉÅÍ¥ÄÍ¥ÄÍ≥ÑÍ∞Ä ÎÜíÏùÄ Ïåç(Í∏∞Î≥∏ ÏûÑÍ≥ÑÍ∞í |corr| ‚â• 0.95)**Ïù¥ 1Í∞ú Ïù¥ÏÉÅ Ï°¥Ïû¨ÌïòÎ©¥ ‚Üí Ìï¥Îãπ ÏåçÎì§Ïóê ÎåÄÌï¥ **Í≥±(product) ÌîºÏ≤ò** ÏÉùÏÑ± + Í∞Å ÌîºÏ≤ò **Ï†úÍ≥±(sq) ÌîºÏ≤ò** ÏÉùÏÑ±\n",
    "  - ÏÉÅÍ¥ÄÍ¥ÄÍ≥ÑÍ∞Ä ÎÜíÏùÄ ÏåçÏù¥ **ÌïòÎÇòÎèÑ ÏóÜÏúºÎ©¥** ‚Üí **Ï†úÍ≥±(sq) ÌîºÏ≤òÎßå** ÏÉùÏÑ±\n",
    "- **Ï†úÍ±∞Ìï† ÌîºÏ≤ò**: `X_17`, `X_09`, `X_40`, `X_21_sq`\n",
    "  - `X_40`ÏùÄ Ï§ëÏöî ÌîºÏ≤ò Î™©Î°ùÏóêÎèÑ ÏûàÏóàÏúºÎÇò, **ÏµúÏ¢ÖÏ†ÅÏúºÎ°ú ÏÇ≠Ï†ú ÏöîÏ≤≠**Ïóê Îî∞Îùº ÌååÏÉùÏóê ÏÇ¨Ïö©ÌïòÏßÄ ÏïäÍ≥† Ï†úÍ±∞Ìï©ÎãàÎã§.\n",
    "- **ÎÇòÎ®∏ÏßÄ Ï†ÑÏ≤òÎ¶¨ Î°úÏßÅÏùÄ Í∑∏ÎåÄÎ°ú Ïú†ÏßÄ**Ìï©ÎãàÎã§.\n",
    "- **Idempotent(Ï§ëÎ≥µ ÏïàÏ†Ñ)**: ÎèôÏùº ÏÖÄÏùÑ Ïó¨Îü¨ Î≤à Ïã§ÌñâÌï¥ÎèÑ Ï§ëÎ≥µ ÏÉùÏÑ±ÎêòÏßÄ ÏïäÎèÑÎ°ù Íµ¨ÌòÑÌñàÏäµÎãàÎã§."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2c456a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Config loaded.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 1. Imports & Config ===\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_classif, f_classif\n",
    "\n",
    "try:\n",
    "    from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "    AUTOGluon_OK = True\n",
    "except Exception as e:\n",
    "    AUTOGluon_OK = False\n",
    "    print('[WARN] AutoGluon import Ïã§Ìå®:', e)\n",
    "\n",
    "# Í≤ΩÎ°ú/ÏÑ§Ï†ï\n",
    "DATA_DIR = Path('.')\n",
    "TRAIN_PATH = DATA_DIR / 'train.csv'\n",
    "TEST_PATH  = DATA_DIR / 'test.csv'\n",
    "TARGET_COL = 'target'\n",
    "ID_COL = None  # Ïòà: 'id' (ÏóÜÏúºÎ©¥ None)\n",
    "\n",
    "N_FOLDS = 5\n",
    "RANDOM_STATE = 42\n",
    "PRESETS = 'medium_quality'\n",
    "TIME_LIMIT = 1800  # Ï¥à(Ìè¥ÎìúÎãπ)\n",
    "\n",
    "SAVE_ROOT = Path('./ag_cv_models')\n",
    "OOF_PATH  = Path('./oof_predictions.csv')\n",
    "FULL_DIR  = Path('./ag_full_model')\n",
    "SUB_PATH  = Path('./submission.csv')\n",
    "\n",
    "# Í∑∏Î£π Ï†ïÏùò\n",
    "G1 = [\"X_05\",\"X_09\",\"X_20\",\"X_22\",\"X_25\",\"X_51\"]\n",
    "VAR_RATIO = 0.95  # PCA ÏÑ§Î™ÖÎ∂ÑÏÇ∞ ÏûÑÍ≥Ñ\n",
    "\n",
    "# ÏùºÎã® Ïó¨Í∏∞ÏÑ† Î≥ÑÎ°ú ÏÇ¨Ïö©Ïù¥ Ïïà Îê®.\n",
    "# Ïåç Î≥ÄÏàò Í∑∏Î£π (ÌïÑÏöîÏãú Îçî Ï∂îÍ∞Ä)\n",
    "PAIR_GROUPS = [\n",
    "    [\"X_04\",\"X_39\"],\n",
    "    [\"X_06\",\"X_45\"],\n",
    "    ####################\n",
    "    [\"X_10\",\"X_17\"],\n",
    "    [\"X_07\",\"X_33\"],\n",
    "    [\"X_12\",\"X_21\"],\n",
    "    [\"X_26\",\"X_30\"],\n",
    "    [\"X_38\",\"X_47\"],\n",
    "    \n",
    "    # [\"X_05\",\"X_25\"], ...  # ÏòàÏãúÎ°ú Îçî ÎÑ£ÏùÑ Ïàò ÏûàÏùå\n",
    "]\n",
    "\n",
    "# Ïû¨ÌòÑÏÑ±\n",
    "random.seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print('[INFO] Config loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Feature Importance (after training)\n",
    "- AutoGluon `TabularPredictor`Í∞Ä ÏûàÏúºÎ©¥ `predictor.feature_importance(...)` ÏÇ¨Ïö©\n",
    "- Í∑∏Î†áÏßÄ ÏïäÏúºÎ©¥, `feature_importances_` ÏÜçÏÑ±Ïù¥ ÏûàÎäî Ìä∏Î¶¨Í≥ÑÏó¥ Î™®Îç∏ÏóêÏÑú Ï§ëÏöîÎèÑ Ï∂îÏ∂ú\n",
    "- ÏÉÅÏúÑ 30Í∞úÎ•º Í∞ÄÎ°úÎ∞î(barh)Î°ú ÏãúÍ∞ÅÌôî"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FI] Ï§ëÏöîÎèÑÎ•º Í≥ÑÏÇ∞Ìï† Ïàò ÏûàÎäî ÌïôÏäµ Í∞ùÏ≤¥Î•º Ï∞æÏßÄ Î™ªÌñàÏäµÎãàÎã§.\n"
     ]
    }
   ],
   "source": [
    "# === Feature Importance (Generic) ===\n",
    "import inspect\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ÌõÑÎ≥¥ Îç∞Ïù¥ÌÑ∞/ÎùºÎ≤®Î™Ö\n",
    "label_candidates = [\"target\", \"TARGET\", \"label\", \"Label\", \"y\"]\n",
    "df_candidates = [\"train_pair\", \"train\", \"train_df\", \"df_train\"]\n",
    "X_candidates  = [\"X_train\", \"X_tr\", \"X\"]\n",
    "model_candidates = [\"predictor\", \"model\", \"clf\", \"estimator\"]\n",
    "\n",
    "def _detect_label_name(df):\n",
    "    for col in df.columns:\n",
    "        for lab in label_candidates:\n",
    "            if col.lower() == lab.lower():\n",
    "                return col\n",
    "    return None\n",
    "\n",
    "def _get_first_existing(names):\n",
    "    g = globals()\n",
    "    for nm in names:\n",
    "        if nm in g:\n",
    "            return nm, g[nm]\n",
    "    return None, None\n",
    "\n",
    "imp_df = None\n",
    "used_route = None\n",
    "\n",
    "# 1) Try AutoGluon TabularPredictor\n",
    "name_pred, predictor = _get_first_existing([\"predictor\"])\n",
    "if predictor is not None:\n",
    "    try:\n",
    "        # Check class name to avoid false positives\n",
    "        clsname = predictor.__class__.__name__.lower()\n",
    "        if \"tabularpredictor\" in clsname and hasattr(predictor, \"feature_importance\"):\n",
    "            # find a labeled df\n",
    "            df_name, df_obj = _get_first_existing(df_candidates)\n",
    "            if isinstance(df_obj, pd.DataFrame):\n",
    "                label_col = _detect_label_name(df_obj)\n",
    "                if label_col is not None:\n",
    "                    fi = predictor.feature_importance(df_obj, silent=True)\n",
    "                    # AutoGluon returns a DataFrame with 'importance' column (usually 'importance' or 'importance_abs')\n",
    "                    # Normalize to a standard format\n",
    "                    if isinstance(fi, pd.DataFrame):\n",
    "                        # try common column names\n",
    "                        value_col = None\n",
    "                        for c in [\"importance\", \"importance_value\", \"importance_mean\", \"importance_abs\"]:\n",
    "                            if c in fi.columns:\n",
    "                                value_col = c\n",
    "                                break\n",
    "                        if value_col is None and fi.shape[1] >= 1:\n",
    "                            value_col = fi.columns[0]\n",
    "                        imp_df = fi[[value_col]].copy()\n",
    "                        imp_df.columns = [\"importance\"]\n",
    "                    else:\n",
    "                        # Unexpected; coerce\n",
    "                        imp_df = pd.DataFrame({\"importance\": fi})\n",
    "                    used_route = f\"AutoGluon({name_pred}) on {df_name}\"\n",
    "    except Exception as e:\n",
    "        print(\"[FI] AutoGluon route skipped:\", repr(e))\n",
    "\n",
    "# 2) Try any model with feature_importances_\n",
    "if imp_df is None:\n",
    "    # Pick a model object exposing feature_importances_\n",
    "    model_obj = None\n",
    "    for k, v in list(globals().items()):\n",
    "        try:\n",
    "            if hasattr(v, \"feature_importances_\"):\n",
    "                model_obj = v\n",
    "                model_name = k\n",
    "                break\n",
    "        except Exception:\n",
    "            continue\n",
    "    if model_obj is not None:\n",
    "        # Determine feature names\n",
    "        feat_names = None\n",
    "        # Prefer X_train with columns\n",
    "        Xn, Xobj = _get_first_existing([\"X_train\", \"X_tr\"])\n",
    "        if isinstance(Xobj, pd.DataFrame):\n",
    "            feat_names = list(Xobj.columns)\n",
    "        if feat_names is None:\n",
    "            # fallback: from train_pair excluding label\n",
    "            df_name, df_obj = _get_first_existing(df_candidates)\n",
    "            if isinstance(df_obj, pd.DataFrame):\n",
    "                label_col = _detect_label_name(df_obj)\n",
    "                if label_col is not None:\n",
    "                    feat_names = [c for c in df_obj.columns if c != label_col]\n",
    "        # Build df\n",
    "        try:\n",
    "            vals = np.array(model_obj.feature_importances_).ravel()\n",
    "            if feat_names is None:\n",
    "                feat_names = [f\"f{i}\" for i in range(len(vals))]\n",
    "            imp_df = pd.DataFrame({\"feature\": feat_names, \"importance\": vals}).set_index(\"feature\")\n",
    "            used_route = f\"{model_name}.feature_importances_\"\n",
    "        except Exception as e:\n",
    "            print(\"[FI] feature_importances_ route skipped:\", repr(e))\n",
    "\n",
    "# 3) If nothing worked, exit gracefully\n",
    "if imp_df is None or imp_df.empty:\n",
    "    print(\"[FI] Ï§ëÏöîÎèÑÎ•º Í≥ÑÏÇ∞Ìï† Ïàò ÏûàÎäî ÌïôÏäµ Í∞ùÏ≤¥Î•º Ï∞æÏßÄ Î™ªÌñàÏäµÎãàÎã§.\")\n",
    "else:\n",
    "    # Sort and take top 30\n",
    "    imp_df = imp_df.sort_values(\"importance\", ascending=False).head(30)\n",
    "    display(imp_df)\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, max(4, 0.3*len(imp_df))))\n",
    "    imp_df.sort_values(\"importance\").plot(kind=\"barh\", legend=False)\n",
    "    plt.title(f\"Feature Importance (top {len(imp_df)})\\n{used_route if used_route else ''}\")\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6baf9e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] train.shape = (21693, 54)\n",
      "[INFO] test.shape  = (15004, 53)\n",
      "[INFO] ÌÅ¥ÎûòÏä§ Ïàò: 21\n",
      "[INFO] ÏòàÏãú: ['0', '1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '3', '4', '5', '6', '7', '8']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 2. Load train/test, ÌÉÄÍπÉ Ï†êÍ≤Ä ===\n",
    "assert TRAIN_PATH.exists(), f'train.csvÍ∞Ä {TRAIN_PATH} Ïóê ÏóÜÏäµÎãàÎã§.'\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "print('[INFO] train.shape =', train.shape)\n",
    "assert TARGET_COL in train.columns, f'{TARGET_COL} Ïª¨ÎüºÏù¥ trainÏóê ÏóÜÏäµÎãàÎã§.'\n",
    "\n",
    "if TEST_PATH.exists():\n",
    "    test = pd.read_csv(TEST_PATH)\n",
    "    print('[INFO] test.shape  =', test.shape)\n",
    "else:\n",
    "    test = None\n",
    "    print('[WARN] test.csv Í∞Ä ÏóÜÏñ¥ Ï∂îÎ°† ÏÖÄÏùÄ Ïä§ÌÇµÎê† Ïàò ÏûàÏäµÎãàÎã§.')\n",
    "\n",
    "# ÌÉÄÍπÉ ÌÅ¥Î¶∞ÏóÖ(Î¨∏ÏûêÏó¥ ÌÜµÏùº + Í≤∞Ï∏° Ï†úÍ±∞)\n",
    "y = train[TARGET_COL].astype(str)\n",
    "na_mask = y.isna() | (y.str.lower()=='nan') | (y=='None') | (y=='')\n",
    "if na_mask.any():\n",
    "    print(f'[WARN] ÌÉÄÍπÉ Í≤∞Ï∏°/Ïú†ÏÇ¨Í≤∞Ï∏° {na_mask.sum()}Í±¥ Î∞úÍ≤¨ ‚Üí Ìï¥Îãπ Ìñâ Ï†úÍ±∞')\n",
    "    train = train.loc[~na_mask].reset_index(drop=True)\n",
    "    y = train[TARGET_COL].astype(str)\n",
    "\n",
    "ALL_CLASSES = sorted(y.unique())\n",
    "print('[INFO] ÌÅ¥ÎûòÏä§ Ïàò:', len(ALL_CLASSES))\n",
    "print('[INFO] ÏòàÏãú:', ALL_CLASSES[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06254dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ÎåÄÌëú ÏÑ†ÌÉù Í≤∞Í≥º: {('X_04', 'X_39'): 'X_04', ('X_06', 'X_45'): 'X_06', ('X_10', 'X_17'): 'X_17', ('X_07', 'X_33'): 'X_07', ('X_12', 'X_21'): 'X_12', ('X_26', 'X_30'): 'X_26', ('X_38', 'X_47'): 'X_38'}\n",
      "[INFO] ÏÇ≠Ï†ú ÎåÄÏÉÅ(ÎåÄÌëú ÏïÑÎãå Î≥ÄÏàò): ['X_39', 'X_45', 'X_10', 'X_33', 'X_21', 'X_30', 'X_47']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:58: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 3. PAIR_GROUPS Ï†ÑÏó≠ ÎåÄÌëú Î≥ÄÏàò ÏÑ†ÌÉù (train Í∏∞Ï§Ä) ===\n",
    "# - Í≤∞Ï∏°Î•† ÎÇÆÏùÑÏàòÎ°ù Ï¢ãÏùå\n",
    "# - mutual information ÎÜíÏùÑÏàòÎ°ù Ï¢ãÏùå\n",
    "# - ANOVA-F ÎÜíÏùÑÏàòÎ°ù Ï¢ãÏùå\n",
    "# Ï†êÏàò = (-Í≤∞Ï∏°Î•†) Ï†ïÍ∑úÌôî + (MI) Ï†ïÍ∑úÌôî + (F) Ï†ïÍ∑úÌôî  (Í∞Å 1/3 Í∞ÄÏ§ë)\n",
    "\n",
    "# Í≥†ÏÉÅÍ¥Ä Ïåç(PAIR_GROUPS)Ïùò Í∞Å ÏåçÏóêÏÑú ÎåÄÌëú Î≥ÄÏàò 1Í∞úÎßå ÎÇ®Í∏∞Í∏∞ ÏúÑÌï¥, Îëê Î≥ÄÏàòÏùò ‚ÄúÌíàÏßà+Ïú†Ïö©ÏÑ±‚ÄùÏùÑ Ï†êÏàòÎ°ú Í≥ÑÏÇ∞Ìï¥ ÎπÑÍµêÌïòÎäî Î°úÏßÅ\n",
    "# -> 39, 45 ÏÇ≠Ï†ú\n",
    "\n",
    "rep_map = {}         # {('X_04','X_39'): 'X_04', ...}\n",
    "to_drop_pairs = []   # ÎåÄÌëúÍ∞Ä ÏïÑÎãå Î≥ÄÏàò Î™©Î°ù\n",
    "\n",
    "# Ïä§ÏΩîÏñ¥ Í≥ÑÏÇ∞ÏùÑ ÏúÑÌïú ÏûÑÏãú Îç∞Ïù¥ÌÑ∞(Í≤∞Ï∏° Ï±ÑÏõÄ: ÏàòÏπò median) Íµ¨ÏÑ±\n",
    "num_cols = train.drop(columns=[TARGET_COL]).select_dtypes(include=[np.number]).columns.tolist()\n",
    "tmp = train.copy()\n",
    "\n",
    "if len(num_cols) > 0:\n",
    "    imputer_num = SimpleImputer(strategy='median')\n",
    "    tmp[num_cols] = imputer_num.fit_transform(tmp[num_cols])\n",
    "\n",
    "# Í∞Å ÏåçÏóê ÎåÄÌï¥ Ï†êÏàò ÏÇ∞Ï∂ú\n",
    "for pair in PAIR_GROUPS:\n",
    "    pair = [c for c in pair if c in train.columns and c != TARGET_COL]\n",
    "    if len(pair) != 2:\n",
    "        print(f'[WARN] Ïåç {pair} Ï§ë Ïú†Ìö® Ïª¨ÎüºÏù¥ 2Í∞úÍ∞Ä ÏïÑÎãôÎãàÎã§. Ïä§ÌÇµ.')\n",
    "        continue\n",
    "\n",
    "    cols_ok = []\n",
    "    scores = []\n",
    "    for col in pair:\n",
    "        # Í≤∞Ï∏°Î•† (ÏõêÎ≥∏ Í∏∞Ï§Ä)\n",
    "        miss_rate = train[col].isna().mean()\n",
    "\n",
    "        # MI/FÎäî ÏàòÏπòÌòïÏóêÏÑúÎßå Î∞îÎ°ú Í≥ÑÏÇ∞. ÏàòÏπòÍ∞Ä ÏïÑÎãê Í≤ΩÏö∞ ÏûÑÏãú Î≥ÄÌôò(Î≤îÏ£º‚ÜíÏàúÎ≤à Ïù∏ÏΩîÎî©)\n",
    "        s = tmp[col]\n",
    "        if not np.issubdtype(s.dtype, np.number):\n",
    "            # Í∞ÑÎã® ÎùºÎ≤® Ïù∏ÏΩîÎî©\n",
    "            s = s.astype('category').cat.codes\n",
    "\n",
    "        X_ = s.values.reshape(-1, 1)\n",
    "        y_ = train[TARGET_COL].astype(str).values\n",
    "\n",
    "        # MI (discrete target)\n",
    "        try:\n",
    "            mi = mutual_info_classif(X_, y_, discrete_features=True, random_state=RANDOM_STATE)[0]\n",
    "        except Exception:\n",
    "            mi = 0.0\n",
    "\n",
    "        # ANOVA-F\n",
    "        try:\n",
    "            f, _ = f_classif(X_, pd.factorize(y_)[0])\n",
    "            f = float(f[0])\n",
    "        except Exception:\n",
    "            f = 0.0\n",
    "\n",
    "        cols_ok.append(col)\n",
    "        scores.append({'col': col, 'miss': miss_rate, 'mi': mi, 'f': f})\n",
    "\n",
    "    if len(scores) != 2:\n",
    "        print(f'[WARN] {pair} Ï†êÏàò Í≥ÑÏÇ∞ Ïã§Ìå®(Ïä§ÌÇµ)')\n",
    "        continue\n",
    "\n",
    "    # Ï†ïÍ∑úÌôî\n",
    "    miss_vals = np.array([s['miss'] for s in scores])\n",
    "    mi_vals   = np.array([s['mi']   for s in scores])\n",
    "    f_vals    = np.array([s['f']    for s in scores])\n",
    "\n",
    "    def norm(v):\n",
    "        v = v.astype(float)\n",
    "        if np.allclose(v.max(), v.min()):\n",
    "            return np.zeros_like(v)\n",
    "        return (v - v.min()) / (v.max() - v.min())\n",
    "\n",
    "    miss_n = norm(1 - miss_vals)  # Í≤∞Ï∏°Î•† ÎÇÆÏùÑÏàòÎ°ù‚Üë ‚Üí (1 - miss)\n",
    "    mi_n   = norm(mi_vals)\n",
    "    f_n    = norm(f_vals)\n",
    "\n",
    "    final  = (miss_n + mi_n + f_n) / 3.0\n",
    "    best_idx = int(np.argmax(final))\n",
    "    rep = scores[best_idx]['col']\n",
    "    rep_map[tuple(pair)] = rep\n",
    "\n",
    "    drop_cols = [c for c in pair if c != rep]\n",
    "    to_drop_pairs.extend(drop_cols)\n",
    "\n",
    "print('[INFO] ÎåÄÌëú ÏÑ†ÌÉù Í≤∞Í≥º:', rep_map)\n",
    "print('[INFO] ÏÇ≠Ï†ú ÎåÄÏÉÅ(ÎåÄÌëú ÏïÑÎãå Î≥ÄÏàò):', to_drop_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed51c973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] train_pair.shape = (21693, 47)\n",
      "[INFO] test_pair.shape  = (15004, 46)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 4. ÎåÄÌëú ÏïÑÎãå Î≥ÄÏàò ÏÇ≠Ï†ú(Ï†ÑÏó≠) ===\n",
    "train_pair = train.drop(columns=[c for c in to_drop_pairs if c in train.columns], errors='ignore').copy()\n",
    "if test is not None:\n",
    "    test_pair = test.drop(columns=[c for c in to_drop_pairs if c in test.columns], errors='ignore').copy()\n",
    "else:\n",
    "    test_pair = None\n",
    "\n",
    "print('[INFO] train_pair.shape =', train_pair.shape)\n",
    "if test_pair is not None:\n",
    "    print('[INFO] test_pair.shape  =', test_pair.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4502f01b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'X_01', 'X_02', 'X_03', 'X_04', 'X_05', 'X_06', 'X_07', 'X_08',\n",
       "       'X_09', 'X_11', 'X_12', 'X_13', 'X_14', 'X_15', 'X_16', 'X_17', 'X_18',\n",
       "       'X_19', 'X_20', 'X_22', 'X_23', 'X_24', 'X_25', 'X_26', 'X_27', 'X_28',\n",
       "       'X_29', 'X_31', 'X_32', 'X_34', 'X_35', 'X_36', 'X_37', 'X_38', 'X_40',\n",
       "       'X_41', 'X_42', 'X_43', 'X_44', 'X_46', 'X_48', 'X_49', 'X_50', 'X_51',\n",
       "       'X_52', 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pair.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37efda8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ÌååÏÉù ÌîºÏ≤ò Ï∂îÍ∞Ä ÏôÑÎ£å. train_pair: (21693, 68) | test_pair: (15004, 67)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === ÌååÏÉù ÌîºÏ≤ò Ï∂îÍ∞Ä (Ìï®Ïàò ÏóÜÏù¥ Î∞îÎ°ú Ï†ÅÏö©) ===\n",
    "_base_cols = [c for c in train_pair.columns if c != TARGET_COL]\n",
    "_num_cols  = train_pair[_base_cols].select_dtypes(include=[float, int]).columns\n",
    "\n",
    "# 1) Ìñâ Îã®ÏúÑ ÏöîÏïΩ\n",
    "train_pair['row_na_cnt'] = train_pair[_base_cols].isna().sum(axis=1)\n",
    "if test_pair is not None:\n",
    "    test_pair['row_na_cnt'] = test_pair[_base_cols].isna().sum(axis=1)\n",
    "\n",
    "train_pair['row_num_mean'] = train_pair[_num_cols].mean(axis=1)\n",
    "train_pair['row_num_std']  = train_pair[_num_cols].std(axis=1)\n",
    "if test_pair is not None:\n",
    "    test_pair['row_num_mean'] = test_pair[_num_cols].mean(axis=1)\n",
    "    test_pair['row_num_std']  = test_pair[_num_cols].std(axis=1)\n",
    "\n",
    "if len(_num_cols) > 0:\n",
    "    _Q1  = train_pair[_num_cols].quantile(0.25)\n",
    "    _Q3  = train_pair[_num_cols].quantile(0.75)\n",
    "    _IQR = (_Q3 - _Q1).replace(0, 1e-12)\n",
    "\n",
    "    _below_tr = (train_pair[_num_cols] < (_Q1 - 1.5 * _IQR))\n",
    "    _above_tr = (train_pair[_num_cols] > (_Q3 + 1.5 * _IQR))\n",
    "    train_pair['row_outlier_cnt'] = (_below_tr | _above_tr).sum(axis=1)\n",
    "\n",
    "    if test_pair is not None:\n",
    "        _below_te = (test_pair[_num_cols] < (_Q1 - 1.5 * _IQR))\n",
    "        _above_te = (test_pair[_num_cols] > (_Q3 + 1.5 * _IQR))\n",
    "        test_pair['row_outlier_cnt'] = (_below_te | _above_te).sum(axis=1)\n",
    "\n",
    "# 2) G1 Í∑∏Î£π ÌÜµÍ≥Ñ\n",
    "_g1_cols = [c for c in G1 if c in train_pair.columns]\n",
    "if len(_g1_cols) > 0:\n",
    "    train_pair['G1_mean']  = train_pair[_g1_cols].mean(axis=1)\n",
    "    train_pair['G1_std']   = train_pair[_g1_cols].std(axis=1)\n",
    "    train_pair['G1_min']   = train_pair[_g1_cols].min(axis=1)\n",
    "    train_pair['G1_max']   = train_pair[_g1_cols].max(axis=1)\n",
    "    train_pair['G1_range'] = train_pair['G1_max'] - train_pair['G1_min']\n",
    "\n",
    "    if test_pair is not None:\n",
    "        test_pair['G1_mean']  = test_pair[_g1_cols].mean(axis=1)\n",
    "        test_pair['G1_std']   = test_pair[_g1_cols].std(axis=1)\n",
    "        test_pair['G1_min']   = test_pair[_g1_cols].min(axis=1)\n",
    "        test_pair['G1_max']   = test_pair[_g1_cols].max(axis=1)\n",
    "        test_pair['G1_range'] = test_pair['G1_max'] - test_pair['G1_min']\n",
    "\n",
    "\n",
    "# 3) ÎπÑÏÑ†Ìòï ÌååÏÉù (Ïòà: X_05, X_09)\n",
    "if 'X_05' in train_pair.columns and 'X_09' in train_pair.columns:\n",
    "    train_pair['X05_div_X09'] = train_pair['X_05'] / (train_pair['X_09'].replace(0, 1e-12) + 1e-12)\n",
    "    train_pair['X05_mul_X09'] = train_pair['X_05'] * train_pair['X_09']\n",
    "    if test_pair is not None:\n",
    "        test_pair['X05_div_X09'] = test_pair['X_05'] / (test_pair['X_09'].replace(0, 1e-12) + 1e-12)\n",
    "        test_pair['X05_mul_X09'] = test_pair['X_05'] * test_pair['X_09']\n",
    "\n",
    "if 'X_05' in train_pair.columns:\n",
    "    train_pair['X05_sq'] = train_pair['X_05'] ** 2\n",
    "    if test_pair is not None:\n",
    "        test_pair['X05_sq'] = test_pair['X_05'] ** 2\n",
    "\n",
    "if 'X_09' in train_pair.columns:\n",
    "    train_pair['X09_sq'] = train_pair['X_09'] ** 2\n",
    "    if test_pair is not None:\n",
    "        test_pair['X09_sq'] = test_pair['X_09'] ** 2\n",
    "\n",
    "\n",
    "if 'X_20' in train_pair.columns and 'X_22' in train_pair.columns:\n",
    "    train_pair['X20_div_X22'] = train_pair['X_20'] / (train_pair['X_22'].replace(0, 1e-12) + 1e-12)\n",
    "    train_pair['X20_mul_X22'] = train_pair['X_20'] * train_pair['X_22']\n",
    "    if test_pair is not None:\n",
    "        test_pair['X20_div_X22'] = test_pair['X_20'] / (test_pair['X_22'].replace(0, 1e-12) + 1e-12)\n",
    "        test_pair['X20_mul_X22'] = test_pair['X_20'] * test_pair['X_22']\n",
    "\n",
    "if 'X_20' in train_pair.columns:\n",
    "    train_pair['X20_sq'] = train_pair['X_20'] ** 2\n",
    "    if test_pair is not None:\n",
    "        test_pair['X20_sq'] = test_pair['X_20'] ** 2\n",
    "\n",
    "if 'X_22' in train_pair.columns:\n",
    "    train_pair['X22_sq'] = train_pair['X_22'] ** 2\n",
    "    if test_pair is not None:\n",
    "        test_pair['X22_sq'] = test_pair['X_22'] ** 2\n",
    "\n",
    "\n",
    "\n",
    "if 'X_25' in train_pair.columns and 'X_51' in train_pair.columns:\n",
    "    train_pair['X25_div_X51'] = train_pair['X_25'] / (train_pair['X_51'].replace(0, 1e-12) + 1e-12)\n",
    "    train_pair['X25_mul_X51'] = train_pair['X_25'] * train_pair['X_51']\n",
    "    if test_pair is not None:\n",
    "        test_pair['X25_div_X51'] = test_pair['X_25'] / (test_pair['X_51'].replace(0, 1e-12) + 1e-12)\n",
    "        test_pair['X25_mul_X51'] = test_pair['X_25'] * test_pair['X_51']\n",
    "\n",
    "if 'X_25' in train_pair.columns:\n",
    "    train_pair['X25_sq'] = train_pair['X_25'] ** 2\n",
    "    if test_pair is not None:\n",
    "        test_pair['X25_sq'] = test_pair['X_25'] ** 2\n",
    "\n",
    "if 'X_51' in train_pair.columns:\n",
    "    train_pair['X51_sq'] = train_pair['X_51'] ** 2\n",
    "    if test_pair is not None:\n",
    "        test_pair['X51_sq'] = test_pair['X_51'] ** 2\n",
    "\n",
    "\n",
    "\n",
    "# import itertools\n",
    "\n",
    "# # 1) pairwise Ï°∞Ìï© (Í≥±/ÎÇòÎàóÏÖà)\n",
    "# for f1, f2 in itertools.combinations(G1, 2):\n",
    "#     if f1 in train_pair.columns and f2 in train_pair.columns:\n",
    "#         # ÎÇòÎàóÏÖà (f1 / f2)\n",
    "#         train_pair[f\"{f1}_div_{f2}\"] = train_pair[f1] / (train_pair[f2].replace(0, 1e-12) + 1e-12)\n",
    "#         if test_pair is not None:\n",
    "#             test_pair[f\"{f1}_div_{f2}\"] = test_pair[f1] / (test_pair[f2].replace(0, 1e-12) + 1e-12)\n",
    "\n",
    "#         # Í≥± (f1 * f2)\n",
    "#         train_pair[f\"{f1}_mul_{f2}\"] = train_pair[f1] * train_pair[f2]\n",
    "#         if test_pair is not None:\n",
    "#             test_pair[f\"{f1}_mul_{f2}\"] = test_pair[f1] * test_pair[f2]\n",
    "\n",
    "# # 2) Í∞Å featureÏóê ÎåÄÌï¥ Ï†úÍ≥±\n",
    "# for f in G1:\n",
    "#     if f in train_pair.columns:\n",
    "#         train_pair[f\"{f}_sq\"] = train_pair[f] ** 2\n",
    "#         if test_pair is not None:\n",
    "#             test_pair[f\"{f}_sq\"] = test_pair[f] ** 2\n",
    "\n",
    "print('[INFO] ÌååÏÉù ÌîºÏ≤ò Ï∂îÍ∞Ä ÏôÑÎ£å. train_pair:', train_pair.shape, '| test_pair:', (None if test_pair is None else test_pair.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a024cdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === Custom Feature Engineering (ÏöîÍµ¨ÏÇ¨Ìï≠ Î∞òÏòÅ) ===\n",
    "# from itertools import combinations\n",
    "\n",
    "# # Ï§ëÏöî ÌîºÏ≤ò\n",
    "# important_cols = [\"X_08\", \"X_19\", \"X_29\", \"X_46\", \"X_40\", \"X_41\", \"X_49\"]\n",
    "# # Ï†úÍ±∞Ìï† ÌîºÏ≤ò\n",
    "# # drop_cols = [\"X_17\", \"X_09\", \"X_40\", \"X_21_sq\"]\n",
    "\n",
    "# # drop_cols Ï†úÏô∏ ÌõÑ Ïã§Ï†ú Ï°¥Ïû¨ÌïòÎäî Ï§ëÏöî ÌîºÏ≤òÎßå ÌïÑÌÑ∞ÎßÅ\n",
    "# exist_importants = [c for c in important_cols if c in train_pair.columns and c not in drop_cols]\n",
    "\n",
    "# # ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ ÌôïÏù∏ (ÏûÑÍ≥ÑÍ∞í: 0.95)\n",
    "# high_corr_pairs = []\n",
    "# if len(exist_importants) >= 2:\n",
    "#     corr = train_pair[exist_importants].corr().abs()\n",
    "#     for a, b in combinations(exist_importants, 2):\n",
    "#         if corr.loc[a, b] >= 0.7:\n",
    "#             high_corr_pairs.append((a, b))\n",
    "\n",
    "# # Ï†úÍ≥± ÌîºÏ≤ò ÏÉùÏÑ±\n",
    "# for c in exist_importants:\n",
    "#     new_col = f\"{c}_sq\"\n",
    "#     if new_col not in train_pair.columns:\n",
    "#         train_pair[new_col] = train_pair[c] ** 2\n",
    "#         test_pair[new_col] = test_pair[c] ** 2\n",
    "\n",
    "# # Í≥± ÌîºÏ≤ò ÏÉùÏÑ± (ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ ÎÜíÏùÄ Í≤ΩÏö∞Îßå)\n",
    "# if high_corr_pairs:\n",
    "#     for a, b in high_corr_pairs:\n",
    "#         new_col = f\"{a}x{b}\"\n",
    "#         if new_col not in train_pair.columns:\n",
    "#             train_pair[new_col] = train_pair[a] * train_pair[b]\n",
    "#             test_pair[new_col] = test_pair[a] * test_pair[b]\n",
    "\n",
    "# if high_corr_pairs:\n",
    "#     for a, b in high_corr_pairs:\n",
    "#         new_col = f\"{a}x{b}\"\n",
    "#         if new_col not in train_pair.columns:\n",
    "#             train_pair[new_col] = train_pair[a] * train_pair[b]\n",
    "#             test_pair[new_col] = test_pair[a] * test_pair[b]\n",
    "            \n",
    "# # # Î∂àÌïÑÏöî ÌîºÏ≤ò Ï†úÍ±∞\n",
    "# # for c in drop_cols:\n",
    "# #     if c in train_pair.columns:\n",
    "# #         train_pair.drop(columns=[c], inplace=True)\n",
    "# #     if c in test_pair.columns:\n",
    "# #         test_pair.drop(columns=[c], inplace=True)\n",
    "\n",
    "# print(\"[Custom FE] Ï†úÍ≥± Ï∂îÍ∞Ä:\", [f\"{c}_sq\" for c in exist_importants])\n",
    "# print(\"[Custom FE] Í≥± Ï∂îÍ∞Ä:\", [f\"{a}x{b}\" for a, b in high_corr_pairs])\n",
    "# print(\"[Custom FE] Ï†úÍ±∞:\", drop_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62c97dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Custom FE] row_neg_count Ï∂îÍ∞Ä ÏôÑÎ£å\n",
      "[Custom FE] scaling Ï∂îÍ∞Ä: ['X_11', 'X_19', 'X_37', 'X_40']\n",
      "[Custom FE] squares: ['X_08_sq', 'X_19_sq', 'X_29_sq', 'X_46_sq', 'X_41_sq', 'X_49_sq']\n",
      "[Custom FE] products: ['X_08xX_19', 'X_08xX_29']\n",
      "[Custom FE] ratios: ['X_08_div_X_19', 'X_08_div_X_29', 'X_19_div_X_08', 'X_29_div_X_08']\n",
      "[Custom FE] Ï†úÍ±∞: ['X_17', 'X_09', 'X_40', 'X_21_sq']\n"
     ]
    }
   ],
   "source": [
    "# === Custom FE (inline, k-fold ÏßÅÏ†Ñ) ===\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ---- ÏÑ§Ï†ï ----\n",
    "important_cols = [\"X_08\", \"X_19\", \"X_29\", \"X_46\", \"X_40\", \"X_41\", \"X_49\"]\n",
    "drop_cols = [\"X_17\", \"X_09\", \"X_40\", \"X_21_sq\"]\n",
    "corr_threshold = 0.7\n",
    "scale_cols = [\"X_11\", \"X_19\", \"X_37\", \"X_40\"]\n",
    "label_candidates = [\"target\", \"TARGET\", \"label\", \"Label\", \"y\"]\n",
    "\n",
    "# ---- Ïú†Ìã∏ ----\n",
    "def _safe_ratio(a: pd.Series, b: pd.Series, eps: float = 1e-9) -> pd.Series:\n",
    "    # 0/0 ÎòêÎäî Î∂ÑÎ™® 0 ÎåÄÏùë\n",
    "    return a / (b.replace(0, np.nan))\n",
    "    # ÌïÑÏöîÏãú: return a / (b + eps)\n",
    "\n",
    "def _numeric_feature_cols(df: pd.DataFrame):\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    to_exclude = []\n",
    "    for c in num_cols:\n",
    "        for lab in label_candidates:\n",
    "            if c.lower() == lab.lower():\n",
    "                to_exclude.append(c)\n",
    "                break\n",
    "    # id Î•ò Ïª¨Îüº Ï†úÏô∏(ÏûàÎã§Î©¥)\n",
    "    for c in num_cols:\n",
    "        if c.lower() in (\"id\", \"idx\"):\n",
    "            to_exclude.append(c)\n",
    "    to_exclude = list(dict.fromkeys(to_exclude))\n",
    "    return [c for c in num_cols if c not in to_exclude]\n",
    "\n",
    "# ---- ÏùåÏàò Í∞úÏàò ÌîºÏ≤ò ----\n",
    "_numeric_cols_train = _numeric_feature_cols(train_pair)\n",
    "train_pair[\"row_neg_count\"] = (train_pair[_numeric_cols_train] < 0).sum(axis=1)\n",
    "\n",
    "_numeric_cols_test = [c for c in _numeric_cols_train if c in test_pair.columns]\n",
    "test_pair[\"row_neg_count\"] = (test_pair[_numeric_cols_test] < 0).sum(axis=1)\n",
    "\n",
    "# ---- Ïä§ÏºÄÏùºÎßÅ ----\n",
    "for c in scale_cols:\n",
    "    if c in train_pair.columns:\n",
    "        scaler = StandardScaler()\n",
    "        train_pair[f\"{c}_std\"] = scaler.fit_transform(train_pair[[c]]).ravel()\n",
    "        if c in test_pair.columns:\n",
    "            test_pair[f\"{c}_std\"] = scaler.transform(test_pair[[c]]).ravel()\n",
    "\n",
    "# ---- Ï§ëÏöî ÌîºÏ≤ò ÌååÏÉù ----\n",
    "exist_importants = [c for c in important_cols if c in train_pair.columns and c not in drop_cols]\n",
    "\n",
    "# ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ Í≥ÑÏÇ∞\n",
    "high_corr_pairs = []\n",
    "if len(exist_importants) >= 2:\n",
    "    corr = train_pair[exist_importants].corr().abs()\n",
    "    for a, b in combinations(exist_importants, 2):\n",
    "        if corr.loc[a, b] >= corr_threshold:\n",
    "            high_corr_pairs.append((a, b))\n",
    "\n",
    "# Ï†úÍ≥±\n",
    "for c in exist_importants:\n",
    "    sq = f\"{c}_sq\"\n",
    "    if sq not in train_pair.columns and np.issubdtype(train_pair[c].dtype, np.number):\n",
    "        train_pair[sq] = train_pair[c] ** 2\n",
    "        if c in test_pair.columns:\n",
    "            test_pair[sq] = test_pair[c] ** 2\n",
    "\n",
    "# Í≥±/ÎÇòÎàóÏÖà (Í≥†ÏÉÅÍ¥ÄÏùº Îïå)\n",
    "if high_corr_pairs:\n",
    "    for a, b in high_corr_pairs:\n",
    "        prod = f\"{a}x{b}\"\n",
    "        if prod not in train_pair.columns:\n",
    "            train_pair[prod] = train_pair[a] * train_pair[b]\n",
    "            if (a in test_pair.columns) and (b in test_pair.columns):\n",
    "                test_pair[prod] = test_pair[a] * test_pair[b]\n",
    "        # ratio a/b, b/a\n",
    "        div1 = f\"{a}_div_{b}\"\n",
    "        div2 = f\"{b}_div_{a}\"\n",
    "        if div1 not in train_pair.columns:\n",
    "            train_pair[div1] = _safe_ratio(train_pair[a], train_pair[b])\n",
    "            if (a in test_pair.columns) and (b in test_pair.columns):\n",
    "                test_pair[div1] = _safe_ratio(test_pair[a], test_pair[b])\n",
    "        if div2 not in train_pair.columns:\n",
    "            train_pair[div2] = _safe_ratio(train_pair[b], train_pair[a])\n",
    "            if (a in test_pair.columns) and (b in test_pair.columns):\n",
    "                test_pair[div2] = _safe_ratio(test_pair[b], test_pair[a])\n",
    "\n",
    "# ---- Ïª¨Îüº Ï†úÍ±∞ ----\n",
    "for c in drop_cols:\n",
    "    if c in train_pair.columns:\n",
    "        train_pair.drop(columns=[c], inplace=True)\n",
    "    if c in test_pair.columns:\n",
    "        test_pair.drop(columns=[c], inplace=True)\n",
    "\n",
    "print(\"[Custom FE] row_neg_count Ï∂îÍ∞Ä ÏôÑÎ£å\")\n",
    "print(\"[Custom FE] scaling Ï∂îÍ∞Ä:\", [c for c in scale_cols if f\"{c}_std\" in train_pair.columns])\n",
    "print(\"[Custom FE] squares:\", [f\"{c}_sq\" for c in exist_importants])\n",
    "print(\"[Custom FE] products:\", [f\"{a}x{b}\" for a,b in high_corr_pairs])\n",
    "print(\"[Custom FE] ratios:\", [f\"{a}_div_{b}\" for a,b in high_corr_pairs] + [f\"{b}_div_{a}\" for a,b in high_corr_pairs])\n",
    "print(\"[Custom FE] Ï†úÍ±∞:\", [c for c in drop_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d81a6bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'X_01', 'X_02', 'X_03', 'X_04', 'X_05', 'X_06', 'X_07', 'X_08',\n",
       "       'X_11', 'X_12', 'X_13', 'X_14', 'X_15', 'X_16', 'X_18', 'X_19', 'X_20',\n",
       "       'X_22', 'X_23', 'X_24', 'X_25', 'X_26', 'X_27', 'X_28', 'X_29', 'X_31',\n",
       "       'X_32', 'X_34', 'X_35', 'X_36', 'X_37', 'X_38', 'X_41', 'X_42', 'X_43',\n",
       "       'X_44', 'X_46', 'X_48', 'X_49', 'X_50', 'X_51', 'X_52', 'target',\n",
       "       'row_na_cnt', 'row_num_mean', 'row_num_std', 'row_outlier_cnt',\n",
       "       'G1_mean', 'G1_std', 'G1_min', 'G1_max', 'G1_range', 'X05_div_X09',\n",
       "       'X05_mul_X09', 'X05_sq', 'X09_sq', 'X20_div_X22', 'X20_mul_X22',\n",
       "       'X20_sq', 'X22_sq', 'X25_div_X51', 'X25_mul_X51', 'X25_sq', 'X51_sq',\n",
       "       'row_neg_count', 'X_11_std', 'X_19_std', 'X_37_std', 'X_40_std',\n",
       "       'X_08_sq', 'X_19_sq', 'X_29_sq', 'X_46_sq', 'X_41_sq', 'X_49_sq',\n",
       "       'X_08xX_19', 'X_08_div_X_19', 'X_19_div_X_08', 'X_08xX_29',\n",
       "       'X_08_div_X_29', 'X_29_div_X_08'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pair.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f88b4e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== [Fold 1/5] =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_cv_models\\fold_1\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.9\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          28\n",
      "Memory Avail:       21.84 GB / 31.69 GB (68.9%)\n",
      "Disk Space Avail:   837.35 GB / 953.01 GB (87.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 1800s\n",
      "AutoGluon will save models to \"c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\ag_cv_models\\fold_1\"\n",
      "Train Data Rows:    17354\n",
      "Train Data Columns: 77\n",
      "Tuning Data Rows:    4339\n",
      "Tuning Data Columns: 77\n",
      "Label Column:       target\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
      "\tFirst 10 (of 21) unique label values:  [np.int64(0), np.int64(20), np.int64(1), np.int64(19), np.int64(15), np.int64(8), np.int64(12), np.int64(4), np.int64(5), np.int64(11)]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 21\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    22361.17 MB\n",
      "\tTrain Data (Original)  Memory Usage: 13.99 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['row_na_cnt']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['ID']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('object', []) : 1 | ['ID']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 73 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_06', ...]\n",
      "\t\t('int', [])   :  2 | ['row_outlier_cnt', 'row_neg_count']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 73 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_06', ...]\n",
      "\t\t('int', [])   :  2 | ['row_outlier_cnt', 'row_neg_count']\n",
      "\t0.2s = Fit runtime\n",
      "\t75 features in original data used to generate 75 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 12.41 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.21s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_macro'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 1799.79s of the 1799.79s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.1/21.6 GB\n",
      "\t0.8649\t = Validation score   (f1_macro)\n",
      "\t20.69s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 1779.03s of the 1779.03s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.2/21.6 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_logloss: 0.546444\tvalid_set's f1_macro: 0.823349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.8263\t = Validation score   (f1_macro)\n",
      "\t24.22s\t = Training   runtime\n",
      "\t0.44s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1752.43s of the 1752.43s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.2/21.7 GB\n",
      "\t0.8286\t = Validation score   (f1_macro)\n",
      "\t13.32s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 1738.14s of the 1738.14s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/21.7 GB\n",
      "\t0.8014\t = Validation score   (f1_macro)\n",
      "\t4.19s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 1733.70s of the 1733.70s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/21.4 GB\n",
      "\t0.7985\t = Validation score   (f1_macro)\n",
      "\t9.42s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 1724.03s of the 1724.03s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.8156\t = Validation score   (f1_macro)\n",
      "\t269.12s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 1454.84s of the 1454.84s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/21.2 GB\n",
      "\t0.7804\t = Validation score   (f1_macro)\n",
      "\t1.12s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 1453.26s of the 1453.26s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/20.8 GB\n",
      "\t0.7713\t = Validation score   (f1_macro)\n",
      "\t1.16s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 1451.73s of the 1451.73s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.8262\t = Validation score   (f1_macro)\n",
      "\t18.52s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 1432.85s of the 1432.85s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.0/20.9 GB\n",
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t0.8641\t = Validation score   (f1_macro)\n",
      "\t123.61s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 1309.17s of the 1309.17s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.4/21.0 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_logloss: 0.706388\tvalid_set's f1_macro: 0.824661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.8251\t = Validation score   (f1_macro)\n",
      "\t34.82s\t = Training   runtime\n",
      "\t0.41s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1272.30s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.5, 'NeuralNetTorch': 0.25, 'ExtraTreesGini': 0.083, 'ExtraTreesEntr': 0.083, 'RandomForestEntr': 0.042, 'XGBoost': 0.042}\n",
      "\t0.8746\t = Validation score   (f1_macro)\n",
      "\t0.45s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 528.18s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 10932.8 rows/s (4339 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\ag_cv_models\\fold_1\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_cv_models\\fold_2\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.9\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          28\n",
      "Memory Avail:       20.82 GB / 31.69 GB (65.7%)\n",
      "Disk Space Avail:   835.57 GB / 953.01 GB (87.7%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1] macro-F1: 0.8746\n",
      "\n",
      "===== [Fold 2/5] =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ... Time limit = 1800s\n",
      "AutoGluon will save models to \"c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\ag_cv_models\\fold_2\"\n",
      "Train Data Rows:    17354\n",
      "Train Data Columns: 77\n",
      "Tuning Data Rows:    4339\n",
      "Tuning Data Columns: 77\n",
      "Label Column:       target\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
      "\tFirst 10 (of 21) unique label values:  [np.int64(20), np.int64(1), np.int64(15), np.int64(0), np.int64(8), np.int64(16), np.int64(14), np.int64(18), np.int64(3), np.int64(5)]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 21\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    21302.49 MB\n",
      "\tTrain Data (Original)  Memory Usage: 13.99 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['row_na_cnt']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['ID']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('object', []) : 1 | ['ID']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 73 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_06', ...]\n",
      "\t\t('int', [])   :  2 | ['row_outlier_cnt', 'row_neg_count']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 73 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_06', ...]\n",
      "\t\t('int', [])   :  2 | ['row_outlier_cnt', 'row_neg_count']\n",
      "\t0.2s = Fit runtime\n",
      "\t75 features in original data used to generate 75 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 12.41 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.22s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_macro'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 1799.78s of the 1799.78s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.1/20.8 GB\n",
      "\t0.8661\t = Validation score   (f1_macro)\n",
      "\t16.0s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 1783.71s of the 1783.71s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.2/20.9 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_logloss: 0.553579\tvalid_set's f1_macro: 0.831979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.8329\t = Validation score   (f1_macro)\n",
      "\t24.96s\t = Training   runtime\n",
      "\t0.43s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1756.53s of the 1756.53s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.2/20.8 GB\n",
      "\t0.8294\t = Validation score   (f1_macro)\n",
      "\t10.83s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 1745.04s of the 1745.04s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/20.8 GB\n",
      "\t0.8013\t = Validation score   (f1_macro)\n",
      "\t4.53s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 1740.24s of the 1740.24s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/20.5 GB\n",
      "\t0.8052\t = Validation score   (f1_macro)\n",
      "\t9.99s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 1729.97s of the 1729.97s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.8201\t = Validation score   (f1_macro)\n",
      "\t270.56s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 1459.33s of the 1459.33s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/20.6 GB\n",
      "\t0.788\t = Validation score   (f1_macro)\n",
      "\t1.18s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 1457.78s of the 1457.78s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/20.0 GB\n",
      "\t0.7726\t = Validation score   (f1_macro)\n",
      "\t1.21s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 1456.22s of the 1456.22s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.8314\t = Validation score   (f1_macro)\n",
      "\t18.96s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 1436.87s of the 1436.87s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.0/20.3 GB\n",
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t0.8618\t = Validation score   (f1_macro)\n",
      "\t237.32s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 1199.48s of the 1199.48s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.4/20.6 GB\n",
      "\t0.8255\t = Validation score   (f1_macro)\n",
      "\t27.82s\t = Training   runtime\n",
      "\t0.25s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1170.40s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.75, 'NeuralNetTorch': 0.25}\n",
      "\t0.8722\t = Validation score   (f1_macro)\n",
      "\t0.45s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 630.08s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 40063.1 rows/s (4339 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\ag_cv_models\\fold_2\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_cv_models\\fold_3\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.9\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          28\n",
      "Memory Avail:       20.49 GB / 31.69 GB (64.7%)\n",
      "Disk Space Avail:   833.83 GB / 953.01 GB (87.5%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 1800s\n",
      "AutoGluon will save models to \"c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\ag_cv_models\\fold_3\"\n",
      "Train Data Rows:    17354\n",
      "Train Data Columns: 77\n",
      "Tuning Data Rows:    4339\n",
      "Tuning Data Columns: 77\n",
      "Label Column:       target\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
      "\tFirst 10 (of 21) unique label values:  [np.int64(0), np.int64(20), np.int64(1), np.int64(19), np.int64(8), np.int64(16), np.int64(12), np.int64(14), np.int64(18), np.int64(3)]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 2] macro-F1: 0.8722\n",
      "\n",
      "===== [Fold 3/5] =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Data Class Count: 21\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    21003.62 MB\n",
      "\tTrain Data (Original)  Memory Usage: 13.99 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['row_na_cnt']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['ID']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('object', []) : 1 | ['ID']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 73 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_06', ...]\n",
      "\t\t('int', [])   :  2 | ['row_outlier_cnt', 'row_neg_count']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 73 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_06', ...]\n",
      "\t\t('int', [])   :  2 | ['row_outlier_cnt', 'row_neg_count']\n",
      "\t0.2s = Fit runtime\n",
      "\t75 features in original data used to generate 75 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 12.41 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.22s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_macro'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 1799.78s of the 1799.78s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.1/20.5 GB\n",
      "\t0.8676\t = Validation score   (f1_macro)\n",
      "\t17.73s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 1781.99s of the 1781.99s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.2/20.5 GB\n",
      "\t0.8247\t = Validation score   (f1_macro)\n",
      "\t8.93s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1772.46s of the 1772.46s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.2/20.5 GB\n",
      "\t0.8358\t = Validation score   (f1_macro)\n",
      "\t8.66s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 1763.37s of the 1763.37s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/20.5 GB\n",
      "\t0.8022\t = Validation score   (f1_macro)\n",
      "\t3.92s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 1759.19s of the 1759.19s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/20.2 GB\n",
      "\t0.8019\t = Validation score   (f1_macro)\n",
      "\t9.44s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 1749.52s of the 1749.52s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.8219\t = Validation score   (f1_macro)\n",
      "\t238.71s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 1510.73s of the 1510.73s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/20.5 GB\n",
      "\t0.7889\t = Validation score   (f1_macro)\n",
      "\t1.19s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 1509.17s of the 1509.16s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/20.0 GB\n",
      "\t0.7825\t = Validation score   (f1_macro)\n",
      "\t1.26s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 1507.48s of the 1507.48s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.8299\t = Validation score   (f1_macro)\n",
      "\t19.21s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 1487.92s of the 1487.92s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.0/20.4 GB\n",
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t0.8637\t = Validation score   (f1_macro)\n",
      "\t295.88s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 1191.96s of the 1191.96s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.4/20.5 GB\n",
      "\t0.8298\t = Validation score   (f1_macro)\n",
      "\t27.9s\t = Training   runtime\n",
      "\t0.27s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1162.71s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.5, 'NeuralNetTorch': 0.188, 'ExtraTreesGini': 0.125, 'LightGBM': 0.062, 'ExtraTreesEntr': 0.062, 'XGBoost': 0.062}\n",
      "\t0.8766\t = Validation score   (f1_macro)\n",
      "\t0.45s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 637.77s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 10586.3 rows/s (4339 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\ag_cv_models\\fold_3\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_cv_models\\fold_4\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.9\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          28\n",
      "Memory Avail:       20.34 GB / 31.69 GB (64.2%)\n",
      "Disk Space Avail:   832.13 GB / 953.01 GB (87.3%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 1800s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 3] macro-F1: 0.8766\n",
      "\n",
      "===== [Fold 4/5] =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AutoGluon will save models to \"c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\ag_cv_models\\fold_4\"\n",
      "Train Data Rows:    17355\n",
      "Train Data Columns: 77\n",
      "Tuning Data Rows:    4338\n",
      "Tuning Data Columns: 77\n",
      "Label Column:       target\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
      "\tFirst 10 (of 21) unique label values:  [np.int64(0), np.int64(20), np.int64(1), np.int64(19), np.int64(15), np.int64(8), np.int64(16), np.int64(12), np.int64(14), np.int64(18)]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 21\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    20809.96 MB\n",
      "\tTrain Data (Original)  Memory Usage: 13.99 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['row_na_cnt']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['ID']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('object', []) : 1 | ['ID']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 73 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_06', ...]\n",
      "\t\t('int', [])   :  2 | ['row_outlier_cnt', 'row_neg_count']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 73 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_06', ...]\n",
      "\t\t('int', [])   :  2 | ['row_outlier_cnt', 'row_neg_count']\n",
      "\t0.2s = Fit runtime\n",
      "\t75 features in original data used to generate 75 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 12.41 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.21s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_macro'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 1799.79s of the 1799.79s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.1/20.3 GB\n",
      "\t0.8649\t = Validation score   (f1_macro)\n",
      "\t17.51s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 1782.22s of the 1782.21s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.2/20.3 GB\n",
      "\t0.8285\t = Validation score   (f1_macro)\n",
      "\t13.86s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1767.14s of the 1767.14s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.2/20.3 GB\n",
      "\t0.8372\t = Validation score   (f1_macro)\n",
      "\t9.92s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 1756.67s of the 1756.67s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/20.3 GB\n",
      "\t0.8109\t = Validation score   (f1_macro)\n",
      "\t4.03s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 1752.41s of the 1752.41s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/20.0 GB\n",
      "\t0.8154\t = Validation score   (f1_macro)\n",
      "\t9.81s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 1742.37s of the 1742.37s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.8225\t = Validation score   (f1_macro)\n",
      "\t285.75s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 1456.52s of the 1456.52s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/20.4 GB\n",
      "\t0.7962\t = Validation score   (f1_macro)\n",
      "\t1.24s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 1454.90s of the 1454.90s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/19.8 GB\n",
      "\t0.787\t = Validation score   (f1_macro)\n",
      "\t1.39s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 1453.12s of the 1453.12s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.8315\t = Validation score   (f1_macro)\n",
      "\t16.84s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 1435.99s of the 1435.99s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.0/20.4 GB\n",
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t0.8628\t = Validation score   (f1_macro)\n",
      "\t181.64s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 1254.27s of the 1254.27s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.4/20.4 GB\n",
      "\t0.8306\t = Validation score   (f1_macro)\n",
      "\t29.07s\t = Training   runtime\n",
      "\t0.34s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1223.74s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.333, 'ExtraTreesEntr': 0.2, 'NeuralNetTorch': 0.2, 'ExtraTreesGini': 0.133, 'RandomForestGini': 0.067, 'RandomForestEntr': 0.067}\n",
      "\t0.8772\t = Validation score   (f1_macro)\n",
      "\t0.5s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 576.8s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 8951.8 rows/s (4338 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\ag_cv_models\\fold_4\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_cv_models\\fold_5\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.9\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          28\n",
      "Memory Avail:       18.83 GB / 31.69 GB (59.4%)\n",
      "Disk Space Avail:   830.36 GB / 953.01 GB (87.1%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 4] macro-F1: 0.8772\n",
      "\n",
      "===== [Fold 5/5] =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ... Time limit = 1800s\n",
      "AutoGluon will save models to \"c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\ag_cv_models\\fold_5\"\n",
      "Train Data Rows:    17355\n",
      "Train Data Columns: 77\n",
      "Tuning Data Rows:    4338\n",
      "Tuning Data Columns: 77\n",
      "Label Column:       target\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
      "\tFirst 10 (of 21) unique label values:  [np.int64(0), np.int64(19), np.int64(15), np.int64(1), np.int64(16), np.int64(12), np.int64(14), np.int64(18), np.int64(3), np.int64(20)]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 21\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    19247.79 MB\n",
      "\tTrain Data (Original)  Memory Usage: 13.99 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['row_na_cnt']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['ID']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('object', []) : 1 | ['ID']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 73 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_06', ...]\n",
      "\t\t('int', [])   :  2 | ['row_outlier_cnt', 'row_neg_count']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 73 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_06', ...]\n",
      "\t\t('int', [])   :  2 | ['row_outlier_cnt', 'row_neg_count']\n",
      "\t0.2s = Fit runtime\n",
      "\t75 features in original data used to generate 75 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 12.41 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.26s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_macro'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 1799.74s of the 1799.74s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.1/18.8 GB\n",
      "\t0.8669\t = Validation score   (f1_macro)\n",
      "\t18.43s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 1781.25s of the 1781.25s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.2/17.4 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's multi_logloss: 0.564171\tvalid_set's f1_macro: 0.824979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.827\t = Validation score   (f1_macro)\n",
      "\t21.81s\t = Training   runtime\n",
      "\t0.35s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1757.45s of the 1757.45s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.2/20.2 GB\n",
      "\t0.8296\t = Validation score   (f1_macro)\n",
      "\t10.1s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 1746.82s of the 1746.82s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/20.3 GB\n",
      "\t0.804\t = Validation score   (f1_macro)\n",
      "\t4.56s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 1742.00s of the 1742.00s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/20.0 GB\n",
      "\t0.8052\t = Validation score   (f1_macro)\n",
      "\t9.95s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 1731.81s of the 1731.81s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.8174\t = Validation score   (f1_macro)\n",
      "\t251.2s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 1480.53s of the 1480.53s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/20.6 GB\n",
      "\t0.7895\t = Validation score   (f1_macro)\n",
      "\t1.22s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 1478.89s of the 1478.89s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.2/20.0 GB\n",
      "\t0.7723\t = Validation score   (f1_macro)\n",
      "\t1.23s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 1477.21s of the 1477.21s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.8297\t = Validation score   (f1_macro)\n",
      "\t23.13s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 1453.67s of the 1453.67s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.0/19.6 GB\n",
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t0.8633\t = Validation score   (f1_macro)\n",
      "\t212.21s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 1241.39s of the 1241.39s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.4/19.0 GB\n",
      "\t0.8228\t = Validation score   (f1_macro)\n",
      "\t31.57s\t = Training   runtime\n",
      "\t0.29s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1208.28s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.75, 'NeuralNetTorch': 0.25}\n",
      "\t0.8764\t = Validation score   (f1_macro)\n",
      "\t0.46s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 592.21s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 36142.9 rows/s (4338 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\ag_cv_models\\fold_5\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 5] macro-F1: 0.8764\n",
      "\n",
      "===== CV Í≤∞Í≥º =====\n",
      "Fold scores: [0.8746, 0.8722, 0.8766, 0.8772, 0.8764]\n",
      "OOF macro-F1: 0.8755\n",
      "[INFO] OOF Ï†ÄÏû•: C:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\oof_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 5. Stratified K-Fold CV (foldÎ≥ÑÎ°ú G1 ÌëúÏ§ÄÌôî+PCA Ï†ÅÏö© ÌõÑ AutoGluon ÌïôÏäµ) ===\n",
    "if not AUTOGluon_OK:\n",
    "    raise ImportError('AutoGluon Î∂àÎü¨Ïò§Í∏∞ Ïã§Ìå®. ÏÑ§Ïπò ÌïÑÏöî: pip install autogluon.tabular')\n",
    "\n",
    "SAVE_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "fold_indices = list(skf.split(train_pair.drop(columns=[TARGET_COL]), train_pair[TARGET_COL].astype(str)))\n",
    "\n",
    "oof_pred = pd.Series(index=np.arange(len(train_pair)), dtype=object)\n",
    "fold_scores = []\n",
    "\n",
    "for fold, (trn_idx, val_idx) in enumerate(fold_indices, start=1):\n",
    "    print(f'\\n===== [Fold {fold}/{N_FOLDS}] =====')\n",
    "    trn_df = train_pair.iloc[trn_idx].reset_index(drop=True).copy()\n",
    "    val_df = train_pair.iloc[val_idx].reset_index(drop=True).copy()\n",
    "\n",
    "    # # --- G1 Ï≤òÎ¶¨: ÌëúÏ§ÄÌôî ‚Üí PCA(0.95) ‚Üí PC_G1_* ÏÉùÏÑ±, G1 ÏõêÎ≥∏ ÏÇ≠Ï†ú ---\n",
    "    # g1_cols = [c for c in G1 if c in trn_df.columns and c != TARGET_COL]\n",
    "    # if len(g1_cols) > 0:\n",
    "    #     # Í≤∞Ï∏° Ï±ÑÏö∞Í∏∞(ÏàòÏπò median) ‚Üí Ïä§ÏºÄÏùºÎü¨/ PCA ÌïôÏäµÏùÄ trn Í∏∞Ï§Ä\n",
    "    #     imputer_g1 = SimpleImputer(strategy='median')\n",
    "    #     scaler_g1  = StandardScaler()\n",
    "    #     pca_g1     = PCA(n_components=VAR_RATIO, svd_solver='full')\n",
    "\n",
    "    #     trn_g1 = imputer_g1.fit_transform(trn_df[g1_cols])\n",
    "    #     trn_g1 = scaler_g1.fit_transform(trn_g1)\n",
    "    #     trn_g1_pc = pca_g1.fit_transform(trn_g1)\n",
    "\n",
    "    #     # Í≤ÄÏ¶ù Î≥ÄÌôò\n",
    "    #     val_g1 = imputer_g1.transform(val_df[g1_cols])\n",
    "    #     val_g1 = scaler_g1.transform(val_g1)\n",
    "    #     val_g1_pc = pca_g1.transform(val_g1)\n",
    "\n",
    "    #     # PC Ïó¥Î™Ö ÎßåÎì§Í∏∞\n",
    "    #     n_pc = trn_g1_pc.shape[1]\n",
    "    #     pc_cols = [f'PC_G1_{i+1}' for i in range(n_pc)]\n",
    "\n",
    "    #     # Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑÏóê Î∂ôÏù¥Í≥† ÏõêÎ≥∏ G1 ÏÇ≠Ï†ú\n",
    "    #     trn_pc_df = pd.DataFrame(trn_g1_pc, columns=pc_cols)\n",
    "    #     val_pc_df = pd.DataFrame(val_g1_pc, columns=pc_cols)\n",
    "\n",
    "    #     trn_df = pd.concat([trn_df.drop(columns=g1_cols), trn_pc_df], axis=1)\n",
    "    #     val_df = pd.concat([val_df.drop(columns=g1_cols), val_pc_df], axis=1)\n",
    "    # else:\n",
    "    #     print('[INFO] Ïù¥Î≤à foldÏóêÏÑú G1 Ïª¨ÎüºÏù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏïÑ PCA Ïä§ÌÇµ')\n",
    "\n",
    "\n",
    "\n",
    "    # --- G1 Ï≤òÎ¶¨: ÌëúÏ§ÄÌôî ‚Üí PCA(0.95) ‚Üí PC_G1_* ÏÉùÏÑ±, G1 ÏõêÎ≥∏ Ïú†ÏßÄ ---\n",
    "    g1_cols = [c for c in G1 if c in trn_df.columns and c != TARGET_COL]\n",
    "    if len(g1_cols) > 0:\n",
    "        # Í≤∞Ï∏° Ï±ÑÏö∞Í∏∞(ÏàòÏπò median) ‚Üí Ïä§ÏºÄÏùºÎü¨/ PCA ÌïôÏäµÏùÄ trn Í∏∞Ï§Ä\n",
    "        imputer_g1 = SimpleImputer(strategy='median')\n",
    "        scaler_g1  = StandardScaler()\n",
    "        pca_g1     = PCA(n_components=VAR_RATIO, svd_solver='full')\n",
    "\n",
    "        trn_g1 = imputer_g1.fit_transform(trn_df[g1_cols])\n",
    "        trn_g1 = scaler_g1.fit_transform(trn_g1)\n",
    "        trn_g1_pc = pca_g1.fit_transform(trn_g1)\n",
    "\n",
    "        # Í≤ÄÏ¶ù Î≥ÄÌôò\n",
    "        val_g1 = imputer_g1.transform(val_df[g1_cols])\n",
    "        val_g1 = scaler_g1.transform(val_g1)\n",
    "        val_g1_pc = pca_g1.transform(val_g1)\n",
    "\n",
    "        # PC Ïó¥Î™Ö ÎßåÎì§Í∏∞\n",
    "        n_pc = trn_g1_pc.shape[1]\n",
    "        pc_cols = [f'PC_G1_{i+1}' for i in range(n_pc)]\n",
    "\n",
    "        # Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑÏóê Î∂ôÏù¥Í≥† ÏõêÎ≥∏ G1 Ïú†ÏßÄ\n",
    "        trn_pc_df = pd.DataFrame(trn_g1_pc, columns=pc_cols, index=trn_df.index)\n",
    "        val_pc_df = pd.DataFrame(val_g1_pc, columns=pc_cols, index=val_df.index)\n",
    "\n",
    "        trn_df = pd.concat([trn_df, trn_pc_df], axis=1)\n",
    "        val_df = pd.concat([val_df, val_pc_df], axis=1)\n",
    "    else:\n",
    "        print('[INFO] Ïù¥Î≤à foldÏóêÏÑú G1 Ïª¨ÎüºÏù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏïÑ PCA Ïä§ÌÇµ')\n",
    "\n",
    "\n",
    "\n",
    "    # AutoGluon ÌïôÏäµ\n",
    "    fold_dir = SAVE_ROOT / f'fold_{fold}'\n",
    "    if fold_dir.exists():\n",
    "        print(f'[INFO] {fold_dir} Ïû¨ÏÇ¨Ïö©/ÎçÆÏñ¥Ïì∞Í∏∞')\n",
    "    fold_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    ag_trn = TabularDataset(trn_df)\n",
    "    ag_val = TabularDataset(val_df)\n",
    "\n",
    "    predictor = TabularPredictor(label=TARGET_COL, path=str(fold_dir), eval_metric='f1_macro')\n",
    "    predictor.fit(train_data=ag_trn, tuning_data=ag_val, presets=PRESETS, time_limit=TIME_LIMIT, verbosity=2)\n",
    "\n",
    "    # Í≤ÄÏ¶ù ÏòàÏ∏°\n",
    "    y_true = val_df[TARGET_COL].astype(str).reset_index(drop=True)\n",
    "    y_pred = predictor.predict(ag_val).astype(str).reset_index(drop=True)\n",
    "\n",
    "    score = f1_score(y_true, y_pred, average='macro', labels=ALL_CLASSES, zero_division=0)\n",
    "    print(f'[Fold {fold}] macro-F1: {score:.4f}')\n",
    "    fold_scores.append(score)\n",
    "\n",
    "    oof_pred.iloc[val_idx] = y_pred.values\n",
    "\n",
    "# OOF ÏÑ±Îä•\n",
    "y_all = train_pair[TARGET_COL].astype(str).reset_index(drop=True)\n",
    "assert oof_pred.isna().sum() == 0, 'OOF ÏòàÏ∏°Ïóê Í≤∞Ï∏°Ïù¥ ÏûàÏäµÎãàÎã§.'\n",
    "\n",
    "oof_score = f1_score(y_all, oof_pred.astype(str), average='macro', labels=ALL_CLASSES, zero_division=0)\n",
    "print('\\n===== CV Í≤∞Í≥º =====')\n",
    "print('Fold scores:', np.round(fold_scores, 4).tolist())\n",
    "print(f'OOF macro-F1: {oof_score:.4f}')\n",
    "\n",
    "oof_df = pd.DataFrame({'y_true': y_all, 'y_pred': oof_pred.astype(str)})\n",
    "oof_df.to_csv(OOF_PATH, index=False, encoding='utf-8')\n",
    "print('[INFO] OOF Ï†ÄÏû•:', OOF_PATH.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b614d758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"ag_full_model\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.9\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          28\n",
      "Memory Avail:       19.18 GB / 31.69 GB (60.5%)\n",
      "Disk Space Avail:   828.62 GB / 953.01 GB (86.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 1800s\n",
      "AutoGluon will save models to \"c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\ag_full_model\"\n",
      "Train Data Rows:    21693\n",
      "Train Data Columns: 82\n",
      "Label Column:       target\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
      "\tFirst 10 (of 21) unique label values:  [np.int64(0), np.int64(20), np.int64(1), np.int64(19), np.int64(15), np.int64(8), np.int64(16), np.int64(12), np.int64(14), np.int64(18)]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 21\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    19662.89 MB\n",
      "\tTrain Data (Original)  Memory Usage: 14.81 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['row_na_cnt']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['ID']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('object', []) : 1 | ['ID']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 78 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_05', ...]\n",
      "\t\t('int', [])   :  2 | ['row_outlier_cnt', 'row_neg_count']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 78 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_05', ...]\n",
      "\t\t('int', [])   :  2 | ['row_outlier_cnt', 'row_neg_count']\n",
      "\t0.2s = Fit runtime\n",
      "\t80 features in original data used to generate 80 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 13.24 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.21s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_macro'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 19523, Val Rows: 2170\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 1799.79s of the 1799.79s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.1/19.2 GB\n",
      "\t0.8528\t = Validation score   (f1_macro)\n",
      "\t20.05s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 1779.69s of the 1779.69s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.2/19.2 GB\n",
      "\t0.8226\t = Validation score   (f1_macro)\n",
      "\t13.58s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1765.12s of the 1765.11s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.2/19.2 GB\n",
      "\t0.8208\t = Validation score   (f1_macro)\n",
      "\t10.02s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 1754.61s of the 1754.61s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.3/19.2 GB\n",
      "\t0.8007\t = Validation score   (f1_macro)\n",
      "\t4.47s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 1749.87s of the 1749.87s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.3/18.7 GB\n",
      "\t0.7926\t = Validation score   (f1_macro)\n",
      "\t11.33s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 1738.29s of the 1738.29s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.8126\t = Validation score   (f1_macro)\n",
      "\t283.17s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 1455.04s of the 1455.04s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.3/19.2 GB\n",
      "\t0.7787\t = Validation score   (f1_macro)\n",
      "\t1.29s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr ... Training model for up to 1453.36s of the 1453.36s of remaining time.\n",
      "\tFitting with cpus=28, gpus=0, mem=0.3/18.5 GB\n",
      "\t0.7658\t = Validation score   (f1_macro)\n",
      "\t1.34s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 1451.62s of the 1451.62s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0\n",
      "\t0.8303\t = Validation score   (f1_macro)\n",
      "\t28.7s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 1422.48s of the 1422.48s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.1/18.8 GB\n",
      "c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t0.8595\t = Validation score   (f1_macro)\n",
      "\t297.36s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 1125.06s of the 1125.06s of remaining time.\n",
      "\tFitting with cpus=20, gpus=0, mem=0.4/19.1 GB\n",
      "\t0.818\t = Validation score   (f1_macro)\n",
      "\t27.29s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1096.94s of remaining time.\n",
      "\tEnsemble Weights: {'XGBoost': 0.333, 'NeuralNetTorch': 0.333, 'NeuralNetFastAI': 0.222, 'ExtraTreesEntr': 0.111}\n",
      "\t0.8641\t = Validation score   (f1_macro)\n",
      "\t0.34s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 703.45s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 10137.0 rows/s (2170 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\ag_full_model\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] submission Ï†ÄÏû•: C:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\submission.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 6. Full-train + Test ÏòàÏ∏° (Ï†ÑÏó≠ ÎåÄÌëú/ÎìúÎ°≠ Ï†ÅÏö© Í∏∞Ï§Ä, G1ÏùÄ full-train Í∏∞Ï§ÄÏúºÎ°ú PCA) ===\n",
    "if test is None:\n",
    "    print('[WARN] test.csv ÏóÜÏùå ‚Üí Ïä§ÌÇµ')\n",
    "else:\n",
    "    # Ï†ÑÏó≠ pair-drop Î∞òÏòÅÎêú ÏÇ¨Î≥∏ ÏÇ¨Ïö©\n",
    "    trn_full = train_pair.copy()\n",
    "    tst_full = test_pair.copy()\n",
    "\n",
    "    # # G1 Ï≤òÎ¶¨: Í≤∞Ï∏°‚ÜíÌëúÏ§ÄÌôî‚ÜíPCA(0.95) ‚Üí PC_G1_* ÎÇ®Í∏∞Í≥† ÏõêÎ≥∏ ÏÇ≠Ï†ú (full-train Í∏∞Ï§ÄÏúºÎ°ú Ï†ÅÌï©)\n",
    "    # g1_cols_full = [c for c in G1 if c in trn_full.columns and c != TARGET_COL]\n",
    "    # if len(g1_cols_full) > 0:\n",
    "    #     imputer_g1_full = SimpleImputer(strategy='median')\n",
    "    #     scaler_g1_full  = StandardScaler()\n",
    "    #     pca_g1_full     = PCA(n_components=VAR_RATIO, svd_solver='full')\n",
    "\n",
    "    #     trn_g1f = imputer_g1_full.fit_transform(trn_full[g1_cols_full])\n",
    "    #     trn_g1f = scaler_g1_full.fit_transform(trn_g1f)\n",
    "    #     trn_g1f_pc = pca_g1_full.fit_transform(trn_g1f)\n",
    "\n",
    "    #     tst_g1f = imputer_g1_full.transform(tst_full[g1_cols_full])\n",
    "    #     tst_g1f = scaler_g1_full.transform(tst_g1f)\n",
    "    #     tst_g1f_pc = pca_g1_full.transform(tst_g1f)\n",
    "\n",
    "    #     n_pc_full = trn_g1f_pc.shape[1]\n",
    "    #     pc_cols_full = [f'PC_G1_{i+1}' for i in range(n_pc_full)]\n",
    "\n",
    "    #     trn_pc_full = pd.DataFrame(trn_g1f_pc, columns=pc_cols_full)\n",
    "    #     tst_pc_full = pd.DataFrame(tst_g1f_pc, columns=pc_cols_full)\n",
    "\n",
    "    #     trn_full = pd.concat([trn_full.drop(columns=g1_cols_full), trn_pc_full], axis=1)\n",
    "    #     tst_full = pd.concat([tst_full.drop(columns=g1_cols_full), tst_pc_full], axis=1)\n",
    "    # else:\n",
    "    #     print('[INFO] full-train Í∏∞Ï§Ä G1 Ïª¨Îüº ÏóÜÏùå ‚Üí PCA Ïä§ÌÇµ')\n",
    "\n",
    "\n",
    "    # G1 Ï≤òÎ¶¨: Í≤∞Ï∏°‚ÜíÌëúÏ§ÄÌôî‚ÜíPCA(0.95) ‚Üí PC_G1_* Ï∂îÍ∞Ä (ÏõêÎ≥∏ÏùÄ Ïú†ÏßÄ)\n",
    "    g1_cols_full = [c for c in G1 if c in trn_full.columns and c != TARGET_COL]\n",
    "    if len(g1_cols_full) > 0:\n",
    "        imputer_g1_full = SimpleImputer(strategy='median')\n",
    "        scaler_g1_full  = StandardScaler()\n",
    "        pca_g1_full     = PCA(n_components=VAR_RATIO, svd_solver='full')\n",
    "\n",
    "        # train\n",
    "        trn_g1f = imputer_g1_full.fit_transform(trn_full[g1_cols_full])\n",
    "        trn_g1f = scaler_g1_full.fit_transform(trn_g1f)\n",
    "        trn_g1f_pc = pca_g1_full.fit_transform(trn_g1f)\n",
    "\n",
    "        # test\n",
    "        tst_g1f = imputer_g1_full.transform(tst_full[g1_cols_full])\n",
    "        tst_g1f = scaler_g1_full.transform(tst_g1f)\n",
    "        tst_g1f_pc = pca_g1_full.transform(tst_g1f)\n",
    "\n",
    "        # PC Ïª¨Îüº ÏÉùÏÑ±\n",
    "        n_pc_full = trn_g1f_pc.shape[1]\n",
    "        pc_cols_full = [f'PC_G1_{i+1}' for i in range(n_pc_full)]\n",
    "\n",
    "        trn_pc_full = pd.DataFrame(trn_g1f_pc, columns=pc_cols_full, index=trn_full.index)\n",
    "        tst_pc_full = pd.DataFrame(tst_g1f_pc, columns=pc_cols_full, index=tst_full.index)\n",
    "\n",
    "        # ÏõêÎ≥∏ÏùÄ Ïú†ÏßÄÌïòÍ≥† PCÎßå Ï∂îÍ∞Ä\n",
    "        trn_full = pd.concat([trn_full, trn_pc_full], axis=1)\n",
    "        tst_full = pd.concat([tst_full, tst_pc_full], axis=1)\n",
    "    else:\n",
    "        print('[INFO] full-train Í∏∞Ï§Ä G1 Ïª¨Îüº ÏóÜÏùå ‚Üí PCA Ïä§ÌÇµ')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # AutoGluon Ï†ÑÏ≤¥ ÌïôÏäµ\n",
    "    FULL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    ag_full = TabularDataset(trn_full)\n",
    "    full_predictor = TabularPredictor(label=TARGET_COL, path=str(FULL_DIR), eval_metric='f1_macro')\n",
    "    full_predictor.fit(train_data=ag_full, presets=PRESETS, time_limit=TIME_LIMIT, verbosity=2)\n",
    "\n",
    "    # Test ÏòàÏ∏° & Ï†úÏ∂ú Ï†ÄÏû•\n",
    "    ag_test = TabularDataset(tst_full)\n",
    "    test_pred = full_predictor.predict(ag_test).astype(str)\n",
    "\n",
    "    if ID_COL is not None and ID_COL in test.columns:\n",
    "        sub = pd.DataFrame({ID_COL: test[ID_COL], TARGET_COL: test_pred})\n",
    "    else:\n",
    "        sub = pd.DataFrame({'row_id': np.arange(len(test_pred)), TARGET_COL: test_pred})\n",
    "\n",
    "    sub.to_csv(SUB_PATH, index=False, encoding='utf-8')\n",
    "    print('[INFO] submission Ï†ÄÏû•:', SUB_PATH.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8df4dd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "These features in provided data are not utilized by the predictor and will be ignored: ['ID', 'X_05', 'X_20', 'X_22', 'X_25', 'X_51', 'row_na_cnt']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"1 required columns are missing from the provided dataset to transform using AutoMLPipelineFeatureGenerator. 1 missing columns: ['PC_G1_1'] | 74 available columns: ['X_01', 'X_02', 'X_03', 'X_04', 'X_06', 'X_07', 'X_08', 'X_11', 'X_12', 'X_13', 'X_14', 'X_15', 'X_16', 'X_18', 'X_19', 'X_23', 'X_24', 'X_26', 'X_27', 'X_28', 'X_29', 'X_31', 'X_32', 'X_34', 'X_35', 'X_36', 'X_37', 'X_38', 'X_41', 'X_42', 'X_43', 'X_44', 'X_46', 'X_48', 'X_49', 'X_50', 'X_52', 'row_num_mean', 'row_num_std', 'row_outlier_cnt', 'G1_mean', 'G1_std', 'G1_min', 'G1_max', 'G1_range', 'X05_div_X09', 'X05_mul_X09', 'X05_sq', 'X09_sq', 'X20_div_X22', 'X20_mul_X22', 'X20_sq', 'X22_sq', 'X25_div_X51', 'X25_mul_X51', 'X25_sq', 'X51_sq', 'row_neg_count', 'X_11_std', 'X_19_std', 'X_37_std', 'X_40_std', 'X_08_sq', 'X_19_sq', 'X_29_sq', 'X_46_sq', 'X_41_sq', 'X_49_sq', 'X_08xX_19', 'X_08_div_X_19', 'X_19_div_X_08', 'X_08xX_29', 'X_08_div_X_29', 'X_29_div_X_08']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\autogluon\\features\\generators\\abstract.py:356\u001b[39m, in \u001b[36mAbstractFeatureGenerator.transform\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    353\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(X.columns) != \u001b[38;5;28mself\u001b[39m.features_in:\n\u001b[32m    354\u001b[39m         \u001b[38;5;66;03m# It comes at a cost when making a copy of the DataFrame,\u001b[39;00m\n\u001b[32m    355\u001b[39m         \u001b[38;5;66;03m# therefore, try avoid copying by checking the expected features first.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m         X = \u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures_in\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4112\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4115\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6264\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6263\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6264\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['PC_G1_1'] not in index\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Ï§ëÏöîÎèÑ Í≥ÑÏÇ∞\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m fi = \u001b[43mpredictor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeature_importance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Ï†ÑÏ≤¥ Ï∂úÎ†•\u001b[39;00m\n\u001b[32m     16\u001b[39m display(fi)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:3490\u001b[39m, in \u001b[36mTabularPredictor.feature_importance\u001b[39m\u001b[34m(self, data, model, features, feature_stage, subsample_size, time_limit, num_shuffle_sets, include_confidence_band, confidence_level, silent)\u001b[39m\n\u001b[32m   3487\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_shuffle_sets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3488\u001b[39m     num_shuffle_sets = \u001b[32m10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m time_limit \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m5\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m3490\u001b[39m fi_df = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_learner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_feature_importance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3492\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3493\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_stage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3495\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubsample_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubsample_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_shuffle_sets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_shuffle_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3498\u001b[39m \u001b[43m    \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m=\u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3499\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3501\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_confidence_band:\n\u001b[32m   3502\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m confidence_level <= \u001b[32m0.5\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m confidence_level >= \u001b[32m1.0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\autogluon\\tabular\\learner\\abstract_learner.py:1010\u001b[39m, in \u001b[36mAbstractTabularLearner.get_feature_importance\u001b[39m\u001b[34m(self, model, X, y, features, feature_stage, subsample_size, silent, **kwargs)\u001b[39m\n\u001b[32m   1007\u001b[39m         X = X.drop(columns=unused_features)\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m feature_stage == \u001b[33m\"\u001b[39m\u001b[33moriginal\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1010\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_feature_importance_raw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubsample_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubsample_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform_func\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m=\u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m     X = \u001b[38;5;28mself\u001b[39m.transform_features(X)\n\u001b[32m   1014\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\autogluon\\tabular\\trainer\\abstract_trainer.py:3522\u001b[39m, in \u001b[36mAbstractTabularTrainer._get_feature_importance_raw\u001b[39m\u001b[34m(self, X, y, model, eval_metric, **kwargs)\u001b[39m\n\u001b[32m   3520\u001b[39m model: AbstractModel = \u001b[38;5;28mself\u001b[39m.load_model(model)\n\u001b[32m   3521\u001b[39m predict_func_kwargs = \u001b[38;5;28mdict\u001b[39m(model=model)\n\u001b[32m-> \u001b[39m\u001b[32m3522\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompute_permutation_feature_importance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3523\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3524\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3525\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpredict_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpredict_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3526\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpredict_func_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpredict_func_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3527\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3528\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantile_levels\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquantile_levels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3529\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3530\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\autogluon\\core\\utils\\utils.py:983\u001b[39m, in \u001b[36mcompute_permutation_feature_importance\u001b[39m\u001b[34m(X, y, predict_func, eval_metric, features, subsample_size, num_shuffle_sets, predict_func_kwargs, transform_func, transform_func_kwargs, time_limit, silent, log_prefix, importance_as_list, random_state, **kwargs)\u001b[39m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m subsample \u001b[38;5;129;01mor\u001b[39;00m shuffle_repeat == \u001b[32m0\u001b[39m:\n\u001b[32m    982\u001b[39m     time_start_score = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m983\u001b[39m     X_transformed = X \u001b[38;5;28;01mif\u001b[39;00m transform_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mtransform_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtransform_func_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    984\u001b[39m     y_pred = predict_func(X_transformed, **predict_func_kwargs)\n\u001b[32m    985\u001b[39m     score_baseline = eval_metric(y, y_pred, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\autogluon\\tabular\\learner\\abstract_learner.py:488\u001b[39m, in \u001b[36mAbstractTabularLearner.transform_features\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    486\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtransform_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m    487\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m feature_generator \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.feature_generators:\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m         X = \u001b[43mfeature_generator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    489\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SSAFY\\Desktop\\STUDY\\Autogluon\\venv\\Lib\\site-packages\\autogluon\\features\\generators\\abstract.py:362\u001b[39m, in \u001b[36mAbstractFeatureGenerator.transform\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    360\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m X.columns:\n\u001b[32m    361\u001b[39m             missing_cols.append(col)\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[32m    363\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(missing_cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m required columns are missing from the provided dataset to transform using \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    364\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(missing_cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m missing columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_cols\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    365\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(X.columns))\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m available columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(X.columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    366\u001b[39m     )\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pre_astype_generator:\n\u001b[32m    368\u001b[39m     X = \u001b[38;5;28mself\u001b[39m._pre_astype_generator.transform(X)\n",
      "\u001b[31mKeyError\u001b[39m: \"1 required columns are missing from the provided dataset to transform using AutoMLPipelineFeatureGenerator. 1 missing columns: ['PC_G1_1'] | 74 available columns: ['X_01', 'X_02', 'X_03', 'X_04', 'X_06', 'X_07', 'X_08', 'X_11', 'X_12', 'X_13', 'X_14', 'X_15', 'X_16', 'X_18', 'X_19', 'X_23', 'X_24', 'X_26', 'X_27', 'X_28', 'X_29', 'X_31', 'X_32', 'X_34', 'X_35', 'X_36', 'X_37', 'X_38', 'X_41', 'X_42', 'X_43', 'X_44', 'X_46', 'X_48', 'X_49', 'X_50', 'X_52', 'row_num_mean', 'row_num_std', 'row_outlier_cnt', 'G1_mean', 'G1_std', 'G1_min', 'G1_max', 'G1_range', 'X05_div_X09', 'X05_mul_X09', 'X05_sq', 'X09_sq', 'X20_div_X22', 'X20_mul_X22', 'X20_sq', 'X22_sq', 'X25_div_X51', 'X25_mul_X51', 'X25_sq', 'X51_sq', 'row_neg_count', 'X_11_std', 'X_19_std', 'X_37_std', 'X_40_std', 'X_08_sq', 'X_19_sq', 'X_29_sq', 'X_46_sq', 'X_41_sq', 'X_49_sq', 'X_08xX_19', 'X_08_div_X_19', 'X_19_div_X_08', 'X_08xX_29', 'X_08_div_X_29', 'X_29_div_X_08']\""
     ]
    }
   ],
   "source": [
    "# === Feature Importance (AutoGluon: Ï†ÑÏ≤¥) ===\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ÎùºÎ≤® ÏûêÎèô ÌÉêÏßÄ\n",
    "label_candidates = [\"target\", \"TARGET\", \"label\", \"Label\", \"y\"]\n",
    "label_col = None\n",
    "for col in train_pair.columns:\n",
    "    if col in label_candidates:\n",
    "        label_col = col\n",
    "        break\n",
    "\n",
    "# Ï§ëÏöîÎèÑ Í≥ÑÏÇ∞\n",
    "fi = predictor.feature_importance(train_pair, silent=True)\n",
    "\n",
    "# Ï†ÑÏ≤¥ Ï∂úÎ†•\n",
    "display(fi)\n",
    "\n",
    "# ÏãúÍ∞ÅÌôî (Ï†ÑÏ≤¥ ÌîºÏ≤ò)\n",
    "plt.figure(figsize=(8, max(6, 0.3*len(fi))))\n",
    "fi.sort_values(\"importance\").plot(kind=\"barh\", y=\"importance\", legend=False)\n",
    "plt.title(\"Feature Importance (All Features)\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9351c1",
   "metadata": {},
   "source": [
    "\n",
    "## Ï†ÑÏ≤òÎ¶¨ Î©îÎ™® (ÏöîÏïΩ)\n",
    "- **PAIR_GROUPS ÎåÄÌëú ÏÑ†ÌÉù**: `(-Í≤∞Ï∏°Î•†) Ï†ïÍ∑úÌôî + MI Ï†ïÍ∑úÌôî + F Ï†ïÍ∑úÌôî` ÌèâÍ∑†ÏúºÎ°ú ÎåÄÌëú 1Í∞ú ÏÑ†ÌÉù(Ï†ÑÏó≠ Í≥†Ï†ï). ÎåÄÌëú Ïô∏ Î≥ÄÏàòÎäî **ÏÇ≠Ï†ú**.\n",
    "- **G1 Í∑∏Î£π**: ÏàòÏπò Í≤∞Ï∏° median ÎåÄÏ≤¥ ‚Üí ÌëúÏ§ÄÌôî(`StandardScaler`) ‚Üí `PCA(n_components=0.95)` ‚Üí `PC_G1_i`Îßå Ïú†ÏßÄ, G1 ÏõêÎ≥∏ **ÏÇ≠Ï†ú**.\n",
    "- **K-Fold ÎÇ¥ ÎàÑÏàò Î∞©ÏßÄ**: G1Ïùò `imputer/scaler/PCA`Îäî **Í∞Å Ìè¥ÎìúÏùò train Î∂ÄÎ∂ÑÏúºÎ°úÎßå Ï†ÅÌï©** ÌõÑ valÏóê Ï†ÅÏö©. (Ï†ÑÏó≠ pair-dropÏùÄ Í∏∞Ï§Ä ÌÜµÏùºÏùÑ ÏúÑÌï¥ train Ï†ÑÏ≤¥ÏóêÏÑú 1Ìöå ÏÇ∞Ï†ï)\n",
    "- AutoGluonÏùÄ ÎÇòÎ®∏ÏßÄ Í≤∞Ï∏°/Î≤îÏ£ºÌòï Ï≤òÎ¶¨Î•º ÏûêÏ≤¥Ï†ÅÏúºÎ°ú ÏàòÌñâ.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
